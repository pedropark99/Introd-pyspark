<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Working with SQL in pyspark – Introduction to `pyspark`</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Chapters/08-transforming2.html" rel="next">
<link href="../Chapters/07-import.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-G42L33VM26"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-G42L33VM26', { 'anonymize_ip': true});
</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Introduction to <code>pyspark</code></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://pedro-faria.netlify.app/"> 
<span class="menu-text">Visit the author’s blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pedropark99/Introd-pyspark"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Chapters/06-dataframes-sql.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with SQL in <code>pyspark</code></span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/02-python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Key concepts of python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/03-spark.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introducing Apache Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/04-dataframes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introducing Spark DataFrames</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/04-columns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing the <code>Column</code> class</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/05-transforming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/07-import.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Importing data to Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/06-dataframes-sql.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with SQL in <code>pyspark</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/08-transforming2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/07-export.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exporting data out of Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/09-strings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Tools for string manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/10-datetime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Tools for dates and datetimes manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/11-window.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introducing window functions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-sql-method-as-the-main-entrypoint" id="toc-the-sql-method-as-the-main-entrypoint" class="nav-link active" data-scroll-target="#the-sql-method-as-the-main-entrypoint"><span class="header-section-number">7.1</span> The <code>sql()</code> method as the main entrypoint</a>
  <ul class="collapse">
  <li><a href="#a-single-sql-statement-per-run" id="toc-a-single-sql-statement-per-run" class="nav-link" data-scroll-target="#a-single-sql-statement-per-run"><span class="header-section-number">7.1.1</span> A single SQL statement per run</a></li>
  </ul></li>
  <li><a href="#creating-sql-tables-in-spark" id="toc-creating-sql-tables-in-spark" class="nav-link" data-scroll-target="#creating-sql-tables-in-spark"><span class="header-section-number">7.2</span> Creating SQL Tables in Spark</a>
  <ul class="collapse">
  <li><a href="#tables-versus-views" id="toc-tables-versus-views" class="nav-link" data-scroll-target="#tables-versus-views"><span class="header-section-number">7.2.1</span> <code>TABLEs</code> versus <code>VIEWs</code></a></li>
  <li><a href="#sec-temp-persist" id="toc-sec-temp-persist" class="nav-link" data-scroll-target="#sec-temp-persist"><span class="header-section-number">7.2.2</span> Temporary versus Persistent sources</a></li>
  <li><a href="#spark-sql-catalog-is-the-bridge-between-sql-and-pyspark" id="toc-spark-sql-catalog-is-the-bridge-between-sql-and-pyspark" class="nav-link" data-scroll-target="#spark-sql-catalog-is-the-bridge-between-sql-and-pyspark"><span class="header-section-number">7.2.3</span> Spark SQL Catalog is the bridge between SQL and <code>pyspark</code></a></li>
  </ul></li>
  <li><a href="#the-penguins-dataframe" id="toc-the-penguins-dataframe" class="nav-link" data-scroll-target="#the-penguins-dataframe"><span class="header-section-number">7.3</span> The <code>penguins</code> DataFrame</a></li>
  <li><a href="#selecting-your-spark-dataframes" id="toc-selecting-your-spark-dataframes" class="nav-link" data-scroll-target="#selecting-your-spark-dataframes"><span class="header-section-number">7.4</span> Selecting your Spark DataFrames</a></li>
  <li><a href="#executing-sql-expressions" id="toc-executing-sql-expressions" class="nav-link" data-scroll-target="#executing-sql-expressions"><span class="header-section-number">7.5</span> Executing SQL expressions</a></li>
  <li><a href="#every-dataframe-transformation-in-python-can-be-translated-into-sql" id="toc-every-dataframe-transformation-in-python-can-be-translated-into-sql" class="nav-link" data-scroll-target="#every-dataframe-transformation-in-python-can-be-translated-into-sql"><span class="header-section-number">7.6</span> Every DataFrame transformation in Python can be translated into SQL</a>
  <ul class="collapse">
  <li><a href="#dataframe-methods-are-usually-translated-into-sql-keywords" id="toc-dataframe-methods-are-usually-translated-into-sql-keywords" class="nav-link" data-scroll-target="#dataframe-methods-are-usually-translated-into-sql-keywords"><span class="header-section-number">7.6.1</span> DataFrame methods are usually translated into SQL keywords</a></li>
  <li><a href="#sec-sql-expr" id="toc-sec-sql-expr" class="nav-link" data-scroll-target="#sec-sql-expr"><span class="header-section-number">7.6.2</span> Spark functions are usually translated into SQL functions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-dataframe-sql-chapter" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with SQL in <code>pyspark</code></span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>As we discussed in <a href="03-spark.html" class="quarto-xref"><span>Chapter 2</span></a>, Spark is a <strong>multi-language</strong> engine for large-scale data processing. This means that we can build our Spark application using many different languages (like Java, Scala, Python and R). Furthermore, you can also use the Spark SQL module of Spark to translate all of your transformations into pure SQL queries.</p>
<p>In more details, Spark SQL is a Spark module for structured data processing <span class="citation" data-cites="sparkdoc">(<a href="references.html#ref-sparkdoc" role="doc-biblioref"><em>Apache Spark Official Documentation</em> 2022</a>)</span>. Because this module works with Spark DataFrames, using SQL, you can translate all transformations that you build with the DataFrame API into a SQL query.</p>
<p>Therefore, you can mix python code with SQL queries very easily in Spark. Virtually all transformations exposed in python throughout this book, can be translated into a SQL query using this module of Spark. We will focus a lot on this exchange between Python and SQL in this chapter.</p>
<p>However, this also means that the Spark SQL module does not handle the transformations produced by the unstructured APIs of Spark, i.e.&nbsp;the Dataset API. Since the Dataset API is not available in <code>pyspark</code>, it is not covered in this book.</p>
<section id="the-sql-method-as-the-main-entrypoint" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="the-sql-method-as-the-main-entrypoint"><span class="header-section-number">7.1</span> The <code>sql()</code> method as the main entrypoint</h2>
<p>The main entrypoint, that is, the main bridge that connects Spark SQL to Python is the <code>sql()</code> method of your Spark Session. This method accepts a SQL query inside a string as input, and will always output a new Spark DataFrame as result. That is why I used the <code>show()</code> method right after <code>sql()</code>, in the example below, to see what this new Spark DataFrame looked like.</p>
<p>As a first example, lets look at a very basic SQL query, that just select a list of code values:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> (</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">VALUES</span> (<span class="dv">11</span>), (<span class="dv">31</span>), (<span class="dv">24</span>), (<span class="dv">35</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>) <span class="kw">AS</span> <span class="kw">List</span>(Codes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To run the above SQL query, and see its results, I must write this query into a string, and give this string to the <code>sql()</code> method of my Spark Session. Then, I use the <code>show()</code> action to see the actual result rows of data generated by this query:</p>
<div id="21bba834" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>sql_query <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT *</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="st">FROM (</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="st">  VALUES (11), (31), (24), (35)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="st">) AS List(Codes)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>spark.sql(sql_query).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----+
|Codes|
+-----+
|   11|
|   31|
|   24|
|   35|
+-----+
</code></pre>
</div>
</div>
<p>If you want to execute a very short SQL query, is fine to write it inside a single pair of quotation marks (for example <code>"SELECT * FROM sales.per_day"</code>). However, since SQL queries usually take multiple lines, you can write your SQL query inside a python docstring (created by a pair of three quotation marks), like in the example above.</p>
<p>Having this in mind, every time you want to execute a SQL query, you can use this <code>sql()</code> method from the object that holds your Spark Session. So the <code>sql()</code> method is the bridge between <code>pyspark</code> and SQL. You give it a pure SQL query inside a string, and, Spark will execute it, considering your Spark SQL context.</p>
<section id="a-single-sql-statement-per-run" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="a-single-sql-statement-per-run"><span class="header-section-number">7.1.1</span> A single SQL statement per run</h3>
<p>Is worth pointing out that, although being the main bridge between Python and SQL, the Spark Session <code>sql()</code> method can execute only a single SQL statement per run. This means that if you try to execute two sequential SQL statements at the same time with <code>sql()</code>, then, Spark SQL will automatically raise a <code>ParseException</code> error, which usually complains about an “extra input”.</p>
<p>In the example below, we are doing two very basic steps to SQL. We first create a dummy database with a <code>CREATE DATABASE</code> statement, then, we ask SQL to use this new database that we created as the default database of the current session, with a <code>USE</code> statement.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">CREATE</span> <span class="kw">DATABASE</span> `dummy`;</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">USE</span> `dummy`;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we try to execute these two steps at once, by using the <code>sql()</code> method, Spark complains with a <code>ParseException</code>, indicating that we have a sytax error in our query, like in the example below:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">CREATE DATABASE `dummy`;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="st">USE `dummy`;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>spark.sql(query).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/opt/spark/python/pyspark/sql/session.py", line 1034, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self)
  File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 196, in deco
    raise converted from None
pyspark.sql.utils.ParseException: 
Syntax error at or near 'USE': extra input 'USE'(line 3, pos 0)

== SQL ==

CREATE DATABASE `dummy`;
USE `dummy`;
^^^</code></pre>
<p>However, there is nothing wrong about the above SQL statements. They are both correct and valid SQL statements, both semantically and syntactically. In other words, the case above results in a <code>ParseException</code> error solely because it contains two different SQL statements.</p>
<p>In essence, the <code>spark.sql()</code> method always expect a single SQL statement as input, and, therefore, it will try to parse this input query as a single SQL statement. If it finds multiple SQL statements inside this input string, the method will automatically raise the above error.</p>
<p>Now, be aware that some SQL queries can take multiple lines, but, <strong>still be considered a single SQL statement</strong>. A query started by a <code>WITH</code> clause is usually a good example of a SQL query that can group multiple <code>SELECT</code> statements, but still be considered a single SQL statement as a whole:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- The query below would execute</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- perfectly fine inside spark.sql():</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">WITH</span> table1 <span class="kw">AS</span> (</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">FROM</span> somewhere</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>),</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>filtering <span class="kw">AS</span> (</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">FROM</span> table1</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="kw">WHERE</span> dateOfTransaction <span class="op">==</span> <span class="fu">CAST</span>(<span class="ot">"2022-02-02"</span> <span class="kw">AS</span> <span class="dt">DATE</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> filtering</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Another example of a usually big and complex query, that can take multiple lines but still be considered a single SQL statement, is a single <code>SELECT</code> statement that selects multiple subqueries that are nested together, like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- The query below would also execute</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- perfectly fine inside spark.sql():</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> (</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- First subquery.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">FROM</span> (</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- Second subquery..</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> (</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>      <span class="co">-- Third subquery...</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>      <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>      <span class="kw">FROM</span> (</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">-- Ok this is enough....</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>However, if we had multiple separate <code>SELECT</code> statements that were independent on each other, like in the example below, then, <code>spark.sql()</code> would issue an <code>ParseException</code> error if we tried to execute these three <code>SELECT</code> statements inside the same input string.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- These three statements CANNOT be executed</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- at the same time inside spark.sql()</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> something;</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> somewhere;</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> sometime;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a conclusion, if you want to easily execute multiple statements, you can use a <code>for</code> loop which calls <code>spark.sql()</code> for each single SQL statement:</p>
<div id="3c5aeccc" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>statements <span class="op">=</span> <span class="st">'''SELECT * FROM something;</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT * FROM somewhere;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT * FROM sometime;'''</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>statements <span class="op">=</span> statements.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> statement <span class="kw">in</span> statements:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  spark.sql(statement)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="creating-sql-tables-in-spark" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="creating-sql-tables-in-spark"><span class="header-section-number">7.2</span> Creating SQL Tables in Spark</h2>
<p>In real life jobs at the industry, is very likely that your data will be allocated inside a SQL-like database. Spark can connect to a external SQL database through JDBC/ODBC connections, or, read tables from Apache Hive. This way, you can sent your SQL queries to this external database.</p>
<p>However, to expose more simplified examples throughout this chapter, we will use <code>pyspark</code> to create a simple temporary SQL table in our Spark SQL context, and use this temporary SQL table in our examples of SQL queries. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in <code>pyspark</code>.</p>
<p>First, lets create our Spark Session. You can see below that I used the <code>config()</code> method to set a specific option of the session called <code>catalogImplementation</code> to the value <code>"hive"</code>. This option controls the implementation of the Spark SQL Catalog, which is a core part of the SQL functionality of Spark <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Spark usually complain with a <code>AnalysisException</code> error when you try to create SQL tables with this option undefined (or not configured). So, if you decide to follow the examples of this chapter, please always set this option right at the start of your script<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div id="d052c5e9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>spark <span class="op">=</span> SparkSession<span class="op">\</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  .builder<span class="op">\</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  .config(<span class="st">"spark.sql.catalogImplementation"</span>,<span class="st">"hive"</span>)<span class="op">\</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  .getOrCreate()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="tables-versus-views" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="tables-versus-views"><span class="header-section-number">7.2.1</span> <code>TABLEs</code> versus <code>VIEWs</code></h3>
<p>To run a complete SQL query over any Spark DataFrame, you must register this DataFrame in the Spark SQL Catalog of your Spark Session. You can register a Spark DataFrame into this catalog as a physical SQL <code>TABLE</code>, or, as a SQL <code>VIEW</code>.</p>
<p>If you are familiar with the SQL language and Relational DataBase Management Systems - RDBMS (such as MySQL), you probably already heard of these two types (<code>TABLE</code> or <code>VIEW</code>) of SQL objects. But if not, we will explain each one in this section. Is worth pointing out that choosing between these two types <strong>does not affect</strong> your code, or your transformations in any way. It just affect the way that Spark SQL stores the table/DataFrame itself.</p>
<section id="views-are-stored-as-sql-queries-or-memory-pointers" class="level4" data-number="7.2.1.1">
<h4 data-number="7.2.1.1" class="anchored" data-anchor-id="views-are-stored-as-sql-queries-or-memory-pointers"><span class="header-section-number">7.2.1.1</span> <code>VIEWs</code> are stored as SQL queries or memory pointers</h4>
<p>When you register a DataFrame as a SQL <code>VIEW</code>, the query to produce this DataFrame is stored, not the DataFrame itself. There are also cases where Spark store a memory pointer instead, that points to the memory adress where this DataFrame is stored in memory. In this perspective, Spark SQL use this pointer every time it needs to access this DataFrame.</p>
<p>Therefore, when you call (or access) this SQL <code>VIEW</code> inside your SQL queries (for example, with a <code>SELECT * FROM</code> statement), Spark SQL will automatically get this SQL <code>VIEW</code> “on the fly” (or “on runtime”), by executing the query necessary to build the initial DataFrame that you stored inside this <code>VIEW</code>, or, if this DataFrame is already stored in memory, Spark will look at the specific memory address it is stored.</p>
<p>In other words, when you create a SQL <code>VIEW</code>, Spark SQL do not store any physical data or rows of the DataFrame. It just stores the SQL query necessary to build your DataFrame. In some way, you can interpret any SQL <code>VIEW</code> as an abbreviation to a SQL query, or a “nickname” to an already existing DataFrame.</p>
<p>As a consequence, for most “use case scenarios”, SQL <code>VIEWs</code> are easier to manage inside your data pipelines. Because you usually do not have to update them. Since they are calculated from scratch, at the moment you request for them, a SQL <code>VIEW</code> will always translate the most recent version of the data.</p>
<p>This means that the concept of a <code>VIEW</code> in Spark SQL is very similar to the concept of a <code>VIEW</code> in other types of SQL databases, such as the MySQL database. If you read the <a href="https://dev.mysql.com/doc/refman/8.0/en/create-view.html">official documentation for the <code>CREATE VIEW</code> statement at MySQL</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> you will get a similar idea of a <code>VIEW</code>:</p>
<blockquote class="blockquote">
<p>The select_statement is a SELECT statement that provides the definition of the view. (Selecting from the view selects, in effect, using the SELECT statement.) …</p>
</blockquote>
<p>The above statement, tells us that selecing a <code>VIEW</code> causes the SQL engine to execute the expression defined at <code>select_statement</code> using the <code>SELECT</code> statement. In other words, in MySQL, a SQL <code>VIEW</code> is basically an alias to an existing <code>SELECT</code> statement.</p>
</section>
<section id="differences-in-spark-sql-views" class="level4" data-number="7.2.1.2">
<h4 data-number="7.2.1.2" class="anchored" data-anchor-id="differences-in-spark-sql-views"><span class="header-section-number">7.2.1.2</span> Differences in Spark SQL <code>VIEW</code>s</h4>
<p>Although a Spark SQL <code>VIEW</code> being very similar to other types of SQL <code>VIEW</code> (such as the MySQL type), on Spark applications, SQL <code>VIEW</code>s are usually registered as <code>TEMPORARY VIEW</code>s<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> instead of standard (and “persistent”) SQL <code>VIEW</code> as in MySQL.</p>
<p>At MySQL there is no notion of a “temporary view”, although other popular kinds of SQL databases do have it, <a href="https://www.postgresql.org/docs/current/sql-createview.html">such as the PostGreSQL database</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. So, a temporary view is not a exclusive concept of Spark SQL. However, is a special type of SQL <code>VIEW</code> that is not present in all popular kinds of SQL databases.</p>
<p>In other words, both Spark SQL and MySQL support the <code>CREATE VIEW</code> statement. In contrast, statements such as <code>CREATE TEMPORARY VIEW</code> and <code>CREATE OR REPLACE TEMPORARY VIEW</code> are available in Spark SQL, but not in MySQL.</p>
</section>
<section id="registering-a-spark-sql-view-in-the-spark-sql-catalog" class="level4" data-number="7.2.1.3">
<h4 data-number="7.2.1.3" class="anchored" data-anchor-id="registering-a-spark-sql-view-in-the-spark-sql-catalog"><span class="header-section-number">7.2.1.3</span> Registering a Spark SQL <code>VIEW</code> in the Spark SQL Catalog</h4>
<p>In <code>pyspark</code>, you can register a Spark DataFrame as a temporary SQL <code>VIEW</code> with the <code>createTempView()</code> or <code>createOrReplaceTempView()</code> DataFrame methods. These methods are equivalent to <code>CREATE TEMPORARY VIEW</code> and <code>CREATE OR REPLACE TEMPORARY VIEW</code> SQL statements of Spark SQL, respectively.</p>
<p>In essence, these methods register your Spark DataFrame as a temporary SQL <code>VIEW</code>, and have a single input, which is the name you want to give to this new SQL <code>VIEW</code> you are creating inside a string:</p>
<div id="b0c1b29c" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To save the `df` DataFrame as a SQL VIEW,</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use one of the methods below:</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>df.createTempView(<span class="st">'example_view'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>df.createOrReplaceTempView(<span class="st">'example_view'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After we executed the above statements, we can now access and use the <code>df</code> DataFrame in any SQL query, like in the example below:</p>
<div id="63290601" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sql_query <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT *</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="st">FROM example_view</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="st">WHERE value &gt; 20</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>spark.sql(sql_query).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-----+----------+
| id|value|      date|
+---+-----+----------+
|  1| 28.3|2021-01-01|
|  3| 20.1|2021-01-02|
+---+-----+----------+
</code></pre>
</div>
</div>
<p>So you use the <code>createTempView()</code> or <code>createOrReplaceTempView()</code> methods when you want to make a Spark DataFrame created in <code>pyspark</code> (that is, a python object), available to Spark SQL.</p>
<p>Besides that, you also have the option to create a temporary <code>VIEW</code> by using pure SQL statements trough the <code>sql()</code> method. However, when you create a temporary <code>VIEW</code> using pure SQL, you can only use (inside this <code>VIEW</code>) native SQL objects that are already stored inside your Spark SQL Context.</p>
<p>This means that you cannot make a Spark DataFrame created in python available to Spark SQL, by using a pure SQL inside the <code>sql()</code> method. To do this, you have to use the DataFrame methods <code>createTempView()</code> and <code>createOrReplaceTempView()</code>.</p>
<p>As an example, the query below uses pure SQL statements to creates the <code>active_brazilian_users</code> temporary <code>VIEW</code>, which selects an existing SQL table called <code>hubspot.user_mails</code>:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">CREATE</span> <span class="kw">TEMPORARY</span> <span class="kw">VIEW</span> active_brazilian_users <span class="kw">AS</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> hubspot.user_mails</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span> state <span class="op">==</span> <span class="st">'Active'</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">AND</span> country_location <span class="op">==</span> <span class="st">'Brazil'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Temporary <code>VIEW</code>s like the one above (which are created from pure SQL statements being executed inside the <code>sql()</code> method) are kind of unusual in Spark SQL. Because you can easily avoid the work of creating a <code>VIEW</code> by using Common Table Expression (CTE) on a <code>WITH</code> statement, like in the query below:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">WITH</span> active_brazilian_users <span class="kw">AS</span> (</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">FROM</span> hubspot.user_mails</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">WHERE</span> state <span class="op">==</span> <span class="st">'Active'</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">AND</span> country_location <span class="op">==</span> <span class="st">'Brazil'</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> A.<span class="fu">user</span>, <span class="fu">SUM</span>(sale_value), B.user_email</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> sales.sales_per_user <span class="kw">AS</span> A</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="kw">INNER</span> <span class="kw">JOIN</span> active_brazilian_users <span class="kw">AS</span> B</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="kw">GROUP</span> <span class="kw">BY</span> A.<span class="fu">user</span>, B.user_email</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Just as a another example, you can also run a SQL query that creates a persistent SQL <code>VIEW</code> (that is, without the <code>TEMPORARY</code> clause). In the example below, I am saving the simple query that I showed at the beginning of this chapter inside a <code>VIEW</code> called <code>list_of_codes</code>. This <code>CREATE VIEW</code> statement, register a persistent SQL <code>VIEW</code> in the SQL Catalog.</p>
<div id="6ac77ff2" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>sql_query <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="st">CREATE OR REPLACE VIEW list_of_codes AS</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT *</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="st">FROM (</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="st">  VALUES (11), (31), (24), (35)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="st">) AS List(Codes)</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>spark.sql(sql_query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>DataFrame[]</code></pre>
</div>
</div>
<p>Now, every time I want to use this SQL query that selects a list of codes, I can use this <code>list_of_codes</code> as a shortcut:</p>
<div id="41fa43ff" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">"SELECT * FROM list_of_codes"</span>).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----+
|Codes|
+-----+
|   11|
|   31|
|   24|
|   35|
+-----+
</code></pre>
</div>
</div>
</section>
<section id="tables-are-stored-as-physical-tables" class="level4" data-number="7.2.1.4">
<h4 data-number="7.2.1.4" class="anchored" data-anchor-id="tables-are-stored-as-physical-tables"><span class="header-section-number">7.2.1.4</span> <code>TABLEs</code> are stored as physical tables</h4>
<p>In the other hand, SQL <code>TABLEs</code> are the “opposite” of SQL <code>VIEWs</code>. That is, SQL <code>TABLEs</code> are stored as physical tables inside the SQL database. In other words, each one of the rows of your table are stored inside the SQL database.</p>
<p>Because of this characteristic, when dealing with huges amounts of data, SQL <code>TABLEs</code> are usually faster to load and transform. Because you just have to read the data stored on the database, you do not need to calculate it from scratch every time you use it.</p>
<p>But, as a collateral effect, you usually have to physically update the data inside this <code>TABLE</code>, by using, for example, <code>INSERT INTO</code> statements. In other words, when dealing with SQL <code>TABLE</code>’s you usually need to create (and manage) data pipelines that are responsible for periodically update and append new data to this SQL <code>TABLE</code>, and this might be a big burden to your process.</p>
</section>
<section id="registering-spark-sql-tables-in-the-spark-sql-catalog" class="level4" data-number="7.2.1.5">
<h4 data-number="7.2.1.5" class="anchored" data-anchor-id="registering-spark-sql-tables-in-the-spark-sql-catalog"><span class="header-section-number">7.2.1.5</span> Registering Spark SQL <code>TABLE</code>s in the Spark SQL Catalog</h4>
<p>In <code>pyspark</code>, you can register a Spark DataFrame as a SQL <code>TABLE</code> with the <code>write.saveAsTable()</code> DataFrame method. This method accepts, as first input, the name you want to give to this SQL <code>TABLE</code> inside a string.</p>
<div id="1b0b9bd0" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To save the `df` DataFrame as a SQL TABLE:</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>df.write.saveAsTable(<span class="st">'example_table'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you expect, after we registered the DataFrame as a SQL table, we can now run any SQL query over <code>example_table</code>, like in the example below:</p>
<div id="800cfdad" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">"SELECT SUM(value) FROM example_table"</span>).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>+----------+
|sum(value)|
+----------+
|      76.8|
+----------+</code></pre>
<p>You can also use pure SQL queries to create an empty SQL <code>TABLE</code> from scratch, and then, feed this table with data by using <code>INSERT INTO</code> statements. In the example below, we create a new database called <code>examples</code>, and, inside of it, a table called <code>code_brazil_states</code>. Then, we use multiple <code>INSERT INTO</code> statements to populate this table with few rows of data.</p>
<div id="c85c7034" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>all_statements <span class="op">=</span> <span class="st">'''CREATE DATABASE `examples`;</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="st">USE `examples`;</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="st">CREATE TABLE `code_brazil_states` (`code` INT, `state_name` STRING);</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="st">INSERT INTO `code_brazil_states` VALUES (31, "Minas Gerais");</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="st">INSERT INTO `code_brazil_states` VALUES (15, "Pará");</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="st">INSERT INTO `code_brazil_states` VALUES (41, "Paraná");</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="st">INSERT INTO `code_brazil_states` VALUES (25, "Paraíba");'''</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>statements <span class="op">=</span> all_statements.split(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> statement <span class="kw">in</span> statements:</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>  spark.sql(statement)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can see now this new physical SQL table using a simple query like this:</p>
<div id="9fe0fa9a" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">\</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  .sql(<span class="st">'SELECT * FROM examples.code_brazil_states'</span>)<span class="op">\</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>+----+------------+
|code|  state_name|
+----+------------+
|  41|      Paraná|
|  31|Minas Gerais|
|  15|        Pará|
|  25|     Paraíba|
+----+------------+</code></pre>
</section>
<section id="sec-sql-save-modes" class="level4" data-number="7.2.1.6">
<h4 data-number="7.2.1.6" class="anchored" data-anchor-id="sec-sql-save-modes"><span class="header-section-number">7.2.1.6</span> The different save “modes”</h4>
<p>There are other arguments that you might want to use in the <code>write.saveAsTable()</code> method, like the <code>mode</code> argument. This argument controls how Spark will save your data into the database. By default, <code>write.saveAsTable()</code> uses the <code>mode = "error"</code> by default. In this mode, Spark will look if the table you referenced already exists, before it saves your data.</p>
<p>Let’s get back to the code we showed before (which is reproduced below). In this code, we asked Spark to save our data into a table called <code>"example_table"</code>. Spark will look if a table with this name already exists. If it does, then, Spark will raise an error that will stop the process (i.e.&nbsp;no data is saved).</p>
<div id="03d3c0d6" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>df.write.saveAsTable(<span class="st">'example_table'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Raising an error when you do not want to accidentaly affect a SQL table that already exist, is a good practice. But, you might want to not raise an error in this situation. In case like this, you might want to just ignore the operation, and get on with your life. For cases like this, <code>write.saveAsTable()</code> offers the <code>mode = "ignore"</code>.</p>
<p>So, in the code example below, we are trying to save the <code>df</code> DataFrame into a table called <code>example_table</code>. But if this <code>example_table</code> already exist, Spark will just silently ignore this operation, and will not save any data.</p>
<div id="ad0bcb92" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>df.write.saveAsTable(<span class="st">'example_table'</span>, mode <span class="op">=</span> <span class="st">"ignore"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In addition, <code>write.saveAsTable()</code> offers two more different modes, which are <code>mode = "overwrite"</code> and <code>mode = "append"</code>. When you use one these two modes, Spark <strong>will always save your data</strong>, no matter if the SQL table you are trying to save into already exist or not. In essence, these two modes control whether Spark will delete or keep previous rows of the SQL table intact, before it saves any new data.</p>
<p>When you use <code>mode = "overwrite"</code>, Spark will automatically rewrite/replace the entire table with the current data of your DataFrame. In contrast, when you use <code>mode = "append"</code>, Spark will just append (or insert, or add) this data into the table. The subfigures at <a href="#fig-save-table-modes" class="quarto-xref">Figure&nbsp;<span>7.1</span></a> demonstrates these two modes visually.</p>
<div id="fig-save-table-modes" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-save-table-modes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-save-table-modes" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-mode-overwrite" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mode-overwrite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./../Figures/table-save-modes-overwrite.png" class="img-fluid figure-img" data-ref-parent="fig-save-table-modes">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mode-overwrite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Mode overwrite
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-save-table-modes" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-mode-append" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mode-append-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./../Figures/table-save-modes-append.png" class="img-fluid figure-img" data-ref-parent="fig-save-table-modes">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mode-append-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Mode append
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-save-table-modes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: How Spark saves your data with different “save modes”
</figcaption>
</figure>
</div>
<p>You can see the full list of arguments of <code>write.SaveAsTable()</code>, and their description by <a href="https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable">looking at the documentation</a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
</section>
</section>
<section id="sec-temp-persist" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="sec-temp-persist"><span class="header-section-number">7.2.2</span> Temporary versus Persistent sources</h3>
<p>When you register any Spark DataFrame as a SQL <code>TABLE</code>, it becomes a persistent source. Because the contents, the data, the rows of the table are stored on disk, inside a database, and can be accessed any time, even after you close or restart your computer (or your Spark Session). In other words, it becomes “persistent” as in the sense of “it does not die”.</p>
<p>As another example, when you save a specific SQL query as a SQL <code>VIEW</code> with the <code>CREATE VIEW</code> statement, this SQL <code>VIEW</code> is saved inside the database. As a consequence, it becomes a persistent source as well, and can be accessed and reused in other Spark Sessions, unless you explicit drop (or “remove”) this SQL <code>VIEW</code> with a <code>DROP VIEW</code> statement.</p>
<p>However, with methods like <code>createTempView()</code> and <code>createOrReplaceTempView()</code> you register your Spark DataFrame as a <em>temporary</em> SQL <code>VIEW</code>. This means that the life (or time of existence) of this <code>VIEW</code> is tied to your Spark Session. In other words, it will exist in your Spark SQL Catalog only for the duration of your Spark Session. When you close your Spark Session, this <code>VIEW</code> just dies. When you start a new Spark Session it does not exist anymore. As a result, you have to register your DataFrame again at the catalog to use it one more time.</p>
</section>
<section id="spark-sql-catalog-is-the-bridge-between-sql-and-pyspark" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="spark-sql-catalog-is-the-bridge-between-sql-and-pyspark"><span class="header-section-number">7.2.3</span> Spark SQL Catalog is the bridge between SQL and <code>pyspark</code></h3>
<p>Remember, to run SQL queries over any Spark DataFrame, you must register this DataFrame into the Spark SQL Catalog. Because of it, this Spark SQL Catalog works almost as the bridge that connects the python objects that hold your Spark DataFrames to the Spark SQL context. Without it, Spark SQL will not find your Spark DataFrames. As a result, it can not run any SQL query over it.</p>
<p>When you try to use a DataFrame that is not currently registered at the Spark SQL Catalog, Spark will automatically raise a <code>AnalysisException</code>, like in the example below:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">\</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  .sql(<span class="st">"SELECT * FROM this.does_not_exist"</span>)<span class="op">\</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>AnalysisException: Table or view not found</code></pre>
<p>The methods <code>saveAsTable()</code>, <code>createTempView()</code> and <code>createOrReplaceTempView()</code> are the main methods to register your Spark DataFrame into this Spark SQL Catalog. This means that you have to use one of these methods before you run any SQL query over your Spark DataFrame.</p>
</section>
</section>
<section id="the-penguins-dataframe" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="the-penguins-dataframe"><span class="header-section-number">7.3</span> The <code>penguins</code> DataFrame</h2>
<p>Over the next examples in this chapter, we will explore the <code>penguins</code> DataFrame. This is the <code>penguins</code> dataset from the <a href="https://allisonhorst.github.io/palmerpenguins/"><code>palmerpenguins</code> R library</a><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. It stores data of multiple measurements of penguin species from the islands in Palmer Archipelago.</p>
<p>These measurements include size (flipper length, body mass, bill dimensions) and sex, and they were collected by researchers of the Antarctica LTER program, a member of the Long Term Ecological Research Network. If you want to understand more about each field/column present in this dataset, I recommend you to read the <a href="https://allisonhorst.github.io/palmerpenguins/reference/penguins.html">official documentation of this dataset</a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<p>To get this data, you can download the CSV file called <code>penguins.csv</code> (remember that this CSV can be downloaded from the book repository<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>). In the code below, I am reading this CSV file and creating a Spark DataFrame with its data. Then, I register this Spark DataFrame as a SQL temporary view (called <code>penguins_view</code>) using the <code>createOrReplaceTempView()</code> method.</p>
<div id="b01ca11a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"../Data/penguins.csv"</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> spark.read<span class="op">\</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  .csv(path, header <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>penguins.createOrReplaceTempView(<span class="st">'penguins_view'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After these commands, I have now a SQL view called <code>penguins_view</code> registered in my Spark SQL context, which I can query it, using pure SQL:</p>
<div id="f6d78584" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">'SELECT * FROM penguins_view'</span>).show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+---------+--------------+-------------+-----------------+-----------+------+----+
|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|
+-------+---------+--------------+-------------+-----------------+-----------+------+----+
| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|
| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|
| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|
| Adelie|Torgersen|          NULL|         NULL|             NULL|       NULL|  NULL|2007|
| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|
+-------+---------+--------------+-------------+-----------------+-----------+------+----+
only showing top 5 rows
</code></pre>
</div>
</div>
</section>
<section id="selecting-your-spark-dataframes" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="selecting-your-spark-dataframes"><span class="header-section-number">7.4</span> Selecting your Spark DataFrames</h2>
<p>An obvious way to access any SQL <code>TABLE</code> or <code>VIEW</code> registered in your Spark SQL context, is to select it, through a simple <code>SELECT * FROM</code> statement, like we saw in the previous examples. However, it can be quite annoying to type “SELECT * FROM” every time you want to use a SQL <code>TABLE</code> or <code>VIEW</code> in Spark SQL.</p>
<p>That is why Spark offers a shortcut to us, which is the <code>table()</code> method of your Spark session. In other words, the code <code>spark.table("table_name")</code> is a shortcut to <code>spark.sql("SELECT * FROM table_name")</code>. They both mean the same thing. For example, we could access <code>penguins_view</code> as:</p>
<div id="af69fbad" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">\</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  .table(<span class="st">'penguins_view'</span>)<span class="op">\</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  .show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+---------+--------------+-------------+-----------------+-----------+------+----+
|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|
+-------+---------+--------------+-------------+-----------------+-----------+------+----+
| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|
| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|
| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|
| Adelie|Torgersen|          NULL|         NULL|             NULL|       NULL|  NULL|2007|
| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|
+-------+---------+--------------+-------------+-----------------+-----------+------+----+
only showing top 5 rows
</code></pre>
</div>
</div>
</section>
<section id="executing-sql-expressions" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="executing-sql-expressions"><span class="header-section-number">7.5</span> Executing SQL expressions</h2>
<p>As I noted at <a href="04-columns.html#sec-columns-related-expressions" class="quarto-xref"><span>Section 4.2</span></a>, columns of a Spark DataFrame (or objects of class <code>Column</code>) are closely related to expressions. As a result, you usually use and execute expressions in Spark when you want to transform (or mutate) columns of a Spark DataFrame.</p>
<p>This is no different for SQL expressions. A SQL expression is basically any expression you would use on the <code>SELECT</code> statement of your SQL query. As you can probably guess, since they are used in the <code>SELECT</code> statement, these expressions are used to transform columns of a Spark DataFrame.</p>
<p>There are many column transformations that are particularly verbose and expensive to write in “pure” <code>pyspark</code>. But you can use a SQL expression in your favor, to translate this transformation into a more short and concise form. Virtually any expression you write in <code>pyspark</code> can be translated into a SQL expression.</p>
<p>To execute a SQL expression, you give this expression inside a string to the <code>expr()</code> function from the <code>pyspark.sql.functions</code> module. Since expressions are used to transform columns, you normally use the <code>expr()</code> function inside a <code>withColumn()</code> or a <code>select()</code> DataFrame method, like in the example below:</p>
<div id="e582d8ae" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>spark<span class="op">\</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  .table(<span class="st">'penguins_view'</span>)<span class="op">\</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'specie_island'</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    expr(<span class="st">"CONCAT(species, '_', island)"</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  )<span class="op">\</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sex_short'</span>,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    expr(<span class="st">"CASE WHEN sex == 'male' THEN 'M' ELSE 'F' END"</span>)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>  )<span class="op">\</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>  .select(<span class="st">'specie_island'</span>, <span class="st">'sex_short'</span>)<span class="op">\</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>  .show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------------+---------+
|   specie_island|sex_short|
+----------------+---------+
|Adelie_Torgersen|        M|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
+----------------+---------+
only showing top 5 rows
</code></pre>
</div>
</div>
<p>I particulaly like to write “if-else” or “case-when” statements using a pure <code>CASE WHEN</code> SQL statement inside the <code>expr()</code> function. By using this strategy you usually get a more simple statement that translates the intention of your code in a cleaner way. But if I wrote the exact same <code>CASE WHEN</code> statement above using pure <code>pyspark</code> functions, I would end up with a shorter (but “less clean”) statement using the <code>when()</code> and <code>otherwise()</code> functions:</p>
<div id="17ed34b9" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> (</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  when, col,</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  concat, lit</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>spark<span class="op">\</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  .table(<span class="st">'penguins_view'</span>)<span class="op">\</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'specie_island'</span>,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    concat(<span class="st">'species'</span>, lit(<span class="st">'_'</span>), <span class="st">'island'</span>)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>  )<span class="op">\</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sex_short'</span>,</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    when(col(<span class="st">"sex"</span>) <span class="op">==</span> <span class="st">'male'</span>, <span class="st">'M'</span>)<span class="op">\</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>      .otherwise(<span class="st">'F'</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>  )<span class="op">\</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>  .select(<span class="st">'specie_island'</span>, <span class="st">'sex_short'</span>)<span class="op">\</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>  .show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------------+---------+
|   specie_island|sex_short|
+----------------+---------+
|Adelie_Torgersen|        M|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
|Adelie_Torgersen|        F|
+----------------+---------+
only showing top 5 rows
</code></pre>
</div>
</div>
</section>
<section id="every-dataframe-transformation-in-python-can-be-translated-into-sql" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="every-dataframe-transformation-in-python-can-be-translated-into-sql"><span class="header-section-number">7.6</span> Every DataFrame transformation in Python can be translated into SQL</h2>
<p>All DataFrame API transformations that you write in Python (using <code>pyspark</code>) can be translated into SQL queries/expressions using the Spark SQL module. Since the DataFrame API is a core part of <code>pyspark</code>, the majority of python code you write with <code>pyspark</code> can be translated into SQL queries (if you wanto to).</p>
<p>Is worth pointing out, that, no matter which language you choose (Python or SQL), they are both further compiled to the same base instructions. The end result is that the Python code you write and his SQL translated version <strong>will perform the same</strong> (they have the same efficiency), because they are compiled to the same instructions before being executed by Spark.</p>
<section id="dataframe-methods-are-usually-translated-into-sql-keywords" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="dataframe-methods-are-usually-translated-into-sql-keywords"><span class="header-section-number">7.6.1</span> DataFrame methods are usually translated into SQL keywords</h3>
<p>When you translate the methods from the python <code>DataFrame</code> class (like <code>orderBy()</code>, <code>select()</code> and <code>where()</code>) into their equivalents in Spark SQL, you usually get SQL keywords (like <code>ORDER BY</code>, <code>SELECT</code> and <code>WHERE</code>).</p>
<p>For example, if I needed to get the top 5 penguins with the biggest body mass at <code>penguins_view</code>, that had sex equal to <code>"female"</code>, and, ordered by bill length, I could run the following python code:</p>
<div id="05327a5d" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> col</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>top_5 <span class="op">=</span> penguins<span class="op">\</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    .where(col(<span class="st">'sex'</span>) <span class="op">==</span> <span class="st">'female'</span>)<span class="op">\</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    .orderBy(col(<span class="st">'body_mass_g'</span>).desc())<span class="op">\</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    .limit(<span class="dv">5</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>top_5<span class="op">\</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    .orderBy(<span class="st">'bill_length_mm'</span>)<span class="op">\</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+------+--------------+-------------+-----------------+-----------+------+----+
|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|
+-------+------+--------------+-------------+-----------------+-----------+------+----+
| Gentoo|Biscoe|          44.9|         13.3|              213|       5100|female|2008|
| Gentoo|Biscoe|          45.1|         14.5|              207|       5050|female|2007|
| Gentoo|Biscoe|          45.2|         14.8|              212|       5200|female|2009|
| Gentoo|Biscoe|          46.5|         14.8|              217|       5200|female|2008|
| Gentoo|Biscoe|          49.1|         14.8|              220|       5150|female|2008|
+-------+------+--------------+-------------+-----------------+-----------+------+----+
</code></pre>
</div>
</div>
<p>I could translate the above python code to the following SQL query:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">WITH</span> top_5 <span class="kw">AS</span> (</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> penguins_view</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">WHERE</span> sex <span class="op">==</span> <span class="st">'female'</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">ORDER</span> <span class="kw">BY</span> body_mass_g <span class="kw">DESC</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">LIMIT</span> <span class="dv">5</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> top_5</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="kw">ORDER</span> <span class="kw">BY</span> bill_length_mm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Again, to execute the above SQL query inside <code>pyspark</code> we need to give this query as a string to the <code>sql()</code> method of our Spark Session, like this:</p>
<div id="d8f50346" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="st">WITH top_5 AS (</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="st">    SELECT *</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="st">    FROM penguins_view</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="st">    WHERE sex == 'female'</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="st">    ORDER BY body_mass_g DESC</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="st">    LIMIT 5</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="st">)</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="st">SELECT *</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="st">FROM top_5</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="st">ORDER BY bill_length_mm</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The same result of the example above</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>spark.sql(query).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-------+------+--------------+-------------+-----------------+-----------+------+----+
|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|
+-------+------+--------------+-------------+-----------------+-----------+------+----+
| Gentoo|Biscoe|          44.9|         13.3|              213|       5100|female|2008|
| Gentoo|Biscoe|          45.1|         14.5|              207|       5050|female|2007|
| Gentoo|Biscoe|          45.2|         14.8|              212|       5200|female|2009|
| Gentoo|Biscoe|          46.5|         14.8|              217|       5200|female|2008|
| Gentoo|Biscoe|          49.1|         14.8|              220|       5150|female|2008|
+-------+------+--------------+-------------+-----------------+-----------+------+----+
</code></pre>
</div>
</div>
</section>
<section id="sec-sql-expr" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="sec-sql-expr"><span class="header-section-number">7.6.2</span> Spark functions are usually translated into SQL functions</h3>
<p>Every function from the <code>pyspark.sql.functions</code> module you might use to describe your transformations in python, can be directly used in Spark SQL. In other words, every Spark function that is accesible in python, is also accesible in Spark SQL.</p>
<p>When you translate these python functions into SQL, they usually become a pure SQL function with the same name. For example, if I wanted to use the <code>regexp_extract()</code> python function, from the <code>pyspark.sql.functions</code> module in Spark SQL, I just use the <code>REGEXP_EXTRACT()</code> SQL function. The same occurs to any other function, like the <code>to_date()</code> function for example.</p>
<div id="ba5247f4" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> to_date, regexp_extract</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="co"># `df1` and `df2` are both equal. Because they both</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># use the same `to_date()` and `regexp_extract()` functions</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> (spark</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>  .table(<span class="st">'penguins_view'</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'extract_number'</span>,</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    regexp_extract(<span class="st">'bill_length_mm'</span>, <span class="st">'[0-9]+'</span>, <span class="dv">0</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">'date'</span>, to_date(<span class="st">'year'</span>, <span class="st">'y'</span>))</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>  .select(</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bill_length_mm'</span>, <span class="st">'year'</span>,</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'extract_number'</span>, <span class="st">'date'</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> (spark</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>  .table(<span class="st">'penguins_view'</span>)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>  .withColumn(</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'extract_number'</span>,</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    expr(<span class="st">"REGEXP_EXTRACT(bill_length_mm, '[0-9]+', 0)"</span>)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>  .withColumn(<span class="st">'date'</span>, expr(<span class="st">"TO_DATE(year, 'y')"</span>))</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>  .select(</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bill_length_mm'</span>, <span class="st">'year'</span>,</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'extract_number'</span>, <span class="st">'date'</span></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>df2.show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------+----+--------------+----------+
|bill_length_mm|year|extract_number|      date|
+--------------+----+--------------+----------+
|          39.1|2007|            39|2007-01-01|
|          39.5|2007|            39|2007-01-01|
|          40.3|2007|            40|2007-01-01|
|          NULL|2007|          NULL|2007-01-01|
|          36.7|2007|            36|2007-01-01|
+--------------+----+--------------+----------+
only showing top 5 rows
</code></pre>
</div>
</div>
<p>This is very handy. Because for every new python function from the <code>pyspark.sql.functions</code> module, that you learn how to use, you automatically learn how to use in Spark SQL as well, because is the same function, with the basically the same name and arguments.</p>
<p>As an example, I could easily translate the above transformations that use the <code>to_date()</code> and <code>regexp_extract()</code> python functions, into the following SQL query (that I could easily execute trough the <code>sql()</code> Spark Session method):</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  bill_length_mm, <span class="dt">year</span>,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  REGEXP_EXTRACT(bill_length_mm, <span class="st">'[0-9]+'</span>, <span class="dv">0</span>) <span class="kw">AS</span> extract_number,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">TO_DATE</span>(<span class="dt">year</span>, <span class="st">'y'</span>) <span class="kw">AS</span> <span class="dt">date</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> penguins_view</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-sparkdoc" class="csl-entry" role="listitem">
<em>Apache Spark Official Documentation</em>. 2022. Documentation for Apache Spark 3.2.1. <a href="https://spark.apache.org/docs/latest/">https://spark.apache.org/docs/latest/</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>There are some very good materials explaining what is the Spark SQL Catalog, and which is the purpose of it. For a soft introduction, I recommend Sarfaraz Hussain post: <a href="https://medium.com/@sarfarazhussain211/metastore-in-apache-spark-9286097180a4" class="uri">https://medium.com/@sarfarazhussain211/metastore-in-apache-spark-9286097180a4</a>. For a more technical introduction, see <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Catalog.html" class="uri">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Catalog.html</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>You can learn more about why this specific option is necessary by looking at this StackOverflow post: <a href="https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error" class="uri">https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://dev.mysql.com/doc/refman/8.0/en/create-view.html" class="uri">https://dev.mysql.com/doc/refman/8.0/en/create-view.html</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I will explain more about the meaning of “temporary” at <a href="#sec-temp-persist" class="quarto-xref"><span>Section 7.2.2</span></a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://www.postgresql.org/docs/current/sql-createview.html" class="uri">https://www.postgresql.org/docs/current/sql-createview.html</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable" class="uri">https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://allisonhorst.github.io/palmerpenguins/" class="uri">https://allisonhorst.github.io/palmerpenguins/</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://allisonhorst.github.io/palmerpenguins/reference/penguins.html" class="uri">https://allisonhorst.github.io/palmerpenguins/reference/penguins.html</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://github.com/pedropark99/Introd-pyspark/tree/main/Data" class="uri">https://github.com/pedropark99/Introd-pyspark/tree/main/Data</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Chapters/07-import.html" class="pagination-link" aria-label="Importing data to Spark">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Importing data to Spark</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Chapters/08-transforming2.html" class="pagination-link" aria-label="Transforming your Spark DataFrame - Part 2">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 2</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>