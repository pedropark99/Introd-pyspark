[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "Welcome\nWelcome! This is the initial page for the “Open Access” HTML version of the book “Introduction to pyspark”, written by Pedro Duarte Faria. This book provides an introduction to pyspark, which is a python API to Apache Spark.\nThis book is still under active construction. This means that not all chapters are ready yet, and some of its current contents might change in the close future.\nIn essence, pyspark is a python package that provides an API for Apache Spark. In other words, with pyspark you are able to use the python language to write Spark applications and run them on a Spark cluster in a scalable and elegant way. This book focus on teaching the fundamentals of pyspark, and how to use it for big data analysis.\nThis book, also contains a small introduction to key python concepts that are important to understand how pyspark is organized. Since we will be using Apache Spark under the hood, it is also very important to understand a little bit of how Apache Spark works, so, we provide a small introduction to Apache Spark as well.\nBig part of the knowledge exposed here is extracted from a lot of practical experience of the author, working with pyspark to analyze big data at platforms such as Databricks1. Another part of the knowledge is extracted from the official documentation of Apache Spark (Apache Spark Official Documentation 2022), as well as some established works such as Chambers and Zaharia (2018) and Damji et al. (2020).\nSome of the main subjects discussed in the book are:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Introduction to pyspark",
    "section": "About the author",
    "text": "About the author\nPedro Duarte Faria have a bachelor degree in Economics from Federal University of Ouro Preto - Brazil. Currently, he is a Data Engineer at Blip, and an Associate Developer for Apache Spark 3.0 certified by Databricks.\nThe author have more than 3 years of experience in the data analysis market. He developed data pipelines, reports and analysis for research institutions and some of the largest companies in the brazilian financial sector, such as the BMG Bank, Sodexo and Pan Bank, besides dealing with databases that go beyond the billion rows.\nFurthermore, Pedro is specialized on the R programming language, and have given several lectures and courses about it, inside graduate centers (such as PPEA-UFOP2), in addition to federal and state organizations (such as FJP-MG3). As researcher, he have experience in the field of Science, Technology and Innovation Economics.\nPersonal Website: https://pedro-faria.netlify.app/\nTwitter: @PedroPark9\nMastodon: @pedropark99@fosstodon.org",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#some-conventions-of-this-book",
    "href": "index.html#some-conventions-of-this-book",
    "title": "Introduction to pyspark",
    "section": "Some conventions of this book",
    "text": "Some conventions of this book\n\nPython code and terminal commands\nThis book is about pyspark, which is a python package. As a result, we will be exposing a lot of python code across the entire book. Examples of python code, are always shown inside a gray rectangle, like this example below.\nEvery visible result that this python code produce, will be written in plain black outside of the gray rectangle, just below the command that produced that visible result. So in the example below, the value 729 is the only visible result of this python code, and, the statement print(y) is the command that triggered this visible result.\nx = 3\ny = 9 ** x\n\nprint(y)\n729\nFurthermore, all terminal commands that we expose in this book, will always be: pre-fixed by Terminal$; written in black; and, not outlined by a gray rectangle. In the example below, the command pip install jupyter should be inserted in the terminal of the OS (whatever is the terminal that your OS uses), and not in the python interpreter, because this command is prefixed with Terminal$.\nTerminal$ pip install jupyter\nSome terminal commands may produce visible results as well. In that case, these results will be right below the respective command, and will not be pre-fixed with Terminal$. For example, we can see below that the command echo \"Hello!\" produces the result \"Hello!\".\nTerminal$ echo \"Hello!\"\nHello!\n\n\nPython objects, functions and methods\nWhen I refer to some python object, function, method or package, I will use a monospaced font. In other words, if I have a python object called “name”, and, I am describing this object, I will use name in the paragraph, and not “name”. The same logic applies to Python functions, methods and package names.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#be-aware-of-differences-between-oss",
    "href": "index.html#be-aware-of-differences-between-oss",
    "title": "Introduction to pyspark",
    "section": "Be aware of differences between OS’s!",
    "text": "Be aware of differences between OS’s!\nSpark is available for all three main operational systems (or OS’s) used in the world (Windows, MacOs and Linux). I will use constantly the word OS as an abbreviation to “operational system”.\nThe snippets of python code shown throughout this book should just run correctly no matter which one of the three OS’s you are using. In other words, the python code snippets are made to be portable. So you can just copy and paste them to your computer, no matter which OS you are using.\nBut, at some points, I may need to show you some terminal commands that are OS specific, and are not easily portable. For example, Linux have a package manager, but Windows does not have one. This means that, if you are on Linux, you will need to use some terminal commands to install some necessary programs (like python). In contrast, if you are on Windows, you will generally download executable files (.exe) that make this installation for you.\nIn cases like this, I will always point out the specific OS of each one of the commands, or, I will describe the necessary steps to be made on each one the OS’s. Just be aware that these differences exists between the OS’s.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#install-the-necessary-software",
    "href": "index.html#install-the-necessary-software",
    "title": "Introduction to pyspark",
    "section": "Install the necessary software",
    "text": "Install the necessary software\nIf you want to follow the examples shown throughout this book, you must have Apache Spark and pyspark installed on your machine. If you do not know how to do this, you can consult the articles from phoenixNAP which are very useful4.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#books-metadata",
    "href": "index.html#books-metadata",
    "title": "Introduction to pyspark",
    "section": "Book’s metadata",
    "text": "Book’s metadata\n\nLicense\nCopyright © 2024 Pedro Duarte Faria. This book is licensed by the CC-BY 4.0 Creative Commons Attribution 4.0 International Public License.\n\n\n\nBook citation\nYou can use the following BibTex entry to cite this book:\n@book{pedro2024,\n    author = {Pedro Duarte Faria},\n    title = {Introduction to pyspark},\n    month = {January},\n    year = {2024},\n    address = {Belo Horizonte}\n}\n\n\nCorresponding author and maintainer\nPedro Duarte Faria\nContact: pedropark99@gmail.com\nPersonal website: https://pedro-faria.netlify.app/\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1. https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nDamji, Jules, Brooke Wenig, Tathagata Das, and Denny Lee. 2020. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "https://databricks.com/↩︎\nhttps://ppea.ufop.br/↩︎\nhttp://fjp.mg.gov.br/↩︎\nhttps://phoenixnap.com/kb/install-spark-on-ubuntu.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#scripts",
    "href": "Chapters/02-python.html#scripts",
    "title": "1  Key concepts of python",
    "section": "1.1 Scripts",
    "text": "1.1 Scripts\nPython programs are written in plain text files that are saved with the .py extension. After you save these files, they are usually called “scripts”. So a script is just a text file that contains all the commands that make your python program.\nThere are many IDEs or programs that help you to write, manage, run and organize this kind of files (like Microsoft Visual Studio Code1, PyCharm2, Anaconda3 and RStudio4). Many of these programs are free to use, and, are easy to install.\nBut, if you do not have any of them installed, you can just create a new plain text file from the built-in Notepad program of your OS (operational system), and, save it with the .py extension.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#how-to-run-a-python-program",
    "href": "Chapters/02-python.html#how-to-run-a-python-program",
    "title": "1  Key concepts of python",
    "section": "1.2 How to run a python program",
    "text": "1.2 How to run a python program\nAs you learn to write your Spark applications with pyspark, at some point, you will want to actually execute this pyspark program, to see its result. To do so, you need to execute it as a python program. There are many ways to run a python program, but I will show you the more “standard” way. That is to use the python command inside the terminal of your OS (you need to have python already installed).\nAs an example, lets create a simple “Hello world” program. First, open a new text file then save it somewhere in your machine (with the name hello.py). Remember to save the file with the .py extension. Then copy and paste the following command into this file:\n\nprint(\"Hello World!\")\n\nIt will be much easier to run this script, if you open your OS’s terminal inside the folder where you save the hello.py file. After you opened the terminal inside the folder, just run the python3 hello.py command. As a result, python will execute hello.py, and, the text Hello World! should be printed to the terminal:\nTerminal$ python3 hello.py\nHello World!\nBut, if for some reason you could not open the terminal inside the folder, just open a terminal (in any way you can), then, use the cd command (stands for “change directory”) with the path to the folder where you saved hello.py. This way, your terminal will be rooted in this folder.\nFor example, if I saved hello.py inside my Documents folder, the path to this folder in Windows would be something like this: \"C:\\Users\\pedro\\Documents\". On the other hand, this path on Linux would be something like \"/usr/pedro/Documents\". So the command to change to this directory would be:\n# On Windows:\nTerminal$ cd \"C:\\Users\\pedro\\Documents\"\n# On Linux:\nTerminal$ cd \"/usr/pedro/Documents\"\nAfter this cd command, you can run the python hello.py command in the terminal, and get the exact same result of the previous example.\nThere you have it! So every time you need to run your python program (or your pyspark program), just open a terminal and run the command python &lt;complete path to your script&gt;. If the terminal is rooted on the folder where you saved your script, you can just use the python &lt;name of the script&gt; command.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#objects",
    "href": "Chapters/02-python.html#objects",
    "title": "1  Key concepts of python",
    "section": "1.3 Objects",
    "text": "1.3 Objects\nAlthough python is a general-purpose language, most of its features are focused on object-oriented programming. Meaning that, python is a programming language focused on creating, managing and modifying objects and classes of objects.\nSo, when you work with python, you are basically applying many operations and functions over a set of objects. In essence, an object in python, is a name that refers to a set of data. This data can be anything that you computer can store (or represent).\nHaving that in mind, an object is just a name, and this name is a reference, or a key to access some data. To define an object in python, you must use the assignment operator, which is the equal sign (=). In the example below, we are defining, or, creating an object called x, and it stores the value 10. Therefore, with the name x we can access this value of 10.\n\nx = 10\nprint(x)\n\n10\n\n\nWhen we store a value inside an object, we can easily reuse this value in multiple operations or expressions:\n\n# Multiply by 2\nprint(x * 2)\n\n20\n\n\n\n# Divide by 3\nprint(x / 3)\n\n3.3333333333333335\n\n\n\n# Print its class\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n\nRemember, an object can store any type of value, or any type of data. For example, it can store a single string, like the object salutation below:\n\nsalutation = \"Hello! My name is Pedro\"\n\nOr, a list of multiple strings:\n\nnames = [\n  \"Anne\", \"Vanse\", \"Elliot\",\n  \"Carlyle\", \"Ed\", \"Memphis\"\n]\n\nprint(names)\n\n['Anne', 'Vanse', 'Elliot', 'Carlyle', 'Ed', 'Memphis']\n\n\nOr a dict containing the description of a product:\n\nproduct = {\n  'name': 'Coca Cola',\n  'volume': '2 litters',\n  'price': 2.52,\n  'group': 'non-alcoholic drinks',\n  'department': 'drinks'\n}\n\nprint(product)\n\n{'name': 'Coca Cola', 'volume': '2 litters', 'price': 2.52, 'group': 'non-alcoholic drinks', 'department': 'drinks'}\n\n\nAnd many other things…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#expressions",
    "href": "Chapters/02-python.html#expressions",
    "title": "1  Key concepts of python",
    "section": "1.4 Expressions",
    "text": "1.4 Expressions\nPython programs are organized in blocks of expressions (or statements). A python expression is a statement that describes an operation to be performed by the program. For example, the expression below describes the sum between 3 and 5.\n\n3 + 5\n\n8\n\n\nThe expression above is composed of numbers (like 3 and 5) and a operator, more specifically, the sum operator (+). But any python expression can include a multitude of different items. It can be composed of functions (like print(), map() and str()), constant strings (like \"Hello World!\"), logical operators (like !=, &lt;, &gt; and ==), arithmetic operators (like *, /, **, %, - and +), structures (like lists, arrays and dicts) and many other types of commands.\nBelow we have a more complex example, that contains the def keyword (which starts a function definition; in the example below, this new function being defined is double()), many built-in functions (list(), map() and print()), a arithmetic operator (*), numbers and a list (initiated by the pair of brackets - []).\n\ndef double(x):\n  return x * 2\n  \nprint(list(map(double, [4, 2, 6, 1])))\n\n[8, 4, 12, 2]\n\n\nPython expressions are evaluated in a sequential manner (from top to bottom of your python file). In other words, python runs the first expression in the top of your file, them, goes to the second expression, and runs it, them goes to the third expression, and runs it, and goes on and on in that way, until it hits the end of the file. So, in the example above, python executes the function definition (initiated at def double(x):), before it executes the print() statement, because the print statement is below the function definition.\nThis order of evaluation is commonly referred as “control flow” in many programming languages. Sometimes, this order can be a fundamental part of the python program. Meaning that, sometimes, if we change the order of the expressions in the program, we can produce unexpected results (like an error), or change the results produced by the program.\nAs an example, the program below prints the result 4, because the print statement is executed before the expression x = 40.\n\nx = 1\n\nprint(x * 4)\n\nx = 40\n\n4\n\n\nBut, if we execute the expression x = 40 before the print statement, we then change the result produced by the program.\n\nx = 1\nx = 40\n\nprint(x * 4)\n\n160\n\n\nIf we go a little further, and, put the print statement as the first expression of the program, we then get a name error. This error warns us that, the object named x is not defined (i.e. it does not exist).\n\nprint(x * 4)\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nNameError: name 'x' is not defined\n\nx = 1\nx = 40\n\nThis error occurs, because inside the print statement, we call the name x. But, this is the first expression of the program, and at this point of the program, we did not defined a object called x. We make this definition, after the print statement, with x = 1 and x = 40. In other words, at this point, python do not know any object called x.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#packages-or-libraries",
    "href": "Chapters/02-python.html#packages-or-libraries",
    "title": "1  Key concepts of python",
    "section": "1.5 Packages (or libraries)",
    "text": "1.5 Packages (or libraries)\nA python package (or a python “library”) is basically a set of functions and classes that provides important functionality to solve a specific problem. And pyspark is one of these many python packages available.\nPython packages are usually published (that is, made available to the public) through the PyPI archive5. If a python package is published in PyPI, then, you can easily install it through the pip tool.\nTo use a python package, you always need to: 1) have this package installed on your machine; 2) import this package in your python script. If a package is not installed in your machine, you will face a ModuleNotFoundError as you try to use it, like in the example below.\n\nimport pandas\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'pandas'\nIf your program produce this error, is very likely that you are trying to use a package that is not currently installed on your machine. To install it, you may use the pip install &lt;name of the package&gt; command on the terminal of your OS.\npip install pandas\nBut, if this package is already installed in your machine, then, you can just import it to your script. To do this, you just include an import statement at the start of your python file. For example, if I want to use the DataFrame function from the pandas package:\n\n# Now that I installed the `pandas` package with `pip`\n# this `import` statement works without any errors:\nimport pandas\n\ndf = pandas.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nTherefore, with import pandas I can access any of the functions available in the pandas package, by using the dot operator after the name of the package (pandas.&lt;name of the function&gt;). However, it can become very annoying to write pandas. every time you want to access a function from pandas, specially if you use it constantly in your code.\nTo make life a little easier, python offers some alternative ways to define this import statement. First, you can give an alias to this package that is shorter/easier to write. As an example, nowadays, is virtually a industry standard to import the pandas package as pd. To do this, you use the as keyword in your import statement. This way, you can access the pandas functionality with pd.&lt;name of the function&gt;:\n\nimport pandas as pd\n\ndf = pd.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nIn contrast, if you want to make your life even easier and produce a more “clean” code, you can import (from the package) just the functions that you need to use. In this method, you can eliminate the dot operator, and refer directly to the function by its name. To use this method, you include the from keyword in your import statement, like this:\n\nfrom pandas import DataFrame\n\ndf = DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nJust to be clear, you can import multiple functions from the package, by listing them. Or, if you prefer, you can import all components of the package (or module/sub-module) by using the star shortcut (*):\n\n# Import `search()`, `match()` and `compile()` functions:\nfrom re import search, match, compile\n# Import all functions from the `os` package\nfrom os import *\n\nSome packages may be very big, and includes many different functions and classes. As the size of the package becomes bigger and bigger, developers tend to divide this package in many “modules”. In other words, the functions and classes of this python package are usually organized in “modules”.\nAs an example, the pyspark package is a fairly large package, that contains many classes and functions. Because of it, the package is organized in a number of modules, such as sql (to access Spark SQL), pandas (to access the Pandas API of Spark), ml (to access Spark MLib).\nTo access the functions available in each one of these modules, you use the dot operator between the name of the package and the name of the module. For example, to import all components from the sql and pandas modules of pyspark, you would do this:\n\nfrom pyspark.sql import *\nfrom pyspark.pandas import *\n\nGoing further, we can have sub-modules (or modules inside a module) too. As an example, the sql module of pyspark have the functions and window sub-modules. To access these sub-modules, you use the dot operator again:\n\n# Importing `functions` and `window` sub-modules:\nimport pyspark.sql.functions as F\nimport pyspark.sql.window as W",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#methods-versus-functions",
    "href": "Chapters/02-python.html#methods-versus-functions",
    "title": "1  Key concepts of python",
    "section": "1.6 Methods versus Functions",
    "text": "1.6 Methods versus Functions\nBeginners tend mix these two types of functions in python, but they are not the same. So lets describe the differences between the two.\nStandard python functions, are functions that we apply over an object. A classical example, is the print() function. You can see in the example below, that we are applying print() over the result object.\n\nresult = 10 + 54\nprint(result)\n\n64\n\n\nOther examples of a standard python function would be map() and list(). See in the example below, that we apply the map() function over a set of objects:\n\nwords = ['apple', 'star', 'abc']\nlengths = map(len, words)\nlist(lengths)\n\n[5, 4, 3]\n\n\nIn contrast, a python method is a function registered inside a python class. In other words, this function belongs to the class itself, and cannot be used outside of it. This means that, in order to use a method, you need to have an instance of the class where it is registered.\nFor example, the startswith() method belongs to the str class (this class is used to represent strings in python). So to use this method, we need to have an instance of this class saved in a object that we can access. Note in the example below, that we access the startswith() method through the name object. This means that, startswith() is a function. But, we cannot use it without an object of class str, like name.\n\nname = \"Pedro\"\nname.startswith(\"P\")\n\nTrue\n\n\nNote in the example above, that we access any class method in the same way that we would access a sub-module/module of a package. That is, by using the dot operator (.).\nSo, if we have a class called people, and, this class has a method called location(), we can use this location() method by using the dot operator (.) with the name of an object of class people. If an object called x is an instance of people class, then, we can do x.location().\nBut if this object x is of a different class, like int, then we can no longer use the location() method, because this method does not belong to the int class. For example, if your object is from class A, and, you try to use a method of class B, you will get an AttributeError.\nIn the example exposed below, I have an object called number of class int, and, I try to use the method startswith() from str class with this object:\n\nnumber = 2\n# You can see below that, the `x` object have class `int`\ntype(number)\n# Trying to use a method from `str` class\nnumber.startswith(\"P\")\n\nAttributeError: 'int' object has no attribute 'startswith'",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "href": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "title": "1  Key concepts of python",
    "section": "1.7 Identifying classes and their methods",
    "text": "1.7 Identifying classes and their methods\nOver the next chapters, you will realize that pyspark programs tend to use more methods than standard functions. So most of the functionality of pyspark resides in class methods. As a result, the capability of understanding the objects that you have in your python program, and, identifying its classes and methods will be crucial while you are developing and debugging your Spark applications.\nEvery existing object in python represents an instance of a class. In other words, every object in python is associated to a given class. You can always identify the class of an object, by applying the type() function over this object. In the example below, we can see that, the name object is an instance of the str class.\n\nname = \"Pedro\"\ntype(name)\n\nstr\n\n\nIf you do not know all the methods that a class have, you can always apply the dir() function over this class to get a list of all available methods. For example, lets suppose you wanted to see all methods from the str class. To do so, you would do this:\n\ndir(str)\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__',\n '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',\n '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__',\n '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', \n '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__',\n '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', \n '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center',\n 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format',\n 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal',\n 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable',\n 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', \n 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', \n 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith',\n 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/02-python.html#footnotes",
    "href": "Chapters/02-python.html#footnotes",
    "title": "1  Key concepts of python",
    "section": "",
    "text": "https://code.visualstudio.com/↩︎\nhttps://www.jetbrains.com/pycharm/↩︎\nhttps://www.anaconda.com/products/distribution↩︎\nhttps://www.rstudio.com/↩︎\nhttps://pypi.org/↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Key concepts of python</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#what-is-spark",
    "href": "Chapters/03-spark.html#what-is-spark",
    "title": "2  Introducing Apache Spark",
    "section": "2.1 What is Spark?",
    "text": "2.1 What is Spark?\nSpark is a multi-language engine for large-scale data processing that supports both single-node machines and clusters of machines (Apache Spark Official Documentation 2022). Nowadays, Spark became the de facto standard for structure and manage big data applications.\nIt has a number of features that its predecessors did not have, like the capacity for in-memory processing and stream processing (Karau et al. 2015). But, the most important feature of all, is that Spark is an unified platform for big data processing (Chambers and Zaharia 2018).\nThis means that Spark comes with multiple built-in libraries and tools that deals with different aspects of the work with big data. It has a built-in SQL engine1 for performing large-scale data processing; a complete library for scalable machine learning (MLib2); a stream processing engine3 for streaming analytics; and much more;\nIn general, big companies have many different data necessities, and as a result, the engineers and analysts may have to combine and integrate many tools and techniques together, so they can build many different data pipelines to fulfill these necessities. But this approach can create a very serious dependency problem, which imposes a great barrier to support this workflow. This is one of the big reasons why Spark got so successful. It eliminates big part of this problem, by already including almost everything that you might need to use.\n\nSpark is designed to cover a wide range of workloads that previously required separate distributed systems … By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools (Karau et al. 2015).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#spark-application",
    "href": "Chapters/03-spark.html#spark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.2 Spark application",
    "text": "2.2 Spark application\nYour personal computer can do a lot of things, but, it cannot efficiently deal with huge amounts of data. For this situation, we need several machines working together, adding up their resources to deal with the volume or complexity of the data. Spark is the framework that coordinates the computations across this set of machines (Chambers and Zaharia 2018). Because of this, a relevant part of Spark’s structure is deeply connected to distributed computing models.\nYou probably do not have a cluster of machines at home. So, while following the examples in this book, you will be running Spark on a single machine (i.e. single node mode). But lets just forget about this detail for a moment.\nIn every Spark application, you always have a single machine behaving as the driver node, and multiple machines behaving as the worker nodes. The driver node is responsible for managing the Spark application, i.e. asking for resources, distributing tasks to the workers, collecting and compiling the results, …. The worker nodes are responsible for executing the tasks that are assigned to them, and they need to send the results of these tasks back to the driver node.\nEvery Spark application is distributed into two different and independent processes: 1) a driver process; 2) and a set of executor processes (Chambers and Zaharia 2018). The driver process, or, the driver program, is where your application starts, and it is executed by the driver node. This driver program is responsible for: 1) maintaining information about your Spark Application; 2) responding to a user’s program or input; 3) and analyzing, distributing, and scheduling work across the executors (Chambers and Zaharia 2018).\nEvery time a Spark application starts, the driver process has to communicate with the cluster manager, to acquire workers to perform the necessary tasks. In other words, the cluster manager decides if Spark can use some of the resources (i.e. some of the machines) of the cluster. If the cluster manager allow Spark to use the nodes it needs, the driver program will break the application into many small tasks, and will assign these tasks to the worker nodes.\nThe executor processes, are the processes that take place within each one of the worker nodes. Each executor process is composed of a set of tasks, and the worker node is responsible for performing and executing these tasks that were assigned to him, by the driver program. After executing these tasks, the worker node will send the results back to the driver node (or the driver program). If they need, the worker nodes can communicate with each other, while performing its tasks.\nThis structure is represented in Figure 2.1:\n\n\n\n\n\n\nFigure 2.1: Spark application structure on a cluster of computers\n\n\n\nWhen you run Spark on a cluster of computers, you write the code of your Spark application (i.e. your pyspark code) on your (single) local computer, and then, submit this code to the driver node. After that, the driver node takes care of the rest, by starting your application, creating your Spark Session, asking for new worker nodes, sending the tasks to be performed, collecting and compiling the results and giving back these results to you.\nHowever, when you run Spark on your (single) local computer, the process is very similar. But, instead of submitting your code to another computer (which is the driver node), you will submit to your own local computer. In other words, when Spark is running on single-node mode, your computer becomes the driver and the worker node at the same time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#spark-application-versus-pyspark-application",
    "href": "Chapters/03-spark.html#spark-application-versus-pyspark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.3 Spark application versus pyspark application",
    "text": "2.3 Spark application versus pyspark application\nThe pyspark package is just a tool to write Spark applications using the python programming language. This means, that every pyspark application is a Spark application written in python.\nWith this conception in mind, you can understand that a pyspark application is a description of a Spark application. When we compile (or execute) our python program, this description is translated into a raw Spark application that will be executed by Spark.\nTo write a pyspark application, you write a python script that uses the pyspark library. When you execute this python script with the python interpreter, the application will be automatically converted to Spark code, and will be sent to Spark to be executed across the cluster;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#core-parts-of-a-pyspark-program",
    "href": "Chapters/03-spark.html#core-parts-of-a-pyspark-program",
    "title": "2  Introducing Apache Spark",
    "section": "2.4 Core parts of a pyspark program",
    "text": "2.4 Core parts of a pyspark program\nIn this section, I want to point out the core parts that composes every pyspark program. This means that every pyspark program that you write will have these “core parts”, which are:\n\nimporting the pyspark package (or modules);\nstarting your Spark Session;\ndefining a set of transformations and actions over Spark DataFrames;\n\n\n2.4.1 Importing the pyspark package (or modules)\nSpark comes with a lot of functionality installed. But, in order to use it in your pyspark program, you have to import most of these functionalities to your session. This means that you have to import specific packages (or “modules”) of pyspark to your python session.\nFor example, most of the functions used to define our transformations and aggregations in Spark DataFrames, comes from the pyspark.sql.functions module.\nThat is why we usually start our python scripts by importing functions from this module, like this:\n\nfrom pyspark.sql.functions import sum, col\nsum_expr = sum(col('Value'))\n\nOr, importing the entire module with the import keyword, like this:\n\nimport pyspark.sql.functions as F\nsum_expr = F.sum(F.col('Value'))\n\n\n\n2.4.2 Starting your Spark Session\nEvery Spark application starts with a Spark Session. Basically, the Spark Session is the entry point to your application. This means that, in every pyspark program that you write, you should always start by defining your Spark Session. We do this, by using the getOrCreate() method from pyspark.sql.SparkSession.builder module.\nJust store the result of this method in any python object. Is very common to name this object as spark, like in the example below. This way, you can access all the information and methods of Spark from this spark object.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n\n\n2.4.3 Defining a set of transformations and actions\nEvery pyspark program is composed by a set of transformations and actions over a set of Spark DataFrames.\nI will explain Spark DataFrames in more deth on the Chapter 3. For now just understand that they are the basic data sctructure that feed all pyspark programs. In other words, on every pyspark program we are transforming multiple Spark DataFrames to get the result we want.\nAs an example, in the script below we begin with the Spark DataFrame stored in the object students, and, apply multiple transformations over it to build the ar_department DataFrame. Lastly, we apply the .show() action over the ar_department DataFrame:\n\nfrom pyspark.sql.functions import col\n# Apply some transformations over\n# the `students` DataFrame:\nar_department = students\\\n  .filter(col('Age') &gt; 22)\\\n  .withColumn('IsArDepartment', col('Department') == 'AR')\\\n  .orderBy(col('Age').desc())\n  \n  \n# Apply the `.show()` action\n# over the `ar_department` DataFrame:\nar_department.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#building-your-first-spark-application",
    "href": "Chapters/03-spark.html#building-your-first-spark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.5 Building your first Spark application",
    "text": "2.5 Building your first Spark application\nTo demonstrate what a pyspark program looks like, lets write and run our first example of a Spark application. This Spark application will build a simple table of 1 column that contains 5 numbers, and then, it will return a simple python list containing this five numbers as a result.\n\n2.5.1 Writing the code\nFirst, create a new blank text file in your computer, and save it somewhere with the name spark-example.py. Do not forget to put the .py extension in the name. This program we are writing together is a python program, and should be treated as such. With the .py extension in the name file, you are stating this fact quite clearly to your computer.\nAfter you created and saved the python script (i.e. the text file with the .py extension), you can start writing your pyspark program. As we noted in the previous section, you should always start your pyspark program by defining your Spark Session, with this code:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nAfter you defined your Spark Session, and saved it in an object called spark, you can now access all Spark’s functionality through this spark object.\nTo create our first Spark table we use the range() method from the spark object. The range() method works similarly as the standard python function called range(). It basically creates a sequence of numbers, from 0 to \\(n - 1\\). However, this range() method from spark stores this sequence of numbers as rows in a Spark table (or a Spark DataFrame):\n\ntable = spark.range(5)\n\nAfter this step, we want to collect all the rows of the resulting table into a python list. And to do that, we use the collect() method from the Spark table:\n\nresult = table.collect()\nprint(result)\n\nSo, the entire program is composed of these three parts (or sections) of code. If you need it, the entire program is reproduced below. You can copy and paste all of this code to your python script, and then, save it:\n\n# The entire program:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ntable = spark.range(5)\nresult = table.collect()\nprint(result)\n\n\n\n2.5.2 Executing the code\nNow that you have written your first Spark application with pyspark, you want to execute this application and see its results. Yet, to run a pyspark program, remember that you need to have the necessary software installed on your machine. In case you do not have Apache Spark installed yet, I personally recommend you to read the articles from PhoenixNAP on how to install Apache Spark4.\nAnyway, to execute this pyspark that you wrote, you need send this script to the python interpreter, and to do this you need to: 1) open a terminal inside the folder where you python script is stored; and, 2) use the python command from the terminal with the name of your python script.\nIn my current situation, I running Spark on a Ubuntu distribution, and, I saved the spark-example.py script inside a folder called SparkExample. This means that, I need to open a terminal that is rooted inside this SparkExample folder.\nYou probably have saved your spark-example.py file in a different folder of your computer. This means that you need to open the terminal from a different folder.\nAfter I opened a terminal rooted inside the SparkExample folder. I just use the python3 command to access the python interpreter, and, give the name of the python script that I want to execute. In this case, the spark-example.py file. As a result, our first pyspark program will be executed:\nTerminal$ python3 spark-example.py\n[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\nYou can see in the above result, that this Spark application produces a sequence of Row objects, inside a Python list. Each row object contains a number from 0 to 4.\nCongratulations! You have just run your first Spark application using pyspark!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#overview-of-pyspark",
    "href": "Chapters/03-spark.html#overview-of-pyspark",
    "title": "2  Introducing Apache Spark",
    "section": "2.6 Overview of pyspark",
    "text": "2.6 Overview of pyspark\nBefore we continue, I want to give you a very brief overview of the main parts of pyspark that are the most useful and most important to know of.\n\n2.6.1 Main python modules\nThe main python modules that exists in pyspark are:\n\npyspark.sql.SparkSession: the SparkSession class that defines your Spark Session, or, the entry point to your Spark application;\npyspark.sql.dataframe: module that defines the DataFrame class;\npyspark.sql.column: module that defines the Column class;\npyspark.sql.types: module that contains all data types of Spark;\npyspark.sq.functions: module that contains all of the main Spark functions that we use in transformations;\npyspark.sql.window: module that defines the Window class, which is responsible for defining windows in a Spark DataFrame;\n\n\n\n2.6.2 Main python classes\nThe main python classes that exists in pyspark are:\n\nDataFrame: represents a Spark DataFrame, and it is the main data structure in pyspark. In essence, they represent a collection of datasets into named columns;\nColumn: represents a column in a Spark DataFrame;\nGroupedData: represents a grouped Spark DataFrame (result of DataFrame.groupby());\nWindow: describes a window in a Spark DataFrame;\nDataFrameReader and DataFrameWriter: classes responsible for reading data from a data source into a Spark DataFrame, and writing data from a Spark DataFrame into a data source;\nDataFrameNaFunctions: class that stores all main methods for dealing with null values (i.e. missing data);\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1. https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/03-spark.html#footnotes",
    "href": "Chapters/03-spark.html#footnotes",
    "title": "2  Introducing Apache Spark",
    "section": "",
    "text": "https://spark.apache.org/sql/↩︎\nhttps://spark.apache.org/docs/latest/ml-guide.html↩︎\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview↩︎\nhttps://phoenixnap.com/kb/install-spark-on-ubuntu.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Apache Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "href": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.1 Spark DataFrames versus Spark Datasets",
    "text": "3.1 Spark DataFrames versus Spark Datasets\nSpark have two notions of structured data: DataFrames and Datasets. In summary, a Spark Dataset, is a distributed collection of data (Apache Spark Official Documentation 2022). In contrast, a Spark DataFrame is a Spark Dataset organized into named columns (Apache Spark Official Documentation 2022).\nThis means that, Spark DataFrames are very similar to tables as we know in relational databases - RDBMS, or, in spreadsheets (like Excel). So in a Spark DataFrame, each column has a name, and they all have the same number of rows. Furthermore, all the rows inside a column must store the same type of data, but each column can store a different type of data.\nIn the other hand, Spark Datasets are considered a collection of any type of data. So a Dataset might be a collection of unstructured data as well, like log files, JSON and XML trees, etc. Spark Datasets can be created and transformed trough the Dataset API of Spark. But this API is available only in Scala and Java API’s of Spark. For this reason, we do not act directly on Datasets with pyspark, only DataFrames. That’s ok, because for the most part of applications, we do want to use DataFrames, and not Datasets, to represent our data.\nHowever, what makes a Spark DataFrame different from other dataframes? Like the pandas DataFrame? Or the R native data.frame structure? Is the distributed aspect of it. Spark DataFrames are based on Spark Datasets, and these Datasets are collections of data that are distributed across the cluster. As an example, lets suppose you have the following table stored as a Spark DataFrame:\n\n\n\nID\nName\nValue\n\n\n\n\n1\nAnne\n502\n\n\n2\nCarls\n432\n\n\n3\nStoll\n444\n\n\n4\nPercy\n963\n\n\n5\nMartha\n123\n\n\n6\nSigrid\n621\n\n\n\nIf you are running Spark in a 4 nodes cluster (one is the driver node, and the other three are worker nodes). Each worker node of the cluster will store a section of this data. So you, as the programmer, will see, manage and transform this table as if it was a single and unified table. But behind the hoods, Spark will split this data and store it as many fragments across the Spark cluster. Figure 3.1 presents this notion in a visual manner.\n\n\n\n\n\n\nFigure 3.1: A Spark DataFrame is distributed across the cluster",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-dataframe-partitions",
    "href": "Chapters/04-dataframes.html#sec-dataframe-partitions",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.2 Partitions of a Spark DataFrame",
    "text": "3.2 Partitions of a Spark DataFrame\nA Spark DataFrame is always broken into many small pieces, and, these pieces are always spread across the cluster of machines. Each one of these small pieces of the total data are considered a DataFrame partition.\nFor the most part, you do not manipulate these partitions manually or individually (Karau et al. 2015), because Spark automatically do this job for you.\nAs we exposed in Figure 3.1, each node of the cluster will hold a piece of the total DataFrame. If we translate this distribution into a “partition” distribution, this means that each node of the cluster can hold one or multiple partitions of the Spark DataFrame.\nIf we sum all partitions present in a node of the cluster, we get a chunk of the total DataFrame. The figure below demonstrates this notion:\n\n\n\n\n\n\nFigure 3.2: Partitions of a DataFrame\n\n\n\nIf the Spark DataFrame is not big, each node of the cluster will probably store just a single partition of this DataFrame. In contrast, depending on the complexity and size of the DataFrame, Spark will split this DataFrame into more partitions that there are nodes in the cluster. In this case, each node of the cluster will hold more than 1 partition of the total DataFrame.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-dataframe-class",
    "href": "Chapters/04-dataframes.html#sec-dataframe-class",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.3 The DataFrame class in pyspark",
    "text": "3.3 The DataFrame class in pyspark\nIn pyspark, every Spark DataFrame is stored inside a python object of class pyspark.sql.dataframe.DataFrame. Or more succintly, a object of class DataFrame.\nLike any python class, the DataFrame class comes with multiple methods that are available for every object of this class. This means that you can use any of these methods in any Spark DataFrame that you create through pyspark.\nAs an example, in the code below I expose all the available methods from this DataFrame class. First, I create a Spark DataFrame with spark.range(5), and, store it in the object df5. After that, I use the dir() function to show all the methods that I can use through this df5 object:\n\ndf5 = spark.range(5)\navailable_methods = dir(df5)\nprint(available_methods)\n\nAll the methods present in this DataFrame class, are commonly referred as the DataFrame API of Spark. Remember, this is the most important API of Spark. Because much of your Spark applications will heavily use this API to compose your data transformations and data flows (Chambers and Zaharia 2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-building-a-dataframe",
    "href": "Chapters/04-dataframes.html#sec-building-a-dataframe",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.4 Building a Spark DataFrame",
    "text": "3.4 Building a Spark DataFrame\nThere are some different methods to create a Spark DataFrame. For example, because a DataFrame is basically a Dataset of rows, we can build a DataFrame from a collection of Row’s, through the createDataFrame() method from your Spark Session:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,1,1)),\n  Row(id = 3, value = 20.1, date = date(2021,1,2)),\n  Row(id = 4, value = 12.6, date = date(2021,1,3))\n]\n\ndf = spark.createDataFrame(data)\n\nRemember that a Spark DataFrame in python is a object of class pyspark.sql.dataframe.DataFrame as you can see below:\n\ntype(df)\n\nIf you try to see what is inside of this kind of object, you will get a small description of the columns present in the DataFrame as a result:\n\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nSo, in the above example, we use the Row() constructor (from pyspark.sql module) to build 4 rows. The createDataFrame() method, stack these 4 rows together to form our new DataFrame df. The result is a Spark DataFrame with 4 rows and 3 columns (id, value and date).\nBut you can use different methods to create the same Spark DataFrame. As another example, with the code below, we are creating a DataFrame called students from two different python lists (data and columns).\nThe first list (data) is a list of rows. Each row is represent by a python tuple, which contains the values in each column. But the secont list (columns) contains the names for each column in the DataFrame.\nTo create the students DataFrame we deliver these two lists to createDataFrame() method:\n\ndata = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(data, columns)\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]\n\n\nYou can also use a method that returns a DataFrame object by default. Examples are the table() and range() methods from your Spark Session, like we used in the Section 3.3, to create the df5 object.\nOther examples are the methods used to read data and import it to pyspark. These methods are available in the spark.read module, like spark.read.csv() and spark.read.json(). These methods will be described in more depth in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-viewing-a-dataframe",
    "href": "Chapters/04-dataframes.html#sec-viewing-a-dataframe",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.5 Visualizing a Spark DataFrame",
    "text": "3.5 Visualizing a Spark DataFrame\nA key aspect of Spark is its laziness. In other words, for most operations, Spark will only check if your code is correct and if it makes sense. Spark will not actually run or execute the operations you are describing in your code, unless you explicit ask for it with a trigger operation, which is called an “action” (this kind of operation is described in Section 5.2).\nYou can notice this laziness in the output below:\n\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]\n\n\nBecause when we call for an object that stores a Spark DataFrame (like df and students), Spark will only calculate and print a summary of the structure of your Spark DataFrame, and not the DataFrame itself.\nSo how can we actually see our DataFrame? How can we visualize the rows and values that are stored inside of it? For this, we use the show() method. With this method, Spark will print the table as pure text, as you can see in the example below:\n\nstudents.show()\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|   Name|Age|Height|Score1|Score2|Score3|Score4|   Course|Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|    12114|   Anne| 21|  1.56|     8|     9|    10|     9|Economics|        SC|\n|    13007| Adrian| 23|  1.82|     6|     6|     8|     7|Economics|        SC|\n|    10045| George| 29|  1.77|    10|     9|    10|     7|      Law|        SC|\n|    12459|Adeline| 26|  1.61|     8|     6|     7|     7|      Law|        SC|\n|    10190|  Mayla| 22|  1.67|     7|     7|     7|     9|   Design|        AR|\n|    11552| Daniel| 24|  1.75|     9|     9|    10|     9|   Design|        AR|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n\n\n\nBy default, this method shows only the top rows of your DataFrame, but you can specify how much rows exactly you want to see, by using show(n), where n is the number of rows. For example, I can visualize only the first 2 rows of df like this:\n\ndf.show(2)\n\n+---+-----+----------+\n| id|value|      date|\n+---+-----+----------+\n|  1| 28.3|2021-01-01|\n|  2| 15.8|2021-01-01|\n+---+-----+----------+\nonly showing top 2 rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#getting-the-name-of-the-columns",
    "href": "Chapters/04-dataframes.html#getting-the-name-of-the-columns",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.6 Getting the name of the columns",
    "text": "3.6 Getting the name of the columns\nIf you need to, you can easily collect a python list with the column names present in your DataFrame, in the same way you would do in a pandas DataFrame. That is, by using the columns method of your DataFrame, like this:\n\nstudents.columns\n\n['StudentID',\n 'Name',\n 'Age',\n 'Height',\n 'Score1',\n 'Score2',\n 'Score3',\n 'Score4',\n 'Course',\n 'Department']",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#getting-the-number-of-rows",
    "href": "Chapters/04-dataframes.html#getting-the-number-of-rows",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.7 Getting the number of rows",
    "text": "3.7 Getting the number of rows\nIf you want to know the number of rows present in a Spark DataFrame, just use the count() method of this DataFrame. As a result, Spark will build this DataFrame, and count the number of rows present in it.\n\nstudents.count()\n\n6",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#spark-data-types",
    "href": "Chapters/04-dataframes.html#spark-data-types",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.8 Spark Data Types",
    "text": "3.8 Spark Data Types\nEach column of your Spark DataFrame is associated with a specific data type. Spark supports a large number of different data types. You can see the full list at the official documentation page1. For now, we will focus on the most used data types, which are listed below:\n\nIntegerType: Represents 4-byte signed integer numbers. The range of numbers that it can represent is from -2147483648 to 2147483647.\nLongType: Represents 8-byte signed integer numbers. The range of numbers that it can represent is from -9223372036854775808 to 9223372036854775807.\nFloatType: Represents 4-byte single-precision floating point numbers.\nDoubleType: Represents 8-byte double-precision floating point numbers.\nStringType: Represents character string values.\nBooleanType: Represents boolean values (true or false).\nTimestampType: Represents datetime values, i.e. values that contains fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time.\nDateType: Represents date values, i.e. values that contains fields year, month and day, without a time-zone.\n\nBesides these more “standard” data types, Spark supports two other complex types, which are ArrayType and MapType:\n\nArrayType(elementType, containsNull): Represents a sequence of elements with the type of elementType. containsNull is used to indicate if elements in a ArrayType value can have null values.\nMapType(keyType, valueType, valueContainsNull): Represents a set of key-value pairs. The data type of keys is described by keyType and the data type of values is described by valueType. For a MapType value, keys are not allowed to have null values. valueContainsNull is used to indicate if values of a MapType value can have null values.\n\nEach one of these Spark data types have a corresponding python class in pyspark, which are stored in the pyspark.sql.types module. As a result, to access, lets say, type StryngType, we can do this:\n\nfrom pyspark.sql.types import StringType\ns = StringType()\nprint(s)\n\nStringType()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-dataframe-schema",
    "href": "Chapters/04-dataframes.html#sec-dataframe-schema",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.9 The DataFrame Schema",
    "text": "3.9 The DataFrame Schema\nThe schema of a Spark DataFrame is the combination of column names and the data types associated with each of these columns. Schemas can be set explicitly by you (that is, you can tell Spark how the schema of your DataFrame should look like), or, they can be automatically defined by Spark while reading or creating your data.\nYou can get a succinct description of a DataFrame schema, by looking inside the object where this DataFrame is stored. For example, lets look again to the df DataFrame.\nIn the result below, we can see that df has three columns (id, value and date). By the description id: bigint, we know that id is a column of type bigint, which translates to the LongType() of Spark. Furthermore, by the descriptions value: double and date: date, we know too that the columns value and date are of type DoubleType() and DateType(), respectively.\n\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nYou can also visualize a more complete report of the DataFrame schema by using the printSchema() method, like this:\n\ndf.printSchema()\n\nroot\n |-- id: long (nullable = true)\n |-- value: double (nullable = true)\n |-- date: date (nullable = true)\n\n\n\n\n3.9.1 Accessing the DataFrame schema\nSo, by calling the object of your DataFrame (i.e. an object of class DataFrame) you can see a small description of the schema of this DataFrame. But, how can you access this schema programmatically?\nYou do this, by using the schema method of your DataFrame, like in the example below:\n\ndf.schema\n\nStructType([StructField('id', LongType(), True), StructField('value', DoubleType(), True), StructField('date', DateType(), True)])\n\n\nThe result of the schema method, is a StructType() object, that contains some information about each column of your DataFrame. More specifically, a StructType() object is filled with multiple StructField() objects. Each StructField() object stores the name and the type of a column, and a boolean value (True or False) that indicates if this column can contain any null value inside of it.\nYou can use a for loop to iterate through this StructType() and get the information about each column separately.\n\nschema = df.schema\nfor column in schema:\n  print(column)\n\nStructField('id', LongType(), True)\nStructField('value', DoubleType(), True)\nStructField('date', DateType(), True)\n\n\nYou can access just the data type of each column by using the dataType method of each StructField() object.\n\nfor column in schema:\n  datatype = column.dataType\n  print(datatype)\n\nLongType()\nDoubleType()\nDateType()\n\n\nAnd you can do the same for column names and the boolean value (that indicates if the column can contain “null” values), by using the name and nullable methods, respectively.\n\n# Accessing the name of each column\nfor column in schema:\n  print(column.name)\n\nid\nvalue\ndate\n\n\n\n# Accessing the boolean value that indicates\n# if the column can contain null values\nfor column in schema:\n  print(column.nullable)\n\nTrue\nTrue\nTrue\n\n\n\n\n3.9.2 Building a DataFrame schema\nWhen Spark creates a new DataFrame, it will automatically guess which schema is appropriate for that DataFrame. In other words, Spark will try to guess which are the appropriate data types for each column. But, this is just a guess, and, sometimes, Spark go way off.\nBecause of that, in some cases, you have to tell Spark how exactly you want this DataFrame schema to be like. To do that, you need to build the DataFrame schema by yourself, with StructType() and StructField() constructors, alongside with the Spark data types (i.e. StringType(), DoubleType(), IntegerType(), …). Remember, all of these python classes come from the pyspark.sql.types module.\nIn the example below, the schema object represents the schema of the registers DataFrame. This DataFrame have three columns (ID, Date, Name) of types IntegerType, DateType and StringType, respectively.\nYou can see below that I deliver this schema object that I built to spark.createDataFrame(). Now spark.createDataFrame() will follow the schema I described in this schema object when building the registers DataFrame.\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DateType, StringType, IntegerType\nfrom datetime import date\n\ndata = [\n  (1, date(2022, 1, 1), 'Anne'),\n  (2, date(2022, 1, 3), 'Layla'),\n  (3, date(2022, 1, 15), 'Wick'),\n  (4, date(2022, 1, 11), 'Paul')\n]\n\nschema = StructType([\n  StructField('ID', IntegerType(), True),\n  StructField('Date', DateType(), True),\n  StructField('Name', StringType(), True)\n])\n\nregisters = spark.createDataFrame(data, schema = schema)\n\nHaving this example in mind, in order to build a DataFrame schema from scratch, you have to build the equivalent StructType() object that represents the schema you want.\n\n\n3.9.3 Checking your DataFrame schema\nIn some cases, you need to include in your pyspark program, some checks that certifies that your Spark DataFrame have the expected schema. In other words, you want to take actions if your DataFrame have a different schema that might cause a problem in your program.\nTo check if a specific column of your DataFrame is associated with the data type \\(x\\), you have to use the DataFrame schema to check if the respective column is an “instance” of the python class that represents that data type \\(x\\). Lets use the df DataFrame as an example.\nSuppose you wanted to check if the id column is of type IntegerType. To do this check, we use the python built-in function isinstance() with the python class that represents the Spark IntegerType data type. But, you can see in the result below, that the id column is not of type IntegerType.\n\nfrom pyspark.sql.types import IntegerType\nschema = df.schema\nid_column = schema[0]\nisinstance(id_column.dataType, IntegerType)\n\nFalse\n\n\nThis unexpected result happens, because the id column is actually from the “big integer” type, or, the LongType (which are 8-byte signed integer). You can see below, that now the test results in true:\n\nfrom pyspark.sql.types import LongType\nisinstance(id_column.dataType, LongType)\n\nTrue\n\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1. https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-dataframes.html#footnotes",
    "href": "Chapters/04-dataframes.html#footnotes",
    "title": "3  Introducing Spark DataFrames",
    "section": "",
    "text": "The full list is available at the link https://spark.apache.org/docs/3.3.0/sql-ref-datatypes.html#supported-data-types↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing Spark DataFrames</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#building-a-column-object",
    "href": "Chapters/04-columns.html#building-a-column-object",
    "title": "4  Introducing the Column class",
    "section": "4.1 Building a column object",
    "text": "4.1 Building a column object\nYou can refer to or create a column, by using the col() and column() functions from pyspark.sql.functions module. These functions receive a string input with the name of the column you want to create/refer to.\nTheir result are always a object of class Column. For example, the code below creates a column called ID:\n\nfrom pyspark.sql.functions import col\nid_column = col('ID')\nprint(id_column)\n\nColumn&lt;'ID'&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#sec-columns-related-expressions",
    "href": "Chapters/04-columns.html#sec-columns-related-expressions",
    "title": "4  Introducing the Column class",
    "section": "4.2 Columns are strongly related to expressions",
    "text": "4.2 Columns are strongly related to expressions\nMany kinds of transformations that we want to apply over a Spark DataFrame, are usually described through expressions, and, these expressions in Spark are mainly composed by column transformations. That is why the Column class, and its methods, are so important in Apache Spark.\nColumns in Spark are so strongly related to expressions that the columns themselves are initially interpreted as expressions. If we look again at the column id from df DataFrame, Spark will bring a expression as a result, and not the values hold by this column.\n\ndf.id\n\nColumn&lt;'id'&gt;\n\n\nHaving these ideas in mind, when I created the column ID on the previous section, I created a “column expression”. This means that col(\"ID\") is just an expression, and as consequence, Spark does not know which are the values of column ID, or, where it lives (which is the DataFrame that this column belongs?). For now, Spark is not interested in this information, it just knows that we have an expression referring to a column called ID.\nThese ideas relates a lot to the lazy aspect of Spark that we talked about in Section 3.5. Spark will not perform any heavy calculation, or show you the actual results/values from you code, until you trigger the calculations with an action (we will talk more about these “actions” on Section 5.2). As a result, when you access a column, Spark will only deliver a expression that represents that column, and not the actual values of that column.\nThis is handy, because we can store our expressions in variables, and, reuse it latter, in multiple parts of our code. For example, I can keep building and merging a column with different kinds of operators, to build a more complex expression. In the example below, I create a expression that doubles the values of ID column:\n\nexpr1 = id_column * 2\nprint(expr1)\n\nColumn&lt;'(ID * 2)'&gt;\n\n\nRemember, with this expression, Spark knows that we want to get a column called ID somewhere, and double its values. But Spark will not perform that action right now.\nLogical expressions follow the same logic. In the example below, I am looking for rows where the value in column Name is equal to 'Anne', and, the value in column Grade is above 6.\nAgain, Spark just checks if this is a valid logical expression. For now, Spark does not want to know where are these Name and Grade columns. Spark does not evaluate the expression, until we ask for it with an action:\n\nexpr2 = (col('Name') == 'Anne') & (col('Grade') &gt; 6)\nprint(expr2)\n\nColumn&lt;'((Name = Anne) AND (Grade &gt; 6))'&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#literal-values-versus-expressions",
    "href": "Chapters/04-columns.html#literal-values-versus-expressions",
    "title": "4  Introducing the Column class",
    "section": "4.3 Literal values versus expressions",
    "text": "4.3 Literal values versus expressions\nWe know now that columns of a Spark DataFrame have a deep connection with expressions. But, on the other hand, there are some situations that you write a value (it can be a string, a integer, a boolean, or anything) inside your pyspark code, and you might actually want Spark to intepret this value as a constant (or a literal) value, rather than a expression.\nAs an example, lets suppose you control the data generated by the sales of five different stores, scattered across different regions of Belo Horizonte city (in Brazil). Now, lets suppose you receive a batch of data generated by the 4th store in the city, which is located at Amazonas Avenue, 324. This batch of data is exposed below:\n\npath = './../Data/sales.json'\nsales = spark.read.json(path)\nsales.show(5)\n\n+-----+----------+------------+-------+-------------------+-----+\n|price|product_id|product_name|sale_id|          timestamp|units|\n+-----+----------+------------+-------+-------------------+-----+\n| 3.12|       134| Milk 1L Mua| 328711|2022-02-01T22:10:02|    1|\n| 1.22|       110|  Coke 350ml| 328712|2022-02-03T11:42:09|    3|\n| 4.65|       117|    Pepsi 2L| 328713|2022-02-03T14:22:15|    1|\n| 1.22|       110|  Coke 350ml| 328714|2022-02-03T18:33:08|    1|\n| 0.85|       341|Trident Mint| 328715|2022-02-04T15:41:36|    1|\n+-----+----------+------------+-------+-------------------+-----+\n\n\n\nIf you look at this batch… there is no indication that these sales come from the 4th store. In other words, this information is not present in the data, is just in your mind. It certainly is a very bad idea to leave this data as is, whithout any identification of the source of it. So, you might want to add some labels and new columns to this batch of data, that can easily identify the store that originated these sales.\nFor example, we could add two new columns to this sales DataFrame. One for the number that identifies the store (4), and, another to keep the store address. Considering that all rows in this batch comes from the 4th store, we should add two “constant” columns, meaning that these columns should have a constant value across all rows in this batch. But, how can we do this? How can we create a “constant” column? The answer is: by forcing Spark to interpret the values as literal values, instead of a expression.\nIn other words, I can not use the col() function to create these two new columns. Because this col() function receives a column name as input. It interprets our input as an expression that refers to a column name. This function does not accept some sort of description of the actual values that this column should store.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#sec-literal-values",
    "href": "Chapters/04-columns.html#sec-literal-values",
    "title": "4  Introducing the Column class",
    "section": "4.4 Passing a literal (or a constant) value to Spark",
    "text": "4.4 Passing a literal (or a constant) value to Spark\nSo how do we force Spark to interpret a value as a literal (or constant) value, rather than a expression? To do this, you must write this value inside the lit() (short for “literal”) function from the pyspark.sql.functions module.\nIn other words, when you write in your code the statement lit(4), Spark understand that you want to create a new column which is filled with 4’s. In other words, this new column is filled with the constant integer 4.\nWith the code below, I am creating two new columns (called store_number and store_address), and adding them to the sales DataFrame.\n\nfrom pyspark.sql.functions import lit\nstore_number = lit(4).alias('store_number')\nstore_address = lit('Amazonas Avenue, 324').alias('store_address')\n\nsales = sales\\\n    .select(\n        '*', store_number, store_address\n    )\n\nsales\\\n    .select(\n        'product_id', 'product_name',\n        'store_number', 'store_address'\n    )\\\n    .show(5)\n\n+----------+------------+------------+--------------------+\n|product_id|product_name|store_number|       store_address|\n+----------+------------+------------+--------------------+\n|       134| Milk 1L Mua|           4|Amazonas Avenue, 324|\n|       110|  Coke 350ml|           4|Amazonas Avenue, 324|\n|       117|    Pepsi 2L|           4|Amazonas Avenue, 324|\n|       110|  Coke 350ml|           4|Amazonas Avenue, 324|\n|       341|Trident Mint|           4|Amazonas Avenue, 324|\n+----------+------------+------------+--------------------+\n\n\n\nIn essence, you normally use the lit() function when you want to write a literal value in places where Spark expects a column name. In the example above, instead of writing a name to an existing column in the sales DataFrame, I wanted to write the literal values 'Amazonas Avenue, 324' and 4, and I used the lit() function to make this intention very clear to Spark. If I did not used the lit() function, the withColumn() method would interpret the value 'Amazonas Avenue, 324' as an existing column named Amazonas Avenue, 324.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#key-methods-of-the-column-class",
    "href": "Chapters/04-columns.html#key-methods-of-the-column-class",
    "title": "4  Introducing the Column class",
    "section": "4.5 Key methods of the Column class",
    "text": "4.5 Key methods of the Column class\nBecause many transformations that we want to apply over our DataFrames are expressed as column transformations, the methods from the Column class will be quite useful on many different contexts. You will see many of these methods across the next chapters, like desc(), alias() and cast().\nRemember, you can always use the dir() function to see the complete list of methods available in any python class. Is always useful to check the official documentation too1. There you will have a more complete description of each method.\nBut since they are so important in Spark, lets just give you a brief overview of some of the most popular methods from the Column class (these methods will be described in more detail in later chapters):\n\ndesc() and asc(): methods to order the values of the column in a descending or ascending order (respectively);\ncast() and astype(): methods to cast (or convert) the values of the column to a specific data type;\nalias(): method to rename a column;\nsubstr(): method that returns a new column with the sub string of each value;\nisNull() and isNotNull(): logical methods to test if each value in the column is a null value or not;\nstartswith() and endswith(): logical methods to search for values that starts with or ends with a specific pattern;\nlike() and rlike(): logical methods to search for a specific pattern or regular expression in the values of the column;\nisin(): logical method to test if each value in the column is some of the listed values;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/04-columns.html#footnotes",
    "href": "Chapters/04-columns.html#footnotes",
    "title": "4  Introducing the Column class",
    "section": "",
    "text": "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducing the `Column` class</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-df-defining-transformations",
    "href": "Chapters/05-transforming.html#sec-df-defining-transformations",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.1 Defining transformations",
    "text": "5.1 Defining transformations\nSpark DataFrames are immutable, meaning that, they cannot be directly changed. But you can use an existing DataFrame to create a new one, based on a set of transformations. In other words, you define a new DataFrame as a transformed version of an older DataFrame.\nBasically every pyspark program that you write will have such transformations. Spark support many types of transformations, however, in this chapter, we will focus on six basic transformations that you can apply to a DataFrame:\n\nFiltering rows based on a logical condition;\nSelecting a subset of rows;\nSelecting specific columns;\nAdding or deleting columns;\nSorting rows;\nCalculating aggregates;\n\nTherefore, when you apply one of the above transformations to an existing DataFrame, you will get a new DataFrame as a result. You usually combine multiple transformations together to get your desired result. As a first example, lets get back to the df DataFrame:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,1,1)),\n  Row(id = 3, value = 20.1, date = date(2021,1,2)),\n  Row(id = 4, value = 12.6, date = date(2021,1,3))\n]\n\ndf = spark.createDataFrame(data)\n\nIn the example below, to create a new DataFrame called big_values, we begin with the df DataFrame, then, we filter its rows where value is greater than 15, then, we select date and value columns, then, we sort the rows based on the value column. So, this set of sequential transformations (filter it, then, select it, then, order it, …) defines what this new big_values DataFrame is.\n\nfrom pyspark.sql.functions import col\n# You define a chain of transformations to\n# create a new DataFrame\nbig_values = df\\\n  .filter(col('value') &gt; 15)\\\n  .select('date', 'value')\\\n  .orderBy('value')\n\nThus, to apply a transformation to an existing DataFrame, we use DataFrame methods such as select(), filter(), orderBy() and many others. Remember, these are methods from the python class that defines Spark DataFrame’s (i.e. the pyspark.sql.dataframe.DataFrame class).\nThis means that you can apply these transformations only to Spark DataFrames, and no other kind of python object. For example, if you try to use the orderBy() method in a standard python string (i.e. an object of class str), you will get an AttributeError error. Because this class of object in python, does not have a orderBy() method:\n\ns = \"A python string\"\ns.orderBy('value')\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAttributeError: 'str' object has no attribute 'orderBy'\nEach one of these DataFrame methods create a lazily evaluated transformation. Once again, we see the lazy aspect of Spark doing its work here. All these transformation methods are lazily evaluated, meaning that, Spark will only check if they make sense with the initial DataFrame that you have. Spark will not actually perform these transformations on your initial DataFrame, not untill you trigger these transformations with an action.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-dataframe-actions",
    "href": "Chapters/05-transforming.html#sec-dataframe-actions",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.2 Triggering calculations with actions",
    "text": "5.2 Triggering calculations with actions\nTherefore, Spark will avoid performing any heavy calculation until such calculation is really needed. But how or when Spark will face this decision? When it encounters an action. An action is the tool you have to trigger Spark to actually perform the transformations you have defined.\n\nAn action instructs Spark to compute the result from a series of transformations. (Chambers and Zaharia 2018).\n\nThere are four kinds of actions in Spark:\n\nShowing an output in the console;\nWriting data to some file or data source;\nCollecting data from a Spark DataFrame to native objects in python (or Java, Scala, R, etc.);\nCounting the number of rows in a Spark DataFrame;\n\nYou already know the first type of action, because we used it before with the show() method. This show() method is an action by itself, because you are asking Spark to show some output to you. So we can make Spark to actually calculate the transformations that defines the big_values DataFrame, by asking Spark to show this DataFrame to us.\n\nbig_values.show()\n\n[Stage 0:&gt;                                                        (0 + 12) / 12]                                                                                \n\n\n+----------+-----+\n|      date|value|\n+----------+-----+\n|2021-01-01| 15.8|\n|2021-01-02| 20.1|\n|2021-01-01| 28.3|\n+----------+-----+\n\n\n\nAnother very useful action is the count() method, that gives you the number of rows in a DataFrame. To be able to count the number of rows in a DataFrame, Spark needs to access this DataFrame in the first place. That is why this count() method behaves as an action. Spark will perform the transformations that defines big_values to access the actual rows of this DataFrame and count them.\n\nbig_values.count()\n\n3\n\n\nFurthermore, sometimes, you want to collect the data of a Spark DataFrame to use it inside python. In other words, sometimes you need to do some work that Spark cannot do by itself. To do so, you collect part of the data that is being generated by Spark, and store it inside a normal python object to use it in a standard python program.\nThat is what the collect() method do. It transfers all the data of your Spark DataFrame into a standard python list that you can easily access with python. More specifically, you get a python list full of Row() values:\n\ndata = big_values.collect()\nprint(data)\n\n[Row(date=datetime.date(2021, 1, 1), value=15.8), Row(date=datetime.date(2021, 1, 2), value=20.1), Row(date=datetime.date(2021, 1, 1), value=28.3)]\n\n\nThe take() method is very similar to collect(). But you usually apply take() when you need to collect just a small section of your DataFrame (and not the entire thing), like the first n rows.\n\nn = 1\nfirst_row = big_values.take(n)\nprint(first_row)\n\n[Row(date=datetime.date(2021, 1, 1), value=15.8)]\n\n\nThe last action would be the write method of a Spark DataFrame, but we will explain this method latter at Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-narrow-wide",
    "href": "Chapters/05-transforming.html#sec-narrow-wide",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.3 Understanding narrow and wide transformations",
    "text": "5.3 Understanding narrow and wide transformations\nThere are two kinds of transformations in Spark: narrow and wide transformations. Remember, a Spark DataFrame is divided into many small parts (called partitions), and, these parts are spread across the cluster. The basic difference between narrow and wide transformations, is if the transformation forces Spark to read data from multiple partitions to generate a single part of the result of that transformation, or not.\nMore technically, narrow transformations are simply transformations where 1 input data (or 1 partition of the input DataFrame) contributes to only 1 partition of the output.\n\n\n\n\n\n\nFigure 5.1: Presenting narrow transformations\n\n\n\nIn other words, each partition of your input DataFrame will be used (separately) to generate one individual part of the result of your transformation. As another perspective, you can understand narrow transformations as those where Spark does not need to read the entire input DataFrame to generate a single and small piece of your result.\nA classic example of narrow transformation is a filter. For example, suppose you have three students (Anne, Carls and Mike), and that each one has a bag full of blue, orange and red balls mixed. Now, suppose you asked them to collect all the red balls of these bags, and combined them in a single bag.\nTo do this task, Mike does not need to know what balls are inside of the bag of Carls or Anne. He just need to collect the red balls that are solely on his bag. At the end of the task, each student will have a part of the end result (that is, all the red balls that were in his own bag), and they just need to combine all these parts to get the total result.\nThe same thing applies to filters in Spark DataFrames. When you filter all the rows where the column state is equal to \"Alaska\", Spark will filter all the rows in each partition separately, and then, will combine all the outputs to get the final result.\nIn contrast, wide transformations are the opposite of that. In wide transformations, Spark needs to use more than 1 partition of the input DataFrame to generate a small piece of the result.\n\n\n\n\n\n\nFigure 5.2: Presenting wide transformations\n\n\n\nWhen this kind of transformation happens, each worker node of the cluster needs to share his partition with the others. In other words, what happens is a partition shuffle. Each worker node sends his partition to the others, so they can have access to it, while performing their assigned tasks.\nPartition shuffles are a very popular topic in Apache Spark, because they can be a serious source of inefficiency in your Spark application (Chambers and Zaharia 2018). In more details, when these shuffles happens, Spark needs to write data back to the hard disk of the computer, and this is not a very fast operation. It does not mean that wide transformations are bad or slow, just that the shuffles they are producing can be a problem.\nA classic example of wide operation is a grouped aggregation. For example, lets suppose we had a DataFrame with the daily sales of multiple stores spread across the country, and, we wanted to calculate the total sales per city/region. To calculate the total sales of a specific city, like “São Paulo”, Spark would need to find all the rows that corresponds to this city, before adding the values, and these rows can be spread across multiple partitions of the cluster.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-transf-dataframe",
    "href": "Chapters/05-transforming.html#sec-transf-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.4 The transf DataFrame",
    "text": "5.4 The transf DataFrame\nTo demonstrate some of the next examples in this chapter, we will use a different DataFrame called transf. The data that represents this DataFrame is freely available as a CSV file. You can download this CSV at the repository of this book1.\nWith the code below, you can import the data from the transf.csv CSV file, to recreate the transf DataFrame in your Spark Session:\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql.types import LongType, TimestampType, DateType\npath = \"../Data/transf.csv\"\nschema = StructType([\n  StructField('dateTransfer', DateType(), False),\n  StructField('datetimeTransfer', TimestampType(), False),\n  StructField('clientNumber', LongType(), False),\n  StructField('transferValue', DoubleType(), False),\n  StructField('transferCurrency', StringType(), False),\n  StructField('transferID', LongType(), False),\n  StructField('transferLog', StringType(), False),\n  StructField('destinationBankNumber', LongType(), False),\n  StructField('destinationBankBranch', LongType(), False),\n  StructField('destinationBankAccount', StringType(), False)\n])\n\ntransf = spark.read\\\n  .csv(path, schema = schema, sep = \";\", header = True)\n\nYou could also use the pandas library to read the DataFrame directly from GitHub, without having to manually download the file:\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/'\nurl = url + 'pedropark99/Introd-pyspark/'\nurl = url + 'main/Data/transf.csv'\n\ntransf_pd = pd.read_csv(\n  url, sep = ';',\n  dtype = str,\n  keep_default_na = False\n)\n\ntransf_pd['transferValue'] = transf_pd['transferValue'].astype('float')\n\ncolumns_to_int = [\n  'clientNumber',\n  'transferID',\n  'destinationBankNumber',\n  'destinationBankBranch'\n]\nfor column in columns_to_int:\n  transf_pd[column] = transf_pd[column].astype('int')\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder.getOrCreate()\ntransf = spark.createDataFrame(transf_pd)\\\n  .withColumn('dateTransfer', col('dateTransfer').cast('date'))\\\n  .withColumn('datetimeTransfer', col('datetimeTransfer').cast('timestamp'))\n\nThis transf DataFrame contains bank transfer records from a fictitious bank. Before I show you the actual data of this DataFrame, is useful to give you a quick description of each column that it contains:\n\ndateTransfer: the date when the transfer occurred;\ndatetimeTransfer: the date and time when the transfer occurred;\nclientNumber the unique number that identifies a client of the bank;\ntransferValue: the nominal value that was transferred;\ntransferCurrency: the currency of the nominal value transferred;\ntransferID: an unique ID for the transfer;\ntransferLog: store any error message that may have appeared during the execution of the transfer;\ndestinationBankNumber: the transfer destination bank number;\ndestinationBankBranch: the transfer destination branch number;\ndestinationBankAccount: the transfer destination account number;\n\nNow, to see the actual data of this DataFrame, we can use the show() action as usual.\n\ntransf.show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nAs you can see below, this transf DataFrame have 2421 rows in total:\n\ntransf.count()\n\n2421",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#filtering-rows-of-your-dataframe",
    "href": "Chapters/05-transforming.html#filtering-rows-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.5 Filtering rows of your DataFrame",
    "text": "5.5 Filtering rows of your DataFrame\nTo filter specific rows of a DataFrame, pyspark offers two equivalent DataFrame methods called where() and filter(). In other words, they both do the same thing, and work in the same way. These methods receives as input a logical expression that translates what you want to filter.\nAs a first example, lets suppose you wanted to inspect all the rows from the transf DataFrame where transferValue is less than 1000. To do so, you can use the following code:\n\ntransf\\\n  .filter(\"transferValue &lt; 1000\")\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       NULL|                  421|                 5420|               43088-1|\n|  2022-12-13|2022-12-13 20:44:23|        5516|       992.15|        dollar $|  20223442|       NULL|                   33|                 5420|               41609-8|\n|  2022-11-24|2022-11-24 20:01:39|        1945|       174.64|        dollar $|  20223319|       NULL|                  421|                 2400|               34025-8|\n|  2022-11-07|2022-11-07 16:35:57|        4862|       570.69|        dollar $|  20223212|       NULL|                  290|                 5420|               51165-3|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       NULL|                  421|                 4078|               43478-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nWriting simple SQL logical expression inside a string is the most easy and “clean” way to create a filter expression in pyspark. However, you could also write the same exact expression in a more “pythonic” way, using the col() function from pyspark.sql.functions module.\n\nfrom pyspark.sql.functions import col\n\ntransf\\\n  .filter(col(\"transferValue\") &lt; 1000)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       NULL|                  421|                 5420|               43088-1|\n|  2022-12-13|2022-12-13 20:44:23|        5516|       992.15|        dollar $|  20223442|       NULL|                   33|                 5420|               41609-8|\n|  2022-11-24|2022-11-24 20:01:39|        1945|       174.64|        dollar $|  20223319|       NULL|                  421|                 2400|               34025-8|\n|  2022-11-07|2022-11-07 16:35:57|        4862|       570.69|        dollar $|  20223212|       NULL|                  290|                 5420|               51165-3|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       NULL|                  421|                 4078|               43478-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nYou still have a more verbose alternative, that does not require the col() function. With this method, you refer to the specific column using the dot operator (.), like in the example below:\n\n# This will give you the exact\n# same result of the examples above\ntransf\\\n  .filter(transf.transferValue &lt; 1000)\n\n\n5.5.1 Logical operators available\nAs we saw in the previous section, there are two ways to write logical expressions in pyspark: 1) write a SQL logical expression inside a string; 2) or, write a python logical expression using the col() function.\nIf you choose to write a SQL logical expressions in a string, you need to use the logical operators of SQL in your expression (not the logical operators of python). In the other hand, if you choose to write in the “python” way, then, you need to use the logical operators of python instead.\nThe logical operators of SQL are described in the table below:\n\n\n\nTable 5.1: List of logical operators of SQL\n\n\n\n\n\n\n\n\n\n\nOperator\nExample of expression\nMeaning of the expression\n\n\n\n\n&lt;\nx &lt; y\nis x less than y?\n\n\n&gt;\nx &gt; y\nis x greater than y?\n\n\n&lt;=\nx &lt;= y\nis x less than or equal to y?\n\n\n&gt;=\nx &gt;= y\nis x greater than or equal to y?\n\n\n==\nx == y\nis x equal to y?\n\n\n!=\nx != y\nis x not equal to y?\n\n\nin\nx in y\nis x one of the values listed in y?\n\n\nand\nx and y\nboth logical expressions x and y are true?\n\n\nor\nx or y\nat least one of logical expressions x and y are true?\n\n\nnot\nnot x\nis the logical expression x not true?\n\n\n\n\n\n\nAnd, the logical operators of python are described in the table below:\n\n\n\nTable 5.2: List of logical operators of python\n\n\n\n\n\n\n\n\n\n\nOperator\nExample of expression\nMeaning of the expression\n\n\n\n\n&lt;\nx &lt; y\nis x less than y?\n\n\n&gt;\nx &gt; y\nis x greater than y?\n\n\n&lt;=\nx &lt;= y\nis x less than or equal to y?\n\n\n&gt;=\nx &gt;= y\nis x greater than or equal to y?\n\n\n==\nx == y\nis x equal to y?\n\n\n!=\nx != y\nis x not equal to y?\n\n\n&\nx & y\nboth logical expressions x and y are true?\n\n\n|\nx | y\nat least one of logical expressions x and y are true?\n\n\n~\n~x\nis the logical expression x not true?\n\n\n\n\n\n\n\n\n5.5.2 Connecting multiple logical expressions\nSometimes, you need to write more complex logical expressions to correctly describe the rows you are interested in. That is, when you combine multiple logical expressions together.\nAs an example, lets suppose you wanted all the rows in transf DataFrame from client of number 1297 where the transfer value is smaller than 1000, and the date of the transfer is after 20 of February 2022. These conditions are dependent, that is, they are connected to each other (the client number, the transfer value and the date of the transfer). That is why I used the and keyword between each condition in the example below (i.e. to connect these three conditions together).\n\ncondition = '''\n  transferValue &lt; 1000\n  and clientNumber == 1297 \n  and dateTransfer &gt; '2022-02-20'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show()\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       NULL|                  421|                 5420|               43088-1|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       NULL|                  421|                 4078|               43478-6|\n|  2022-02-27|2022-02-27 13:27:44|        1297|       697.21|        dollar $|  20221505|       NULL|                  421|                 1100|               60414-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nI could translate this logical expression into the “pythonic” way (using the col() function). However, I would have to surround each individual expression by parentheses, and, use the & operator to substitute the and keyword.\n\ntransf\\\n  .filter(\n    (col('transferValue') &lt; 1000) &\n    (col('clientNumber') == 1297) &\n    (col('dateTransfer') &gt; '2022-02-20')\n  )\\\n  .show()\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       NULL|                  421|                 5420|               43088-1|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       NULL|                  421|                 4078|               43478-6|\n|  2022-02-27|2022-02-27 13:27:44|        1297|       697.21|        dollar $|  20221505|       NULL|                  421|                 1100|               60414-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nThis a very important detail, because it is very easy to forget. When building your complex logical expressions in the “python” way, always remember to surround each expression by a pair of parentheses. Otherwise, you will get a very confusing and useless error message, like this:\n\ntransf\\\n  .filter(\n    col('transferValue') &lt; 1000 &\n    col('clientNumber') == 1297 &\n    col('dateTransfer') &gt; '2022-02-20'\n  )\\\n  .show(5)\n\nPy4JError: An error occurred while calling o216.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n    at py4j.Gateway.invoke(Gateway.java:274)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nIn the above examples, we have logical expressions that are dependent on each other. But, lets suppose these conditions were independent. In this case, we would use the or keyword, instead of and. Now, Spark will look for every row of transf where transferValue is smaller than 1000, or, clientNumber is equal to 1297, or, dateTransfer is greater than 20 of February 2022.\n\ncondition = '''\n  transferValue &lt; 1000\n  or clientNumber == 1297 \n  or dateTransfer &gt; '2022-02-20'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nTo translate this expression into the pythonic way, we have to substitute the or keyword by the | operator, and surround each expression by parentheses again:\n\ntransf\\\n  .filter(\n    (col('transferValue') &lt; 1000) |\n    (col('clientNumber') == 1297) |\n    (col('dateTransfer') &gt; '2022-02-20')\n  )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nYou can also increase the complexity of your logical expressions by mixing dependent expressions with independent expressions. For example, to filter all the rows where dateTransfer is greater than or equal to 01 of October 2022, and clientNumber is either 2727 or 5188, you would have the following code:\n\ncondition = '''\n  (clientNumber == 2727 or clientNumber == 5188)\n  and dateTransfer &gt;= '2022-10-01'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-29|2022-12-29 10:22:02|        2727|      4666.25|          euro €|  20223541|       NULL|                   33|                 5420|               83070-8|\n|  2022-12-27|2022-12-27 03:58:25|        5188|      7821.69|        dollar $|  20223522|       NULL|                   33|                 4078|               46571-3|\n|  2022-12-26|2022-12-25 23:45:02|        2727|      3261.73| british pound £|  20223515|       NULL|                  421|                 6317|               66040-9|\n|  2022-12-23|2022-12-23 05:32:49|        2727|       8042.0|        dollar $|  20223496|       NULL|                  290|                 5420|               37759-7|\n|  2022-12-22|2022-12-22 06:02:47|        5188|      8175.67|        dollar $|  20223490|       NULL|                  666|                 8800|               42657-8|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you investigate the above condition carefully, maybe, you will identify that this condition could be rewritten in a simpler format, by using the in keyword. This way, Spark will look for all the rows where clientNumber is equal to one of the listed values (2727 or 5188), and, that dateTransfer is greater than or equal to 01 of October 2022.\n\ncondition = '''\n  clientNumber in (2727, 5188)\n  and dateTransfer &gt;= '2022-10-01'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-29|2022-12-29 10:22:02|        2727|      4666.25|          euro €|  20223541|       NULL|                   33|                 5420|               83070-8|\n|  2022-12-27|2022-12-27 03:58:25|        5188|      7821.69|        dollar $|  20223522|       NULL|                   33|                 4078|               46571-3|\n|  2022-12-26|2022-12-25 23:45:02|        2727|      3261.73| british pound £|  20223515|       NULL|                  421|                 6317|               66040-9|\n|  2022-12-23|2022-12-23 05:32:49|        2727|       8042.0|        dollar $|  20223496|       NULL|                  290|                 5420|               37759-7|\n|  2022-12-22|2022-12-22 06:02:47|        5188|      8175.67|        dollar $|  20223490|       NULL|                  666|                 8800|               42657-8|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.3 Translating the in keyword to the pythonic way\nPython does have a in keyword just like SQL, but, this keyword does not work as expected in pyspark. To write a logical expression, using the pythonic way, that filters the rows where a column is equal to one of the listed values, you can use the isin() method.\nThis method belongs to the Column class, so, you should always use isin() after a column name or a col() function. In the example below, we are filtering the rows where destinationBankNumber is 290 or 666:\n\ntransf\\\n  .filter(col('destinationBankNumber').isin(290, 666))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:44:46|        1121|       7158.0|          zing ƒ|  20223558|       NULL|                  290|                 1100|               35424-4|\n|  2022-12-31|2022-12-31 01:02:06|        4862|       6714.0|        dollar $|  20223557|       NULL|                  666|                 1002|               71839-1|\n|  2022-12-31|2022-12-31 00:48:47|        3294|     10882.52|        dollar $|  20223556|       NULL|                  666|                 2231|               50190-5|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.4 Negating logical conditions\n\nIn some cases, is easier to describe what rows you do not want in your filter. That is, you want to negate (or invert) your logical expression. For this, SQL provides the not keyword, that you place before the logical expression you want to negate.\nFor example, we can filter all the rows of transf where clientNumber is not equal to 3284. Remember, the methods filter() and where() are equivalents or synonymous (they both mean the same thing).\n\ncondition = '''\n  not clientNumber == 3284\n'''\n\ntransf\\\n  .where(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nTo translate this expression to the pythonic way, we use the ~ operator. However, because we are negating the logical expression as a whole, is important to surround the entire expression with parentheses.\n\ntransf\\\n  .where(~(col('clientNumber') == 3284))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you forget to add the parentheses, Spark will think you are negating just the column, and not the entire expression. That would not make sense, and, as a result, Spark would throw an error:\n\ntransf\\\n  .where(~col('clientNumber') == 3284)\\\n  .show(5)\n\nAnalysisException: cannot resolve '(NOT clientNumber)' due to data type mismatch: argument 1 requires boolean type, however, 'clientNumber' is of bigint type.;\n'Filter (NOT clientNumber#210L = 3284)\nBecause the ~ operator is a little discrete and can go unnoticed, I sometimes use a different approach to negate my logical expressions. I make the entire expression equal to False. This way, I get all the rows where that particular expression is False. This makes my intention more visible in the code, but, is harder to write it.\n\n# Filter all the rows where `clientNumber` is not equal to\n# 2727 or 5188.\ntransf\\\n  .where( (col('clientNumber').isin(2727, 5188)) == False )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.5 Filtering null values (i.e. missing data)\nSometimes, the null values play an important role in your filter. You either want to collect all these null values, so you can investigate why they are null in the first place, or, you want to completely eliminate them from your DataFrame.\nBecause this is a special kind of value in Spark, with a special meaning (the “absence” of a value), you need to use a special syntax to correctly filter these values in your DataFrame. In SQL, you can use the is keyword to filter these values:\n\ntransf\\\n  .where('transferLog is null')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nHowever, if you want to remove these values from your DataFrame, then, you can just negate (or invert) the above expression with the not keyword, like this:\n\ntransf\\\n  .where('not transferLog is null')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|         transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|  2022-12-05|2022-12-05 00:51:00|        2197|      8240.62|          zing ƒ|  20223383| 408 Request Timeout|                  666|                 1100|               58503-9|\n|  2022-09-20|2022-09-19 21:59:51|        5188|       7583.9|        dollar $|  20222912|500 Server Unavai...|                  290|                 1979|               85242-1|\n|  2022-09-03|2022-09-03 06:07:59|        3795|       3654.0|          zing ƒ|  20222814| 408 Request Timeout|                  290|                 9921|               60494-5|\n|  2022-07-02|2022-07-02 15:29:50|        4465|       5294.0|        dollar $|  20222408|500 Server Unavai...|                  421|                 2400|               39070-3|\n|  2022-06-14|2022-06-14 10:21:55|        1121|       7302.0|        dollar $|  20222273| 408 Request Timeout|                  666|                 5420|               47709-2|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nThe is and not keywords in SQL have a special relation. Because you can create the same negation/inversion of the expression by inserting the not keyword in the middle of the expression (you can do this too in expressions with the in keyword). In other words, you might see, in someone else’s code, the same expression above written in this form:\n\ntransf\\\n  .where('transferLog is not null')\\\n  .show(5)\n\nBoth forms are equivalent and valid SQL logical expressions. But the latter is a strange version. Because we cannot use the not keyword in this manner on other kinds of logical expressions. Normally, we put the not keyword before the logical expression we want to negate, not in the middle of it. Anyway, just have in mind that this form of logical expression exists, and, that is a perfectly valid one.\nWhen we translate the above examples to the “pythonic” way, many people tend to use the null equivalent of python, that is, the None value, in the expression. But as you can see in the result below, this method does not work as expected:\n\ntransf\\\n  .where(col('transferLog') == None)\\\n  .show(5)\n\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nThe correct way to do this in pyspark, is to use the isNull() method from the Column class.\n\ntransf\\\n  .where(col('transferLog').isNull())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you want to eliminate the null values, just use the inverse method isNotNull().\n\ntransf\\\n  .where(col('transferLog').isNotNull())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|         transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|  2022-12-05|2022-12-05 00:51:00|        2197|      8240.62|          zing ƒ|  20223383| 408 Request Timeout|                  666|                 1100|               58503-9|\n|  2022-09-20|2022-09-19 21:59:51|        5188|       7583.9|        dollar $|  20222912|500 Server Unavai...|                  290|                 1979|               85242-1|\n|  2022-09-03|2022-09-03 06:07:59|        3795|       3654.0|          zing ƒ|  20222814| 408 Request Timeout|                  290|                 9921|               60494-5|\n|  2022-07-02|2022-07-02 15:29:50|        4465|       5294.0|        dollar $|  20222408|500 Server Unavai...|                  421|                 2400|               39070-3|\n|  2022-06-14|2022-06-14 10:21:55|        1121|       7302.0|        dollar $|  20222273| 408 Request Timeout|                  666|                 5420|               47709-2|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.6 Filtering dates and datetimes in your DataFrame\nJust as a quick side note, when you want to filter rows of your DataFrame that fits into a particular date, you can easily write this particular date as a single string, like in the example below:\n\ndf_0702 = transf\\\n  .where(col('dateTransfer') == '2022-07-02')\n\nWhen filtering datetimes you can also write the datetimes as strings, like this:\n\nlater_0330_pm = transf.where(\n    col('datetimeTransfer') &gt; '2022-07-02 03:30:00'\n)\n\nHowever, is a better practice to write these particular values using the built-in date and datetime classes of python, like this:\n\nfrom datetime import date, datetime\n\nd = date(2022,2,7)\ndt = datetime(2022,2,7,3,30,0)\n\n# Filter all rows where `dateTransfer`\n# is equal to \"2022-07-02\"\ndf_0702 = transf.where(\n    col('dateTransfer') == d\n)\n\n# Filter all rows where `datetimeTransfer`\n# is greater than \"2022-07-02 03:30:00\"\nlater_0330_pm = transf.where(\n    col('datetimeTransfer') &gt; dt\n)\n\nWhen you translate the above expressions to SQL, you can also write the date and datetime values as strings. However, is also a good idea to use the CAST() SQL function to convert these string values into the correct data type before the actual filter. like this:\n\ncondition_d = '''\ndateTransfer == CAST(\"2022-07-02\" AS DATE)\n'''\n\ncondition_dt = '''\ndateTransfer &gt; CAST(\"2022-07-02 03:30:00\" AS TIMESTAMP)\n'''\n\n# Filter all rows where `dateTransfer`\n# is equal to \"2022-07-02\"\ndf_0702 = transf.where(condition_d)\n\n# Filter all rows where `datetimeTransfer`\n# is greater than \"2022-07-02 03:30:00\"\nlater_0330_pm = transf.where(condition_dt)\n\nIn other words, with the SQL expression CAST(\"2022-07-02 03:30:00\" AS TIMESTAMP) we are telling Spark to convert the literal string \"2022-07-02 03:30:00\" into an actual timestamp value.\n\n\n5.5.7 Searching for a particular pattern in string values\nSpark offers different methods to search a particular pattern within a string value. In this section, I want to describe how you can use these different methods to find specific rows in your DataFrame, that fit into the description of these patterns.\n\n5.5.7.1 Starts with, ends with and contains\nYou can use the column methods startswith(), endswith() and contains(), to search for rows where a input string value starts with, ends with, or, contains a particular pattern, respectively.\nThese three methods returns a boolean value that indicates if the input string value matched the input pattern that you gave to the method. And you can use these boolean values they return to filter the rows of your DataFrame that fit into the description of these patterns.\nJust as an example, in the following code, we are creating a new DataFrame called persons, that contains the description of 3 persons (Alice, Bob and Charlie). And I use these three methods to search for different rows in the DataFrame:\n\nfrom pyspark.sql.functions import col\n\npersons = spark.createDataFrame(\n  [\n    ('Alice', 25),\n    ('Bob', 30),\n    ('Charlie', 35)\n  ],\n  ['name', 'age']\n)\n\n# Filter the DataFrame to include only rows\n# where the \"name\" column starts with \"A\"\npersons.filter(col('name').startswith('A'))\\\n  .show()\n\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 25|\n+-----+---+\n\n\n\n\n# Filter the DataFrame to include only rows\n# where the \"name\" column ends with \"e\"\npersons.filter(col('name').endswith('e'))\\\n  .show()\n\n+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 25|\n|Charlie| 35|\n+-------+---+\n\n\n\n\n# Filter the DataFrame to include only rows\n# where the \"name\" column contains \"ob\"\npersons.filter(col('name').contains('ob'))\\\n  .show()\n\n+----+---+\n|name|age|\n+----+---+\n| Bob| 30|\n+----+---+\n\n\n\n\n\n5.5.7.2 Using regular expressions or SQL LIKE patterns\nIn Spark, you can also use a particular “SQL LIKE pattern” or a regular pattern (a.k.a. regex) to filter the rows of a DataFrame, by using the Column methods like() and rlike().\nIn essence, the like() method is the pyspark equivalent of the LIKE SQL operator. As a result, this like() method expects a SQL pattern as input. This means that you can use the SQL metacharacters % (to match any number of characters) and _ (to match exactly one character) inside this pattern.\n\ntransf\\\n  .where(col('transferCurrency').like('british%'))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-30|2022-12-30 11:30:23|        1455|       5141.0| british pound £|  20223552|       NULL|                  421|                 6552|               62671-3|\n|  2022-12-30|2022-12-30 02:35:23|        5986|       6076.0| british pound £|  20223550|       NULL|                   33|                 4078|               83994-4|\n|  2022-12-29|2022-12-29 15:24:04|        4862|       5952.0| british pound £|  20223544|       NULL|                  666|                 1002|               37736-6|\n|  2022-12-29|2022-12-29 14:16:46|        2197|       8771.0| british pound £|  20223543|       NULL|                   33|                 1200|               32390-2|\n|  2022-12-29|2022-12-29 06:51:24|        5987|       2345.0| british pound £|  20223539|       NULL|                   33|                 2231|               70909-9|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nAlthough the style of pattern matching used by like() being very powerful, you might need to use more powerful and flexible patterns. In those cases, you can use the rlike() method, which accepts a regular expression as input. In the example below, I am filtering rows where destinationBankAccount starts by the characters 54.\n\nregex = '^54([0-9]{3})-[0-9]$'\ntransf\\\n  .where(col('destinationBankAccount').rlike(regex))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-29|2022-12-29 02:54:23|        2197|       5752.0| british pound £|  20223536|       NULL|                  666|                 8800|               54159-1|\n|  2022-12-27|2022-12-27 04:51:45|        4862|      11379.0|        dollar $|  20223523|       NULL|                   33|                 4425|               54796-3|\n|  2022-12-05|2022-12-05 05:50:27|        4965|       5986.0| british pound £|  20223386|       NULL|                  421|                 1200|               54118-1|\n|  2022-12-04|2022-12-04 14:31:42|        4965|       8123.0|        dollar $|  20223380|       NULL|                  666|                 3321|               54912-2|\n|  2022-11-29|2022-11-29 16:23:07|        4862|       8060.0|          zing ƒ|  20223351|       NULL|                  421|                 8800|               54194-8|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#selecting-a-subset-of-rows-from-your-dataframe",
    "href": "Chapters/05-transforming.html#selecting-a-subset-of-rows-from-your-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.6 Selecting a subset of rows from your DataFrame",
    "text": "5.6 Selecting a subset of rows from your DataFrame\nAt some point, you might need to use just a small piece of your DataFrame over the next steps of your pipeline, and not the entire thing. For example, you may want to select just the fisrt (or last) 5 rows of this DataFrame, or, maybe, you need to take a random sample of rows from it.\nIn this section I will discuss the main methods offered by Spark to deal with these scenarios. Each method returns a subset of rows from the original DataFrame as a result. But each method works differently from the other, and uses a different strategy to retrieve this subset.\n\n5.6.1 Limiting the number of rows in your DataFrame\nThe limit() method is very similar to the LIMIT SQL keyword. It limits the number of rows present in your DataFrame to a specific amount. So, if I run transf.limit(1) I get a new DataFrame as a result, which have only a single row from the transf DataFrame. As you can see below:\n\nsingle_transfer = transf.limit(1)\nsingle_transfer.show()\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nIs worth mentioning that the limit() method will always try to limit your original DataFrame, to the first \\(n\\) rows. This means that the command df.limit(430) tries to limit the df DataFrame to its first 430 rows.\nThis also means that 430 is the maximum number of rows that will be taken from the df DataFrame. So, if df DataFrame has less than 430 lines, like 14 rows, than, nothing will happen, i.e. the result of df.limit(430) will be equivalent to the df DataFrame itself.\n\n\n5.6.2 Getting the first/last \\(n\\) rows of your DataFrame\nThe methods head() and tail() allows you to collect the first/last \\(n\\) rows of your DataFrame, respectively. One key aspect from these methods, is that they return a list of Row values, instead of a new DataFrame (such as the limit() method). You can compare these methods to the take() and collect() methods that we introduced at Section 5.2, because they both produce a list of Row values as well.\nNow, the head() method produce the same output as the take() method. However, these two methods work very differently under the hoods, and, are recommended to be used in different scenarios.\nMore specifically, if you have a big DataFrame (i.e. a DataFrame with many rows) is recommended to use take() (instead of head()) to collect the first \\(n\\) rows from it. Because the head() method makes Spark to load the entire DataFrame into the driver’s memory, and this can easily cause an “out of memory” situation for big DataFrames. So, use the head() method only for small DataFrames.\nIn the example below, we are using these methods to get the first and last 2 rows of the transf DataFrame:\n\n# First 2 rows of `transf` DataFrame:\nfirst2 = transf.head(2)\n# Last 2 rows of `transf` DataFrame:\nlast2 = transf.tail(2)\n\nprint(last2)\n\n[Row(dateTransfer=datetime.date(2022, 1, 1), datetimeTransfer=datetime.datetime(2022, 1, 1, 4, 7, 44), clientNumber=5987, transferValue=8640.06, transferCurrency='dollar $', transferID=20221144, transferLog=None, destinationBankNumber=666, destinationBankBranch=6552, destinationBankAccount='70021-4'), Row(dateTransfer=datetime.date(2022, 1, 1), datetimeTransfer=datetime.datetime(2022, 1, 1, 3, 56, 58), clientNumber=6032, transferValue=5076.61, transferCurrency='dollar $', transferID=20221143, transferLog=None, destinationBankNumber=33, destinationBankBranch=8800, destinationBankAccount='41326-5')]\n\n\n\n\n5.6.3 Taking a random sample of your DataFrame\nWith the sample() you can take a random sample of rows from your DataFrame. In other words, this method returns a new DataFrame with a random subset of rows from the original DataFrame.\nThis method have three main arguments, which are:\n\nwithReplacement: a boolean value indicating if the samples are with replacement or not. Defaults to False;\nfraction: the fraction of rows you want to sample from the DataFrame. Have to be a positive float value, from 0 to 1;\nseed: an integer representing the seed for the sampling process. This is an optional argument;\n\nIn the example below, we are trying to get a sample that represents 15% of the original transf DataFrame, and using the integer 24 as our sampling seed:\n\ntransf_sample = transf.sample(fraction = 0.15, seed = 24)\ntransf_sample.show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 01:02:06|        4862|       6714.0|        dollar $|  20223557|       NULL|                  666|                 1002|               71839-1|\n|  2022-12-30|2022-12-30 00:18:25|        5832|       6333.0|        dollar $|  20223548|       NULL|                  666|                 8800|               78901-8|\n|  2022-12-29|2022-12-29 06:51:24|        5987|       2345.0| british pound £|  20223539|       NULL|                   33|                 2231|               70909-9|\n|  2022-12-27|2022-12-27 14:08:01|        3294|      6617.17|        dollar $|  20223526|       NULL|                  666|                 2231|               49767-2|\n|  2022-12-26|2022-12-26 11:25:09|        5832|       8178.0|          euro €|  20223517|       NULL|                  290|                 8521|               39648-9|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIn other words, the fraction argument represents a fraction of the total number of rows in the original DataFrame. Since the transf DataFrame have 2421 rows in total, by setting the fraction argument to 0.15, we are asking Spark to collect a sample from transf that have approximately \\(0.15 \\times 2421 \\approx 363\\) rows.\nIf we calculate the number of rows in transf_sample DataFrame, we can see that this DataFrame have a number of rows close to 363:\n\ntransf_sample.count()\n\n355\n\n\nFurthermore, the sampling seed is just a way to ask Spark to produce the same random sample of the original DataFrame. That is, the sampling seed makes the result sample fixed. You always get the same random sample when you run the sample() method.\nOn the other hand, when you do not set the seed argument, then, Spark will likely produce a different random sample of the original DataFrame every time you run the sample() method.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#managing-the-columns-of-your-dataframe",
    "href": "Chapters/05-transforming.html#managing-the-columns-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.7 Managing the columns of your DataFrame",
    "text": "5.7 Managing the columns of your DataFrame\nSometimes, you need manage or transform the columns you have. For example, you might need to change the order of these columns, or, to delete/rename some of them. To do this, you can use the select() and drop() methods of your DataFrame.\nThe select() method works very similarly to the SELECT statement of SQL. You basically list all the columns you want to keep in your DataFrame, in the specific order you want.\n\ntransf\\\n  .select(\n    'transferID', 'datetimeTransfer',\n    'clientNumber', 'transferValue'\n  )\\\n  .show(5)\n\n+----------+-------------------+------------+-------------+\n|transferID|   datetimeTransfer|clientNumber|transferValue|\n+----------+-------------------+------------+-------------+\n|  20223563|2022-12-31 14:00:24|        5516|      7794.31|\n|  20223562|2022-12-31 10:32:07|        4965|       7919.0|\n|  20223561|2022-12-31 07:37:02|        4608|       5603.0|\n|  20223560|2022-12-31 07:35:05|        1121|      4365.22|\n|  20223559|2022-12-31 02:53:44|        1121|       4620.0|\n+----------+-------------------+------------+-------------+\nonly showing top 5 rows\n\n\n\n\n5.7.1 Renaming your columns\nRealize in the example above, that the column names can be delivered directly as strings to select(). This makes life pretty easy, but, it does not give you extra options.\nFor example, you might want to rename some of the columns, and, to do this, you need to use the alias() method from Column class. Since this is a method from Column class, you need to use it after a col() or column() function, or, after a column name using the dot operator.\n\ntransf\\\n  .select(\n    'datetimeTransfer',\n    col('transferID').alias('ID_of_transfer'),\n    transf.clientNumber.alias('clientID')\n  )\\\n  .show(5)\n\n+-------------------+--------------+--------+\n|   datetimeTransfer|ID_of_transfer|clientID|\n+-------------------+--------------+--------+\n|2022-12-31 14:00:24|      20223563|    5516|\n|2022-12-31 10:32:07|      20223562|    4965|\n|2022-12-31 07:37:02|      20223561|    4608|\n|2022-12-31 07:35:05|      20223560|    1121|\n|2022-12-31 02:53:44|      20223559|    1121|\n+-------------------+--------------+--------+\nonly showing top 5 rows\n\n\n\nBy using this alias() method, you can rename multiple columns within a single select() call. However, you can use the withColumnRenamed() method to rename just a single column of your DataFrame. The first argument of this method, is the current name of this column, and, the second argument, is the new name of this column.\n\ntransf\\\n  .withColumnRenamed('clientNumber', 'clientID')\\\n  .show(5)\n\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientID|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|    5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|    4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|    4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|    1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|    1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.7.2 Dropping unnecessary columns\nIn some cases, your DataFrame just have too many columns and you just want to eliminate a few of them. In a situation like this, you can list the columns you want to drop from your DataFrame, inside the drop() method, like this:\n\ntransf\\\n  .drop('dateTransfer', 'clientNumber')\\\n  .show(5)\n\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|   datetimeTransfer|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|2022-12-31 14:00:24|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|2022-12-31 10:32:07|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|2022-12-31 07:37:02|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|2022-12-31 07:35:05|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|2022-12-31 02:53:44|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.7.3 Casting columns to a different data type\nSpark try to do its best when guessing which is correct data type for the columns of your DataFrame. But, obviously, Spark can get it wrong, and, you end up deciding by your own which data type to use for a specific column.\nTo explicit transform a column to a specific data type, you can use cast() or astype() methods inside select(). The astype() method is just an alias to cast(). This cast() method is very similar to the CAST() function in SQL, and belongs to the Column class, so, you should always use it after a column name with the dot operator, or, a col()/column() function:\n\ntransf\\\n  .select(\n    'transferValue',\n    col('transferValue').cast('long').alias('value_as_integer'),\n    transf.transferValue.cast('boolean').alias('value_as_boolean')\n  )\\\n  .show(5)\n\n+-------------+----------------+----------------+\n|transferValue|value_as_integer|value_as_boolean|\n+-------------+----------------+----------------+\n|      7794.31|            7794|            true|\n|       7919.0|            7919|            true|\n|       5603.0|            5603|            true|\n|      4365.22|            4365|            true|\n|       4620.0|            4620|            true|\n+-------------+----------------+----------------+\nonly showing top 5 rows\n\n\n\nTo use cast() or astype() methods, you give the name of the data type (as a string) to which you want to cast the column. The main available data types to cast() or astype() are:\n\n'string': correspond to StringType();\n'int': correspond to IntegerType();\n'long': correspond to LongType();\n'double': correspond to DoubleType();\n'date': correspond to DateType();\n'timestamp': correspond to TimestampType();\n'boolean': correspond to BooleanType();\n'array': correspond to ArrayType();\n'dict': correspond to MapType();\n\n\n\n5.7.4 You can add new columns with select()\nWhen I said that select() works in the same way as the SELECT statement of SQL, I also meant that you can use select() to select columns that do not currently exist in your DataFrame, and add them to the final result.\nFor example, I can select a new column (called by_1000) containing value divided by 1000, like this:\n\ntransf\\\n  .select(\n    'transferValue',\n    (col('transferValue') / 1000).alias('by_1000')\n  )\\\n  .show(5)\n\n+-------------+-------+\n|transferValue|by_1000|\n+-------------+-------+\n|      7794.31|7.79431|\n|       7919.0|  7.919|\n|       5603.0|  5.603|\n|      4365.22|4.36522|\n|       4620.0|   4.62|\n+-------------+-------+\nonly showing top 5 rows\n\n\n\nThis by_1000 column do not exist in transf DataFrame. It was calculated and added to the final result by select(). The formula col('transferValue') / 1000 is the equation that defines what this by_1000 column is, or, how it should be calculated.\nBesides that, select() provides a useful shortcut to reference all the columns of your DataFrame. That is, the star symbol (*) from the SELECT statement in SQL. This shortcut is very useful when you want to maintain all columns, and, add a new column, at the same time.\nIn the example below, we are adding the same by_1000 column, however, we are bringing all the columns of transf together.\n\ntransf\\\n  .select(\n    '*',\n    (col('transferValue') / 1000).alias('by_1000')\n  )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|by_1000|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|7.79431|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|  7.919|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|  5.603|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|4.36522|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|   4.62|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#calculating-or-adding-new-columns-to-your-dataframe",
    "href": "Chapters/05-transforming.html#calculating-or-adding-new-columns-to-your-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.8 Calculating or adding new columns to your DataFrame",
    "text": "5.8 Calculating or adding new columns to your DataFrame\nAlthough you can add new columns with select(), this method is not specialized to do that. As consequence, when you want to add many new columns, it can become pretty annoying to write select('*', new_column) over and over again. That is why pyspark provides a special method called withColumn().\nThis method has two arguments. First, is the name of the new column. Second, is the formula (or the equation) that represents this new column. As an example, I could reproduce the same by_1000 column like this:\n\ntransf\\\n  .withColumn('by_1000', col('transferValue') / 1000)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|by_1000|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|7.79431|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|  7.919|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|  5.603|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|4.36522|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|   4.62|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\nonly showing top 5 rows\n\n\n\nA lot of the times we use the functions from pyspark.sql.functions module to produce such formulas used by withColumn(). You can checkout the complete list of functions present in this module, by visiting the official documentation of pyspark2.\nYou will see a lot more examples of formulas and uses of withColumn() throughout this book. For now, I just want you to know that withColumn() is a method that adds a new column to your DataFrame. The first argument is the name of the new column, and, the second argument is the calculation formula of this new column.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#sorting-rows-of-your-dataframe",
    "href": "Chapters/05-transforming.html#sorting-rows-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.9 Sorting rows of your DataFrame",
    "text": "5.9 Sorting rows of your DataFrame\nSpark, or, pyspark, provides the orderBy() and sort() DataFrame method to sort rows. They both work the same way: you just give the name of the columns that Spark will use to sort the rows.\nIn the example below, Spark will sort transf according to the values in the transferValue column. By default, Spark uses an ascending order while sorting your rows.\n\ntransf\\\n  .orderBy('transferValue')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-07-22|2022-07-22 16:06:25|        3795|         60.0|        dollar $|  20222533|       NULL|                  290|                 1100|               39925-1|\n|  2022-05-09|2022-05-09 14:02:15|        3284|        104.0|        dollar $|  20222033|       NULL|                  666|                 2231|               74766-2|\n|  2022-09-16|2022-09-16 20:35:40|        3294|       129.09|          zing ƒ|  20222896|       NULL|                  290|                 3321|               60867-9|\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       NULL|                  421|                 5420|               43088-1|\n|  2022-08-20|2022-08-20 09:27:55|        2727|        160.0|        dollar $|  20222724|       NULL|                   33|                 1002|               75581-5|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nJust to be clear, you can use the combination between multiple columns to sort your rows. Just give the name of each column (as strings) separated by commas. In the example below, Spark will sort the rows according to clientNumber column first, then, is going to sort the rows of each clientNumber according to transferValue column.\n\ntransf\\\n  .orderBy('clientNumber', 'transferValue')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-03-30|2022-03-30 11:57:22|        1121|        461.0|          euro €|  20221738|       NULL|                  666|                 6552|               35568-9|\n|  2022-05-23|2022-05-23 11:51:02|        1121|       844.66| british pound £|  20222127|       NULL|                  421|                 1002|               32340-0|\n|  2022-08-24|2022-08-24 13:51:30|        1121|      1046.93|          euro €|  20222748|       NULL|                  421|                 6317|               38887-3|\n|  2022-09-23|2022-09-23 19:49:19|        1121|       1327.0| british pound £|  20222938|       NULL|                  290|                 5420|               77350-1|\n|  2022-06-25|2022-06-25 17:07:08|        1121|       1421.0|        dollar $|  20222361|       NULL|                  290|                 9921|               77258-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you want to use a descending order in a specific column, you need to use the desc() method from Column class. In the example below, Spark will sort the rows according to clientNumber column using an ascending order. However, it will use the values from transferValue column in a descending order to sort the rows in each clientNumber.\n\ntransf\\\n  .orderBy('clientNumber', col('transferValue').desc())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-08-18|2022-08-18 13:57:12|        1121|     11490.37|          zing ƒ|  20222712|       NULL|                  421|                 2400|               61244-9|\n|  2022-11-05|2022-11-05 08:00:37|        1121|     10649.59|        dollar $|  20223197|       NULL|                  421|                 3321|               40011-2|\n|  2022-05-17|2022-05-17 10:27:05|        1121|     10471.23| british pound £|  20222086|       NULL|                  666|                 8521|               26534-1|\n|  2022-05-15|2022-05-15 00:25:49|        1121|      10356.0|        dollar $|  20222075|       NULL|                   33|                 1979|               28234-7|\n|  2022-06-10|2022-06-09 23:51:39|        1121|      10142.0|        dollar $|  20222241|       NULL|                   33|                 2400|               36594-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nThis means that you can mix ascending orders with descending orders in orderBy(). Since the ascending order is the default, if you want to use a descending order in all of the columns, then, you need to apply the desc() method to all of them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#calculating-aggregates",
    "href": "Chapters/05-transforming.html#calculating-aggregates",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "5.10 Calculating aggregates",
    "text": "5.10 Calculating aggregates\nTo calculate aggregates of a Spark DataFrame we have two main paths: 1) we can use some standard DataFrame methods, like count() or sum(), to calculate a single aggregate; 2) or, we can use the agg() method to calculate multiple aggregates at the same time.\n\n5.10.1 Using standard DataFrame methods\nThe Spark DataFrame class by itself provides a single aggregate method, which is count(). With this method, you can find out how many rows your DataFrame have. In the example below, we can see that transf have 2421 rows.\n\ntransf.count()\n\n2421\n\n\nHowever, if you have a grouped DataFrame (we will learn more about these objects very soon), pyspark provides some more aggregate methods, which are listed below:\n\nmean(): calculate the average value of each numeric column;\nsum(): return the total sum of a column;\ncount(): count the number of rows;\nmax(): compute the maximum value of a column;\nmin(): compute the minimum value of a column;\n\nThis means that you can use any of the above methods after a groupby() call, to calculate aggregates per group in your Spark DataFrame. For now, lets forget about this “groupby” detail, and learn how to calculate different aggregates by using the agg() method.\n\n\n5.10.2 Using the agg() method\nWith the agg() method, we can calculate many different aggregates at the same time. In this method, you should provide a expression that describes what aggregate measure you want to calculate.\nIn most cases, this “aggregate expression” will be composed of functions from the pyspark.sql.functions module. So, having familiarity with the functions present in this module will help you to compose the formulas of your aggregations in agg().\nIn the example below, I am using the sum() and mean() functions from pyspark.sql.functions to calculate the total sum and the total mean of the transferValue column in the transf DataFrame. I am also using the countDistinct() function to calculate the number of distinct values in the clientNumber column.\n\nimport pyspark.sql.functions as F\n\ntransf.agg(\n    F.sum('transferValue').alias('total_value'),\n    F.mean('transferValue').alias('mean_value'),\n    F.countDistinct('clientNumber').alias('number_of_clients')\n  )\\\n  .show()\n\n+--------------------+-----------------+-----------------+\n|         total_value|       mean_value|number_of_clients|\n+--------------------+-----------------+-----------------+\n|1.5217690679999998E7|6285.704535315985|               26|\n+--------------------+-----------------+-----------------+\n\n\n\n\n\n5.10.3 Without groups, we calculate a aggregate of the entire DataFrame\nWhen we do not define any group for the input DataFrame, agg() always produce a new DataFrame with one single row (like in the above example). This happens because we are calculating aggregates of the entire DataFrame, that is, a set of single values (or single measures) that summarizes (in some way) the entire DataFrame.\nIn the other hand, when we define groups in a DataFrame (by using the groupby() method), the calculations performed by agg() are made inside each group in the DataFrame. In other words, instead of summarizing the entire DataFrame, agg() will produce a set of single values that describes (or summarizes) each group in the DataFrame.\nThis means that each row in the resulting DataFrame describes a specific group in the original DataFrame, and, agg() usually produces a DataFrame with more than one single row when its calculations are performed by group. Because our DataFrames usually have more than one single group.\n\n\n5.10.4 Calculating aggregates per group in your DataFrame\nBut how you define the groups inside your DataFrame? To do this, we use the groupby() and groupBy() methods. These methods are both synonymous (they do the same thing).\nThese methods, produce a grouped DataFrame as a result, or, in more technical words, a object of class pyspark.sql.group.GroupedData. You just need to provide, inside this groupby() method, the name of the columns that define (or “mark”) your groups.\nIn the example below, I am creating a grouped DataFrame per client defined in the clientNumber column. This means that each distinct value in the clientNumber column defines a different group in the DataFrame.\n\ntransf_per_client = transf.groupBy('clientNumber')\ntransf_per_client\n\nGroupedData[grouping expressions: [clientNumber], value: [dateTransfer: date, datetimeTransfer: timestamp ... 8 more fields], type: GroupBy]\n\n\nAt first, it appears that nothing has changed. But the groupBy() method always returns a new object of class pyspark.sql.group.GroupedData. As a consequence, we can no longer use some of the DataFrame methods that we used before, like the show() method to see the DataFrame.\nThat’s ok, because we usually do not want to keep this grouped DataFrame for much time. This grouped DataFrame is just a passage (or a bridge) to the result we want, which is, to calculate aggregates per group of the DataFrame.\nAs an example, I can use the max() method, to find out which is the highest value that each user have tranfered, like this:\n\ntransf_per_client\\\n  .max('transferValue')\\\n  .show(5)\n\n+------------+------------------+\n|clientNumber|max(transferValue)|\n+------------+------------------+\n|        1217|           12601.0|\n|        2489|          12644.56|\n|        3284|          12531.84|\n|        4608|          10968.31|\n|        1297|           11761.0|\n+------------+------------------+\nonly showing top 5 rows\n\n\n\nRemember that, by using standard DataFrame methods (like max() in the example above) we can calculate only a single aggregate value. But, with agg() we can calculate more than one aggregate value at the same time. Since our transf_per_client object is a grouped DataFrame, agg() will calculate these aggregates per group.\nAs an example, if I apply agg() with the exact same expressions exposed on Section 5.10.2 with the transf_per_client DataFrame, instead of a DataFrame with one single row, I get a new DataFrame with nine rows. In each row, I have the total and mean values for a specific user in the input DataFrame.\n\ntransf_per_client\\\n  .agg(\n    F.sum('transferValue').alias('total_value'),\n    F.mean('transferValue').alias('mean_value')\n  )\\\n  .show(5)\n\n+------------+-----------------+------------------+\n|clientNumber|      total_value|        mean_value|\n+------------+-----------------+------------------+\n|        1217|575218.2099999998| 6185.142043010751|\n|        2489|546543.0900000001| 6355.152209302327|\n|        3284|581192.5700000001| 6054.089270833334|\n|        4608|        448784.44| 6233.117222222222|\n|        1297|594869.6699999999|6196.5590624999995|\n+------------+-----------------+------------------+\nonly showing top 5 rows\n\n\n\n\n\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/05-transforming.html#footnotes",
    "href": "Chapters/05-transforming.html#footnotes",
    "title": "5  Transforming your Spark DataFrame - Part 1",
    "section": "",
    "text": "https://github.com/pedropark99/Introd-pyspark/tree/main/Data↩︎\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 1</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#sec-read-files",
    "href": "Chapters/07-import.html#sec-read-files",
    "title": "6  Importing data to Spark",
    "section": "6.1 Reading data from static files",
    "text": "6.1 Reading data from static files\nStatic files are probably the easiest way to transport data from one computer to another. Because you just need to copy and paste this file to the other machine, or download it from the internet.\nBut in order to Spark read any type of static file stored inside your computer, it always need to know the path to this file. Every OS have its own file system, and every file in your computer is stored in a specific address of this file system. The “path” to this file is the path (or “steps”) that your computer needs to follow to reach this specific address, where the file is stored.\nAs we pointed out earlier, to read any static file in Spark, you use one of the available “read engines”, which are in the spark.read module of your Spark Session. This means that, each read engine in this module is responsible for reading a specific file format.\nIf you want to read a CSV file for example, you use the spark.read.csv() engine. In contrast, if you want to read a JSON file, you use the spark.read.json() engine instead. But no matter what read engine you use, you always give the path to your file to any of these “read engines”.\nThe main read engines available in Spark are:\n\nspark.read.json(): to read JSON files.\nspark.read.csv(): to read CSV files.\nspark.read.parquet(): to read Apache Parquet files.\nspark.read.orc(): to read ORC (Apache Optimized Row Columnar format) files.\nspark.read.text(): to read text files.\nspark.read.jdbc(): to read data from databases using the JDBC API.\n\nFor example, to read a JSON file called sales.json that is stored in my Data folder, I can do this:\n\njson_data = spark.read.json(\"../Data/sales.json\")\njson_data.show()\n\n+-----+----------+------------+-------+-------------------+-----+\n|price|product_id|product_name|sale_id|          timestamp|units|\n+-----+----------+------------+-------+-------------------+-----+\n| 3.12|       134| Milk 1L Mua| 328711|2022-02-01T22:10:02|    1|\n| 1.22|       110|  Coke 350ml| 328712|2022-02-03T11:42:09|    3|\n| 4.65|       117|    Pepsi 2L| 328713|2022-02-03T14:22:15|    1|\n| 1.22|       110|  Coke 350ml| 328714|2022-02-03T18:33:08|    1|\n| 0.85|       341|Trident Mint| 328715|2022-02-04T15:41:36|    1|\n+-----+----------+------------+-------+-------------------+-----+",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#an-example-with-a-csv-file",
    "href": "Chapters/07-import.html#an-example-with-a-csv-file",
    "title": "6  Importing data to Spark",
    "section": "6.2 An example with a CSV file",
    "text": "6.2 An example with a CSV file\nAs an example, I have the following CSV file saved in my computer:\nname,age,job\nJorge,30,Developer\nBob,32,Developer\nThis CSV was saved in a file called people.csv, inside a folder called Data. So, to read this static file, Spark needs to know the path to this people.csv file. In other words, Spark needs to know where this file is stored in my computer, to be able to read it.\nIn my specific case, considering where this Data folder is in my computer, a relative path to it would be \"../Data/\". Having the path to the folder where people.csv is stored, I just need to add this file to the path, resulting in \"../Data/people.csv\". See in the example below, that I gave this path to the read.csv() method of my Spark Session. As a result, Spark will visit this address, and, read the file that is stored there:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\npath = \"../Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIn the above example, I gave a relative path to the file I wanted to read. But you can provide an absolute path1 to the file, if you want to. The people.csv is located at a very specific folder in my Linux computer, so, the absolute path to this file is pretty long as you can see below. But, if I were in my Windows machine, this absolute path would be something like \"C:\\Users\\pedro\\Documents\\Projects\\...\".\n\n# The absolute path to `people.csv`:\npath = \"/home/pedro/Documents/Projects/Books/Introd-pyspark/Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIf you give an invalid path (that is, a path that does not exist in your computer), you will get a AnalysisException. In the example below, I try to read a file called \"weird-file.csv\" that (in theory) is located at my current working directory. But when Spark looks inside my current directory, it does not find any file called \"weird-file.csv\". As a result, Spark raises a AnalysisException that warns me about this mistake.\n\ndf = spark.read.csv(\"weird-file.csv\")\n\nTraceback (most recent call last):\npyspark.sql.utils.AnalysisException:\nPath does not exist:\nfile:/home/pedro/Documents/Projects/Books/Introd-pyspark/weird-file.csv\nEvery time you face this “Path does not exist” error, it means that Spark did not found the file that you described in the path you gave to spark.read. In this case, is very likely that you have a typo or a mistake in your path. Maybe your forgot to add the .csv extension to the name of your file. Or maybe you forgot to use the right angled slash (/) instead of the left angled slash (\\). Or maybe, you gave the path to folder \\(x\\), when in fact, you wanted to reach the folder \\(y\\).\nSometimes, is useful to list all the files that are stored inside the folder you are trying to access. This way, you can make sure that you are looking at the right folder of your file system. To do that, you can use the listdir() function from os module of python. As an example, I can list all the files that are stored inside of the Data folder in this way:\n\nfrom os import listdir\nlistdir(\"../Data/\")\n\n['accounts.csv',\n 'books.txt',\n 'livros.txt',\n 'logs.json',\n 'penguins.csv',\n 'people.csv',\n 'sales.json',\n 'transf.csv',\n 'transf_reform.csv',\n 'user-events.json']",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#import-options",
    "href": "Chapters/07-import.html#import-options",
    "title": "6  Importing data to Spark",
    "section": "6.3 Import options",
    "text": "6.3 Import options\nWhile reading and importing data from any type of data source, Spark will always use the default values for each import option defined by the read engine you are using, unless you explicit ask it to use a different value. Each read engine has its own read/import options.\nFor example, the spark.read.orc() engine has a option called mergeSchema. With this option, you can ask Spark to merge the schemas collected from all the ORC part-files. In contrast, the spark.read.csv() engine does not have such option. Because this functionality of “merging schemas” does not make sense with CSV files.\nThis means that, some import options are specific (or characteristic) of some file formats. For example, the sep option (where you define the separator character) is used only in the spark.read.csv() engine. Because you do not have a special character that behaves as the “separator” in the other file formats (like ORC, JSON, Parquet…). So it does not make sense to have such option in the other read engines.\nIn the other hand, some import options can co-exist in multiple read engines. For example, the spark.read.json() and spark.read.csv() have both an encoding option. The encoding is a very important information, and Spark needs it to correctly interpret your file. By default, Spark will always assume that your files use the UTF-8 encoding system. Although, this may not be true for your specific case, and for these cases you use this encoding option to tell Spark which one to use.\nIn the next sections, I will break down some of the most used import options for each file format. If you want to see the complete list of import options, you can visit the Data Source Option section in the specific part of the file format you are using in the Spark SQL Guide2.\nTo define, or, set a specific import option, you use the option() method from a DataFrameReader object. To produce this kind of object, you use the spark.read module, like in the example below. Each call to the option() method is used to set a single import option.\nNotice that the “read engine” of Spark (i.e. csv()) is the last method called at this chain (or sequence) of steps. In other words, you start by creating a DataFrameReader object, then, set the import options, and lastly, you define which “read engine” you want to use.\n\n# Creating a `DataFrameReader` object:\ndf_reader = spark.read\n# Setting the import options:\ndf_reader = df_reader\\\n  .option(\"sep\", \"$\")\\\n  .option(\"locale\", \"pt-BR\")\n  \n# Setting the \"read engine\" to be used with `.csv()`:\nmy_data = df_reader\\\n  .csv(\"../Data/a-csv-file.csv\")\n\nIf you prefer, you can also merge all these calls together like this:\n\nspark.read\\ # a `DataFrameReader` object\n  .option(\"sep\", \"$\")\\ # Setting the `sep` option\n  .option(\"locale\", \"pt-BR\")\\ # Setting the `locale` option\n  .csv(\"../Data/a-csv-file.csv\") # The \"read engine\" to be used\n\nThere are many different import options for each read engine, and you can see the full list in the official documentation for Spark3. But lets just give you a brief overview of the probably most popular import options:\n\nsep: sets the separator character for each field and value in the CSV file (defaults to \",\");\nencoding: sets the character encoding of the file to be read (defaults to \"UTF-8\");\nheader: boolean (defaults to False), should Spark consider the first line of the file as the header of the DataFrame (i.e. the name of the columns) ?\ndateFormat and timestampFormat: sets the format for dates and timestamps in the file (defaults to \"yyyy-MM-dd\" and \"yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\" respectively);",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-separator-character-for-csv-files",
    "href": "Chapters/07-import.html#setting-the-separator-character-for-csv-files",
    "title": "6  Importing data to Spark",
    "section": "6.4 Setting the separator character for CSV files",
    "text": "6.4 Setting the separator character for CSV files\nIn this section, we will use the transf_reform.csv file to demonstrate how to set the separator character of a CSV file. This file, contains some data of transfers made in a fictitious bank. Is worth mentioning that this sep import option is only available for CSV files.\nLets use the peek_file() function defined below to get a quick peek at the first 5 lines of this file. If you look closely to these lines, you can identify that this CSV files uses the \";\" character to separate fields and values, and not the american standard \",\" character.\n\ndef peek_file(path, n_lines = 5):\n  with open(path) as file:\n    lines = [next(file) for i in range(n_lines)]\n  text = ''.join(lines)\n  print(text)\n  \npeek_file(\"../Data/transf_reform.csv\")\n\ndatetime;user;value;transferid;country;description\n2018-12-06T22:19:19Z;Eduardo;598.5984;116241629;Germany;\n2018-12-06T22:10:34Z;Júlio;4610.955;115586504;Germany;\n2018-12-06T21:59:50Z;Nathália;4417.866;115079280;Germany;\n2018-12-06T21:54:13Z;Júlio;2739.618;114972398;Germany;\n\n\n\nThis is usually the standard adopted by countries that uses a comma to define decimal places in real numbers. In other words, in some countries, the number 3.45 is usually written as 3,45.\nAnyway, we know now that the transf_reform.csv file uses a different separator character, so, to correctly read this CSV file into Spark, we need to set the sep import option. Since this file comes with the column names in the first line, I also set the header import option to read this first line as the column names as well.\n\ntransf = spark.read\\\n  .option(\"sep\", \";\")\\\n  .option(\"header\", True)\\\n  .csv(\"../Data/transf_reform.csv\")\n  \ntransf.show(5)\n\n+--------------------+--------+--------+----------+-------+-----------+\n|            datetime|    user|   value|transferid|country|description|\n+--------------------+--------+--------+----------+-------+-----------+\n|2018-12-06T22:19:19Z| Eduardo|598.5984| 116241629|Germany|       NULL|\n|2018-12-06T22:10:34Z|   Júlio|4610.955| 115586504|Germany|       NULL|\n|2018-12-06T21:59:50Z|Nathália|4417.866| 115079280|Germany|       NULL|\n|2018-12-06T21:54:13Z|   Júlio|2739.618| 114972398|Germany|       NULL|\n|2018-12-06T21:41:27Z|     Ana|1408.261| 116262934|Germany|       NULL|\n+--------------------+--------+--------+----------+-------+-----------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-encoding-of-the-file",
    "href": "Chapters/07-import.html#setting-the-encoding-of-the-file",
    "title": "6  Importing data to Spark",
    "section": "6.5 Setting the encoding of the file",
    "text": "6.5 Setting the encoding of the file\nSpark will always assume that your static files use the UTF-8 encoding system. But, that might not be the case for your specific file. In this situation, you have to tell Spark which is the appropriate encoding system to be used while reading the file. This encoding import option is available both for CSV and JSON files.\nTo do this, you can set the encoding import option, with the name of the encoding system to be used. As an example, lets look at the file books.txt, which is a CSV file encoded with the ISO-8859-1 system (i.e. the Latin 1 system).\nIf we use the defaults in Spark, you can see in the result below that some characters in the Title column are not correctly interpreted. Remember, this problem occurs because of a mismatch in encoding systems. Spark thinks books.txt is using the UTF-8 system, but, in reality, it uses the ISO-8859-1 system:\n\nbooks = spark.read\\\n  .option(\"header\", True)\\\n  .csv(\"../Data/books.txt\")\n  \nbooks.show()\n\n+--------------------+--------------------+------+\n|               Title|              Author| Price|\n+--------------------+--------------------+------+\n|            O Hobbit|    J. R. R. Tolkien| 40.72|\n|Matem�tica para E...|Carl P. Simon and...|139.74|\n|Microeconomia: um...|       Hal R. Varian| 141.2|\n|      A Luneta �mbar|      Philip Pullman| 42.89|\n+--------------------+--------------------+------+\n\n\n\nBut if we tell Spark to use the ISO-8859-1 system while reading the file, then, all problems are solved, and all characters in the file are correctly interpreted, as you see in the result below:\n\nbooks = spark.read\\\n  .option(\"header\", True)\\\n  .option(\"encoding\", \"ISO-8859-1\")\\\n  .csv(\"../Data/books.txt\")\n  \nbooks.show()\n\n+--------------------+--------------------+------+\n|               Title|              Author| Price|\n+--------------------+--------------------+------+\n|            O Hobbit|    J. R. R. Tolkien| 40.72|\n|Matemática para E...|Carl P. Simon and...|139.74|\n|Microeconomia: um...|       Hal R. Varian| 141.2|\n|      A Luneta Âmbar|      Philip Pullman| 42.89|\n+--------------------+--------------------+------+",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-format-of-dates-and-timestamps",
    "href": "Chapters/07-import.html#setting-the-format-of-dates-and-timestamps",
    "title": "6  Importing data to Spark",
    "section": "6.6 Setting the format of dates and timestamps",
    "text": "6.6 Setting the format of dates and timestamps\nThe format that humans write dates and timestamps vary drastically over the world. By default, Spark will assume that the dates and timestamps stored in your file are in the format described by the ISO-8601 standard. That is, the “YYYY-mm-dd”, or, “year-month-day” format.\nBut this standard might not be the case for your file. For example: the brazilian people usually write dates in the format “dd/mm/YYYY”, or, “day/month/year”; some parts of Spain write dates in the format “YYYY/dd/mm”, or, “year/day/month”; on Nordic countries (i.e. Sweden, Finland) dates are written in “YYYY.mm.dd” format.\nEvery format of a date or timestamp is defined by using a string with the codes of each part of the date/timestamp, like the letter ‘Y’ which represents a 4-digit year, or the letter ‘d’ which represents a 2-digit day. You can see the complete list of codes and their description in the official documentation of Spark4.\nAs an example, lets look into the user-events.json file. We can see that the dates and timestamps in this file are using the “dd/mm/YYYY” and “dd/mm/YYYY HH:mm:ss” formats respectively.\n\npeek_file(\"../Data/user-events.json\", n_lines=3)\n\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 14:33:10\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"entry\"}\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 14:40:08\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"click: shop\"}\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 15:48:41\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"select: payment-method\"}\n\n\nDate variables are usually interpreted by Spark as string variables. In other words, Spark usually do not convert data that contains dates to the date type of Spark. In order to Spark\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DateType, StringType, TimestampType\n\nschema = StructType([\n  StructField('dateOfEvent', DateType(), True),\n  StructField('timeOfEvent', TimestampType(), True),\n  StructField('userId', StringType(), True),\n  StructField('nameOfEvent', StringType(), True)\n])\n\nuser_events = spark.read\\\n  .option(\"dateFormat\", \"d/M/y\")\\\n  .option(\"timestampFormat\", \"d/M/y k:m:s\")\\\n  .json(\"../Data/user-events.json\", schema = schema)\n  \nuser_events.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-import.html#footnotes",
    "href": "Chapters/07-import.html#footnotes",
    "title": "6  Importing data to Spark",
    "section": "",
    "text": "That is, the complete path to the file, or, in other words, a path that starts in the root folder of your hard drive.↩︎\nFor example, this Data Source Option for Parquet files is located at: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option↩︎\nhttps://spark.apache.org/docs/latest/sql-data-sources-csv.html↩︎\nhttps://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing data to Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#the-sql-method-as-the-main-entrypoint",
    "href": "Chapters/06-dataframes-sql.html#the-sql-method-as-the-main-entrypoint",
    "title": "7  Working with SQL in pyspark",
    "section": "7.1 The sql() method as the main entrypoint",
    "text": "7.1 The sql() method as the main entrypoint\nThe main entrypoint, that is, the main bridge that connects Spark SQL to Python is the sql() method of your Spark Session. This method accepts a SQL query inside a string as input, and will always output a new Spark DataFrame as result. That is why I used the show() method right after sql(), in the example below, to see what this new Spark DataFrame looked like.\nAs a first example, lets look at a very basic SQL query, that just select a list of code values:\nSELECT *\nFROM (\n  VALUES (11), (31), (24), (35)\n) AS List(Codes)\nTo run the above SQL query, and see its results, I must write this query into a string, and give this string to the sql() method of my Spark Session. Then, I use the show() action to see the actual result rows of data generated by this query:\n\nsql_query = '''\nSELECT *\nFROM (\n  VALUES (11), (31), (24), (35)\n) AS List(Codes)\n'''\n\nspark.sql(sql_query).show()\n\n+-----+\n|Codes|\n+-----+\n|   11|\n|   31|\n|   24|\n|   35|\n+-----+\n\n\n\nIf you want to execute a very short SQL query, is fine to write it inside a single pair of quotation marks (for example \"SELECT * FROM sales.per_day\"). However, since SQL queries usually take multiple lines, you can write your SQL query inside a python docstring (created by a pair of three quotation marks), like in the example above.\nHaving this in mind, every time you want to execute a SQL query, you can use this sql() method from the object that holds your Spark Session. So the sql() method is the bridge between pyspark and SQL. You give it a pure SQL query inside a string, and, Spark will execute it, considering your Spark SQL context.\n\n7.1.1 A single SQL statement per run\nIs worth pointing out that, although being the main bridge between Python and SQL, the Spark Session sql() method can execute only a single SQL statement per run. This means that if you try to execute two sequential SQL statements at the same time with sql(), then, Spark SQL will automatically raise a ParseException error, which usually complains about an “extra input”.\nIn the example below, we are doing two very basic steps to SQL. We first create a dummy database with a CREATE DATABASE statement, then, we ask SQL to use this new database that we created as the default database of the current session, with a USE statement.\nCREATE DATABASE `dummy`;\nUSE `dummy`;\nIf we try to execute these two steps at once, by using the sql() method, Spark complains with a ParseException, indicating that we have a sytax error in our query, like in the example below:\nquery = '''\nCREATE DATABASE `dummy`;\nUSE `dummy`;\n'''\n\nspark.sql(query).show()\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/spark/python/pyspark/sql/session.py\", line 1034, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 196, in deco\n    raise converted from None\npyspark.sql.utils.ParseException: \nSyntax error at or near 'USE': extra input 'USE'(line 3, pos 0)\n\n== SQL ==\n\nCREATE DATABASE `dummy`;\nUSE `dummy`;\n^^^\nHowever, there is nothing wrong about the above SQL statements. They are both correct and valid SQL statements, both semantically and syntactically. In other words, the case above results in a ParseException error solely because it contains two different SQL statements.\nIn essence, the spark.sql() method always expect a single SQL statement as input, and, therefore, it will try to parse this input query as a single SQL statement. If it finds multiple SQL statements inside this input string, the method will automatically raise the above error.\nNow, be aware that some SQL queries can take multiple lines, but, still be considered a single SQL statement. A query started by a WITH clause is usually a good example of a SQL query that can group multiple SELECT statements, but still be considered a single SQL statement as a whole:\n-- The query below would execute\n-- perfectly fine inside spark.sql():\nWITH table1 AS (\n  SELECT *\n  FROM somewhere\n),\n\nfiltering AS (\n  SELECT *\n  FROM table1\n  WHERE dateOfTransaction == CAST(\"2022-02-02\" AS DATE)\n)\n\nSELECT *\nFROM filtering\nAnother example of a usually big and complex query, that can take multiple lines but still be considered a single SQL statement, is a single SELECT statement that selects multiple subqueries that are nested together, like this:\n-- The query below would also execute\n-- perfectly fine inside spark.sql():\nSELECT *\nFROM (\n  -- First subquery.\n  SELECT *\n  FROM (\n    -- Second subquery..\n    SELECT *\n    FROM (\n      -- Third subquery...\n      SELECT *\n      FROM (\n        -- Ok this is enough....\n      )\n    )\n  )\n)\nHowever, if we had multiple separate SELECT statements that were independent on each other, like in the example below, then, spark.sql() would issue an ParseException error if we tried to execute these three SELECT statements inside the same input string.\n-- These three statements CANNOT be executed\n-- at the same time inside spark.sql()\nSELECT * FROM something;\nSELECT * FROM somewhere;\nSELECT * FROM sometime;\nAs a conclusion, if you want to easily execute multiple statements, you can use a for loop which calls spark.sql() for each single SQL statement:\n\nstatements = '''SELECT * FROM something;\nSELECT * FROM somewhere;\nSELECT * FROM sometime;'''\n\nstatements = statements.split('\\n')\nfor statement in statements:\n  spark.sql(statement)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "href": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "title": "7  Working with SQL in pyspark",
    "section": "7.2 Creating SQL Tables in Spark",
    "text": "7.2 Creating SQL Tables in Spark\nIn real life jobs at the industry, is very likely that your data will be allocated inside a SQL-like database. Spark can connect to a external SQL database through JDBC/ODBC connections, or, read tables from Apache Hive. This way, you can sent your SQL queries to this external database.\nHowever, to expose more simplified examples throughout this chapter, we will use pyspark to create a simple temporary SQL table in our Spark SQL context, and use this temporary SQL table in our examples of SQL queries. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in pyspark.\nFirst, lets create our Spark Session. You can see below that I used the config() method to set a specific option of the session called spark.sql.catalogImplementation, to the value \"hive\". This option controls the implementation of the Spark SQL Catalog, which is a core part of the SQL functionality of Spark 1.\nSpark usually complain with a AnalysisException error when you try to create SQL tables with this option undefined (or not configured). So, if you decide to follow the examples of this chapter, please always set this option right at the start of your script2.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession\\\n  .builder\\\n  .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n  .getOrCreate()\n\n\n7.2.1 TABLEs versus VIEWs\nTo run a complete SQL query over any Spark DataFrame, you must register this DataFrame in the Spark SQL Catalog of your Spark Session. You can register a Spark DataFrame into this catalog as a physical SQL TABLE, or, as a SQL VIEW.\nIf you are familiar with the SQL language and Relational DataBase Management Systems - RDBMS (such as MySQL), you probably already heard of these two types (TABLE or VIEW) of SQL objects. But if not, we will explain each one in this section. Is worth pointing out that choosing between these two types does not affect your code, or your transformations in any way. It just affect the way that Spark SQL stores the table/DataFrame itself.\n\n7.2.1.1 VIEWs are stored as SQL queries or memory pointers\nWhen you register a DataFrame as a SQL VIEW, the query to produce this DataFrame is stored, not the DataFrame itself. There are also cases where Spark store a memory pointer instead, that points to the memory adress where this DataFrame is stored in memory. In this perspective, Spark SQL use this pointer every time it needs to access this DataFrame.\nTherefore, when you call (or access) this SQL VIEW inside your SQL queries (for example, with a SELECT * FROM statement), Spark SQL will automatically get this SQL VIEW “on the fly” (or “on runtime”), by executing the query necessary to build the initial DataFrame that you stored inside this VIEW, or, if this DataFrame is already stored in memory, Spark will look at the specific memory address it is stored.\nIn other words, when you create a SQL VIEW, Spark SQL do not store any physical data or rows of the DataFrame. It just stores the SQL query necessary to build your DataFrame. In some way, you can interpret any SQL VIEW as an abbreviation to a SQL query, or a “nickname” to an already existing DataFrame.\nAs a consequence, for most “use case scenarios”, SQL VIEWs are easier to manage inside your data pipelines. Because you usually do not have to update them. Since they are calculated from scratch, at the moment you request for them, a SQL VIEW will always translate the most recent version of the data.\nThis means that the concept of a VIEW in Spark SQL is very similar to the concept of a VIEW in other types of SQL databases, such as the MySQL database. If you read the official documentation for the CREATE VIEW statement at MySQL3 you will get a similar idea of a VIEW:\n\nThe select_statement is a SELECT statement that provides the definition of the view. (Selecting from the view selects, in effect, using the SELECT statement.) …\n\nThe above statement, tells us that selecing a VIEW causes the SQL engine to execute the expression defined at select_statement using the SELECT statement. In other words, in MySQL, a SQL VIEW is basically an alias to an existing SELECT statement.\n\n\n7.2.1.2 Differences in Spark SQL VIEWs\nAlthough a Spark SQL VIEW being very similar to other types of SQL VIEW (such as the MySQL type), on Spark applications, SQL VIEWs are usually registered as TEMPORARY VIEWs4 instead of standard (and “persistent”) SQL VIEW as in MySQL.\nAt MySQL there is no notion of a “temporary view”, although other popular kinds of SQL databases do have it, such as the PostGreSQL database5. So, a temporary view is not a exclusive concept of Spark SQL. However, is a special type of SQL VIEW that is not present in all popular kinds of SQL databases.\nIn other words, both Spark SQL and MySQL support the CREATE VIEW statement. In contrast, statements such as CREATE TEMPORARY VIEW and CREATE OR REPLACE TEMPORARY VIEW are available in Spark SQL, but not in MySQL.\n\n\n7.2.1.3 Registering a Spark SQL VIEW in the Spark SQL Catalog\nIn pyspark, you can register a Spark DataFrame as a temporary SQL VIEW with the createTempView() or createOrReplaceTempView() DataFrame methods. These methods are equivalent to CREATE TEMPORARY VIEW and CREATE OR REPLACE TEMPORARY VIEW SQL statements of Spark SQL, respectively.\nIn essence, these methods register your Spark DataFrame as a temporary SQL VIEW, and have a single input, which is the name you want to give to this new SQL VIEW you are creating inside a string:\n\n# To save the `df` DataFrame as a SQL VIEW, use one of the methods below:\ndf.createTempView('example_view')\ndf.createOrReplaceTempView('example_view')\n\nAfter we executed the above statements, we can now access and use the df DataFrame in any SQL query, like in the example below:\n\nsql_query = '''\nSELECT *\nFROM example_view\nWHERE value &gt; 20\n'''\n\nspark.sql(sql_query).show()\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+---+-----+----------+\n| id|value|      date|\n+---+-----+----------+\n|  1| 28.3|2021-01-01|\n|  3| 20.1|2021-01-02|\n+---+-----+----------+\n\n\n\nSo you use the createTempView() or createOrReplaceTempView() methods when you want to make a Spark DataFrame created in pyspark (that is, a python object), available to Spark SQL.\nBesides that, you also have the option to create a temporary VIEW by using pure SQL statements trough the sql() method. However, when you create a temporary VIEW using pure SQL, you can only use (inside this VIEW) native SQL objects that are already stored inside your Spark SQL Context.\nThis means that you cannot make a Spark DataFrame created in python available to Spark SQL, by using a pure SQL inside the sql() method. To do this, you have to use the DataFrame methods createTempView() and createOrReplaceTempView().\nAs an example, the query below uses pure SQL statements to creates the active_brazilian_users temporary VIEW, which selects an existing SQL table called hubspot.user_mails:\nCREATE TEMPORARY VIEW active_brazilian_users AS\nSELECT *\nFROM hubspot.user_mails\nWHERE state == 'Active'\nAND country_location == 'Brazil'\nTemporary VIEWs like the one above (which are created from pure SQL statements being executed inside the sql() method) are kind of unusual in Spark SQL. Because you can easily avoid the work of creating a VIEW by using Common Table Expression (CTE) on a WITH statement, like in the query below:\nWITH active_brazilian_users AS (\n  SELECT *\n  FROM hubspot.user_mails\n  WHERE state == 'Active'\n  AND country_location == 'Brazil'\n)\n\nSELECT A.user, SUM(sale_value), B.user_email\nFROM sales.sales_per_user AS A\nINNER JOIN active_brazilian_users AS B\nGROUP BY A.user, B.user_email\nJust as a another example, you can also run a SQL query that creates a persistent SQL VIEW (that is, without the TEMPORARY clause). In the example below, I am saving the simple query that I showed at the beginning of this chapter inside a VIEW called list_of_codes. This CREATE VIEW statement, register a persistent SQL VIEW in the SQL Catalog.\n\nsql_query = '''\nCREATE OR REPLACE VIEW list_of_codes AS\nSELECT *\nFROM (\n  VALUES (11), (31), (24), (35)\n) AS List(Codes)\n'''\n\nspark.sql(sql_query)\n\nDataFrame[]\n\n\nNow, every time I want to use this SQL query that selects a list of codes, I can use this list_of_codes as a shortcut:\n\nspark.sql(\"SELECT * FROM list_of_codes\").show()\n\n+-----+\n|Codes|\n+-----+\n|   11|\n|   31|\n|   24|\n|   35|\n+-----+\n\n\n\n\n\n7.2.1.4 TABLEs are stored as physical tables\nIn the other hand, SQL TABLEs are the “opposite” of SQL VIEWs. That is, SQL TABLEs are stored as physical tables inside the SQL database. In other words, each one of the rows of your table are stored inside the SQL database.\nBecause of this characteristic, when dealing with huges amounts of data, SQL TABLEs are usually faster to load and transform. Because you just have to read the data stored on the database, you do not need to calculate it from scratch every time you use it.\nBut, as a collateral effect, you usually have to physically update the data inside this TABLE, by using, for example, INSERT INTO statements. In other words, when dealing with SQL TABLE’s you usually need to create (and manage) data pipelines that are responsible for periodically update and append new data to this SQL TABLE, and this might be a big burden to your process.\n\n\n7.2.1.5 Registering Spark SQL TABLEs in the Spark SQL Catalog\nIn pyspark, you can register a Spark DataFrame as a SQL TABLE with the write.saveAsTable() DataFrame method. This method accepts, as first input, the name you want to give to this SQL TABLE inside a string.\n\n# To save the `df` DataFrame as a SQL TABLE:\ndf.write.saveAsTable('example_table')\n\nAs you expect, after we registered the DataFrame as a SQL table, we can now run any SQL query over example_table, like in the example below:\n\nspark.sql(\"SELECT SUM(value) FROM example_table\").show()\n\n+----------+\n|sum(value)|\n+----------+\n|      76.8|\n+----------+\nYou can also use pure SQL queries to create an empty SQL TABLE from scratch, and then, feed this table with data by using INSERT INTO statements. In the example below, we create a new database called examples, and, inside of it, a table called code_brazil_states. Then, we use multiple INSERT INTO statements to populate this table with few rows of data.\n\nall_statements = '''CREATE DATABASE `examples`;\nUSE `examples`;\nCREATE TABLE `code_brazil_states` (`code` INT, `state_name` STRING);\nINSERT INTO `code_brazil_states` VALUES (31, \"Minas Gerais\");\nINSERT INTO `code_brazil_states` VALUES (15, \"Pará\");\nINSERT INTO `code_brazil_states` VALUES (41, \"Paraná\");\nINSERT INTO `code_brazil_states` VALUES (25, \"Paraíba\");'''\n\nstatements = all_statements.split('\\n')\nfor statement in statements:\n  spark.sql(statement)\n\nWe can see now this new physical SQL table using a simple query like this:\n\nspark\\\n  .sql('SELECT * FROM examples.code_brazil_states')\\\n  .show()\n\n+----+------------+\n|code|  state_name|\n+----+------------+\n|  41|      Paraná|\n|  31|Minas Gerais|\n|  15|        Pará|\n|  25|     Paraíba|\n+----+------------+\n\n\n7.2.1.6 The different save “modes”\nThere are other arguments that you might want to use in the write.saveAsTable() method, like the mode argument. This argument controls how Spark will save your data into the database. By default, write.saveAsTable() uses the mode = \"error\" by default. In this mode, Spark will look if the table you referenced already exists, before it saves your data.\nLet’s get back to the code we showed before (which is reproduced below). In this code, we asked Spark to save our data into a table called \"example_table\". Spark will look if a table with this name already exists. If it does, then, Spark will raise an error that will stop the process (i.e. no data is saved).\n\ndf.write.saveAsTable('example_table')\n\nRaising an error when you do not want to accidentaly affect a SQL table that already exist, is a good practice. But, you might want to not raise an error in this situation. In case like this, you might want to just ignore the operation, and get on with your life. For cases like this, write.saveAsTable() offers the mode = \"ignore\".\nSo, in the code example below, we are trying to save the df DataFrame into a table called example_table. But if this example_table already exist, Spark will just silently ignore this operation, and will not save any data.\n\ndf.write.saveAsTable('example_table', mode = \"ignore\")\n\nIn addition, write.saveAsTable() offers two more different modes, which are mode = \"overwrite\" and mode = \"append\". When you use one these two modes, Spark will always save your data, no matter if the SQL table you are trying to save into already exist or not. In essence, these two modes control whether Spark will delete or keep previous rows of the SQL table intact, before it saves any new data.\nWhen you use mode = \"overwrite\", Spark will automatically rewrite/replace the entire table with the current data of your DataFrame. In contrast, when you use mode = \"append\", Spark will just append (or insert, or add) this data into the table. The subfigures at Figure 7.1 demonstrates these two modes visually.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mode overwrite\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Mode append\n\n\n\n\n\n\n\nFigure 7.1: How Spark saves your data with different “save modes”\n\n\n\nYou can see the full list of arguments of write.SaveAsTable(), and their description by looking at the documentation6.\n\n\n\n7.2.2 Temporary versus Persistent sources\nWhen you register any Spark DataFrame as a SQL TABLE, it becomes a persistent source. Because the contents, the data, the rows of the table are stored on disk, inside a database, and can be accessed any time, even after you close or restart your computer (or your Spark Session). In other words, it becomes “persistent” as in the sense of “it does not die”.\nAs another example, when you save a specific SQL query as a SQL VIEW with the CREATE VIEW statement, this SQL VIEW is saved inside the database. As a consequence, it becomes a persistent source as well, and can be accessed and reused in other Spark Sessions, unless you explicit drop (or “remove”) this SQL VIEW with a DROP VIEW statement.\nHowever, with methods like createTempView() and createOrReplaceTempView() you register your Spark DataFrame as a temporary SQL VIEW. This means that the life (or time of existence) of this VIEW is tied to your Spark Session. In other words, it will exist in your Spark SQL Catalog only for the duration of your Spark Session. When you close your Spark Session, this VIEW just dies. When you start a new Spark Session it does not exist anymore. As a result, you have to register your DataFrame again at the catalog to use it one more time.\n\n\n7.2.3 Spark SQL Catalog is the bridge between SQL and pyspark\nRemember, to run SQL queries over any Spark DataFrame, you must register this DataFrame into the Spark SQL Catalog. Because of it, this Spark SQL Catalog works almost as the bridge that connects the python objects that hold your Spark DataFrames to the Spark SQL context. Without it, Spark SQL will not find your Spark DataFrames. As a result, it can not run any SQL query over it.\nWhen you try to use a DataFrame that is not currently registered at the Spark SQL Catalog, Spark will automatically raise a AnalysisException, like in the example below:\nspark\\\n  .sql(\"SELECT * FROM this.does_not_exist\")\\\n  .show()\nAnalysisException: Table or view not found\nThe methods saveAsTable(), createTempView() and createOrReplaceTempView() are the main methods to register your Spark DataFrame into this Spark SQL Catalog. This means that you have to use one of these methods before you run any SQL query over your Spark DataFrame.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#the-penguins-dataframe",
    "href": "Chapters/06-dataframes-sql.html#the-penguins-dataframe",
    "title": "7  Working with SQL in pyspark",
    "section": "7.3 The penguins DataFrame",
    "text": "7.3 The penguins DataFrame\nOver the next examples in this chapter, we will explore the penguins DataFrame. This is the penguins dataset from the palmerpenguins R library. It stores data of multiple measurements of penguin species from the islands in Palmer Archipelago.\nThese measurements include size (flipper length, body mass, bill dimensions) and sex, and they were collected by researchers of the Antarctica LTER program, a member of the Long Term Ecological Research Network. If you want to understand more about each field/column present in this dataset, I recommend you to read the official documentation of this dataset7.\nTo get this data, you can download the CSV file called penguins.csv (remember that this CSV can be downloaded from the book repository8). In the code below, I am reading this CSV file and creating a Spark DataFrame with its data. Then, I register this Spark DataFrame as a SQL temporary view (called penguins_view) using the createOrReplaceTempView() method.\n\npath = \"../Data/penguins.csv\"\npenguins = spark.read\\\n  .csv(path, header = True)\n  \npenguins.createOrReplaceTempView('penguins_view')\n\nAfter these commands, I have now a SQL view called penguins_view registered in my Spark SQL context, which I can query it, using pure SQL:\n\nspark.sql('SELECT * FROM penguins_view').show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|          NULL|         NULL|             NULL|       NULL|  NULL|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#selecting-your-spark-dataframes",
    "href": "Chapters/06-dataframes-sql.html#selecting-your-spark-dataframes",
    "title": "7  Working with SQL in pyspark",
    "section": "7.4 Selecting your Spark DataFrames",
    "text": "7.4 Selecting your Spark DataFrames\nAn obvious way to access any SQL TABLE or VIEW registered in your Spark SQL context, is to select it, through a simple SELECT * FROM statement, like we saw in the previous examples. However, it can be quite annoying to type “SELECT * FROM” every time you want to use a SQL TABLE or VIEW in Spark SQL.\nThat is why Spark offers a shortcut to us, which is the table() method of your Spark session. In other words, the code spark.table(\"table_name\") is a shortcut to spark.sql(\"SELECT * FROM table_name\"). They both mean the same thing. For example, we could access penguins_view as:\n\nspark\\\n  .table('penguins_view')\\\n  .show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|          NULL|         NULL|             NULL|       NULL|  NULL|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#executing-sql-expressions",
    "href": "Chapters/06-dataframes-sql.html#executing-sql-expressions",
    "title": "7  Working with SQL in pyspark",
    "section": "7.5 Executing SQL expressions",
    "text": "7.5 Executing SQL expressions\nAs I noted at Section 4.2, columns of a Spark DataFrame (or objects of class Column) are closely related to expressions. As a result, you usually use and execute expressions in Spark when you want to transform (or mutate) columns of a Spark DataFrame.\nThis is no different for SQL expressions. A SQL expression is basically any expression you would use on the SELECT statement of your SQL query. As you can probably guess, since they are used in the SELECT statement, these expressions are used to transform columns of a Spark DataFrame.\nThere are many column transformations that are particularly verbose and expensive to write in “pure” pyspark. But you can use a SQL expression in your favor, to translate this transformation into a more short and concise form. Virtually any expression you write in pyspark can be translated into a SQL expression.\nTo execute a SQL expression, you give this expression inside a string to the expr() function from the pyspark.sql.functions module. Since expressions are used to transform columns, you normally use the expr() function inside a withColumn() or a select() DataFrame method, like in the example below:\n\nfrom pyspark.sql.functions import expr\n\nspark\\\n  .table('penguins_view')\\\n  .withColumn(\n    'specie_island',\n    expr(\"CONCAT(species, '_', island)\")\n  )\\\n  .withColumn(\n    'sex_short',\n    expr(\"CASE WHEN sex == 'male' THEN 'M' ELSE 'F' END\")\n  )\\\n  .select('specie_island', 'sex_short')\\\n  .show(5)\n\n+----------------+---------+\n|   specie_island|sex_short|\n+----------------+---------+\n|Adelie_Torgersen|        M|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n+----------------+---------+\nonly showing top 5 rows\n\n\n\nI particulaly like to write “if-else” or “case-when” statements using a pure CASE WHEN SQL statement inside the expr() function. By using this strategy you usually get a more simple statement that translates the intention of your code in a cleaner way. But if I wrote the exact same CASE WHEN statement above using pure pyspark functions, I would end up with a shorter (but “less clean”) statement using the when() and otherwise() functions:\n\nfrom pyspark.sql.functions import (\n  when, col,\n  concat, lit\n)\n\nspark\\\n  .table('penguins_view')\\\n  .withColumn(\n    'specie_island',\n    concat('species', lit('_'), 'island')\n  )\\\n  .withColumn(\n    'sex_short',\n    when(col(\"sex\") == 'male', 'M')\\\n      .otherwise('F')\n  )\\\n  .select('specie_island', 'sex_short')\\\n  .show(5)\n\n+----------------+---------+\n|   specie_island|sex_short|\n+----------------+---------+\n|Adelie_Torgersen|        M|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n|Adelie_Torgersen|        F|\n+----------------+---------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#every-dataframe-transformation-in-python-can-be-translated-into-sql",
    "href": "Chapters/06-dataframes-sql.html#every-dataframe-transformation-in-python-can-be-translated-into-sql",
    "title": "7  Working with SQL in pyspark",
    "section": "7.6 Every DataFrame transformation in Python can be translated into SQL",
    "text": "7.6 Every DataFrame transformation in Python can be translated into SQL\nAll DataFrame API transformations that you write in Python (using pyspark) can be translated into SQL queries/expressions using the Spark SQL module. Since the DataFrame API is a core part of pyspark, the majority of python code you write with pyspark can be translated into SQL queries (if you wanto to).\nIs worth pointing out, that, no matter which language you choose (Python or SQL), they are both further compiled to the same base instructions. The end result is that the Python code you write and his SQL translated version will perform the same (they have the same efficiency), because they are compiled to the same instructions before being executed by Spark.\n\n7.6.1 DataFrame methods are usually translated into SQL keywords\nWhen you translate the methods from the python DataFrame class (like orderBy(), select() and where()) into their equivalents in Spark SQL, you usually get SQL keywords (like ORDER BY, SELECT and WHERE).\nFor example, if I needed to get the top 5 penguins with the biggest body mass at penguins_view, that had sex equal to \"female\", and, ordered by bill length, I could run the following python code:\n\nfrom pyspark.sql.functions import col\ntop_5 = penguins\\\n    .where(col('sex') == 'female')\\\n    .orderBy(col('body_mass_g').desc())\\\n    .limit(5)\n\ntop_5\\\n    .orderBy('bill_length_mm')\\\n    .show()\n\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n| Gentoo|Biscoe|          44.9|         13.3|              213|       5100|female|2008|\n| Gentoo|Biscoe|          45.1|         14.5|              207|       5050|female|2007|\n| Gentoo|Biscoe|          45.2|         14.8|              212|       5200|female|2009|\n| Gentoo|Biscoe|          46.5|         14.8|              217|       5200|female|2008|\n| Gentoo|Biscoe|          49.1|         14.8|              220|       5150|female|2008|\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n\n\n\nI could translate the above python code to the following SQL query:\nWITH top_5 AS (\n    SELECT *\n    FROM penguins_view\n    WHERE sex == 'female'\n    ORDER BY body_mass_g DESC\n    LIMIT 5\n)\n\nSELECT *\nFROM top_5\nORDER BY bill_length_mm\nAgain, to execute the above SQL query inside pyspark we need to give this query as a string to the sql() method of our Spark Session, like this:\n\nquery = '''\nWITH top_5 AS (\n    SELECT *\n    FROM penguins_view\n    WHERE sex == 'female'\n    ORDER BY body_mass_g DESC\n    LIMIT 5\n)\n\nSELECT *\nFROM top_5\nORDER BY bill_length_mm\n'''\n\n# The same result of the example above\nspark.sql(query).show()\n\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n| Gentoo|Biscoe|          44.9|         13.3|              213|       5100|female|2008|\n| Gentoo|Biscoe|          45.1|         14.5|              207|       5050|female|2007|\n| Gentoo|Biscoe|          45.2|         14.8|              212|       5200|female|2009|\n| Gentoo|Biscoe|          46.5|         14.8|              217|       5200|female|2008|\n| Gentoo|Biscoe|          49.1|         14.8|              220|       5150|female|2008|\n+-------+------+--------------+-------------+-----------------+-----------+------+----+\n\n\n\n\n\n7.6.2 Spark functions are usually translated into SQL functions\nEvery function from the pyspark.sql.functions module you might use to describe your transformations in python, can be directly used in Spark SQL. In other words, every Spark function that is accesible in python, is also accesible in Spark SQL.\nWhen you translate these python functions into SQL, they usually become a pure SQL function with the same name. For example, if I wanted to use the regexp_extract() python function, from the pyspark.sql.functions module in Spark SQL, I just use the REGEXP_EXTRACT() SQL function. The same occurs to any other function, like the to_date() function for example.\n\nfrom pyspark.sql.functions import to_date, regexp_extract\n# `df1` and `df2` are both equal. Because they both\n# use the same `to_date()` and `regexp_extract()` functions\ndf1 = (spark\n  .table('penguins_view')\n  .withColumn(\n    'extract_number',\n    regexp_extract('bill_length_mm', '[0-9]+', 0)\n  )\n  .withColumn('date', to_date('year', 'y'))\n  .select(\n    'bill_length_mm', 'year',\n    'extract_number', 'date'\n  )\n)\n\ndf2 = (spark\n  .table('penguins_view')\n  .withColumn(\n    'extract_number',\n    expr(\"REGEXP_EXTRACT(bill_length_mm, '[0-9]+', 0)\")\n  )\n  .withColumn('date', expr(\"TO_DATE(year, 'y')\"))\n  .select(\n    'bill_length_mm', 'year',\n    'extract_number', 'date'\n  )\n)\n\ndf2.show(5)\n\n+--------------+----+--------------+----------+\n|bill_length_mm|year|extract_number|      date|\n+--------------+----+--------------+----------+\n|          39.1|2007|            39|2007-01-01|\n|          39.5|2007|            39|2007-01-01|\n|          40.3|2007|            40|2007-01-01|\n|          NULL|2007|          NULL|2007-01-01|\n|          36.7|2007|            36|2007-01-01|\n+--------------+----+--------------+----------+\nonly showing top 5 rows\n\n\n\nThis is very handy. Because for every new python function from the pyspark.sql.functions module, that you learn how to use, you automatically learn how to use in Spark SQL as well, because is the same function, with the basically the same name and arguments.\nAs an example, I could easily translate the above transformations that use the to_date() and regexp_extract() python functions, into the following SQL query (that I could easily execute trough the sql() Spark Session method):\nSELECT \n  bill_length_mm, year,\n  REGEXP_EXTRACT(bill_length_mm, '[0-9]+', 0) AS extract_number,\n  TO_DATE(year, 'y') AS date\nFROM penguins_view\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1. https://spark.apache.org/docs/latest/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#footnotes",
    "href": "Chapters/06-dataframes-sql.html#footnotes",
    "title": "7  Working with SQL in pyspark",
    "section": "",
    "text": "There are some very good materials explaining what is the Spark SQL Catalog, and which is the purpose of it. For a soft introduction, I recommend Sarfaraz Hussain post: https://medium.com/@sarfarazhussain211/metastore-in-apache-spark-9286097180a4. For a more technical introduction, see https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Catalog.html.↩︎\nYou can learn more about why this specific option is necessary by looking at this StackOverflow post: https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error.↩︎\nhttps://dev.mysql.com/doc/refman/8.0/en/create-view.html↩︎\nI will explain more about the meaning of “temporary” at Section 7.2.2.↩︎\nhttps://www.postgresql.org/docs/current/sql-createview.html↩︎\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable↩︎\nhttps://allisonhorst.github.io/palmerpenguins/reference/penguins.html↩︎\nhttps://github.com/pedropark99/Introd-pyspark/tree/main/Data↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Working with SQL in `pyspark`</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#sec-remove-duplicates",
    "href": "Chapters/08-transforming2.html#sec-remove-duplicates",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.1 Removing duplicated values from your DataFrame",
    "text": "8.1 Removing duplicated values from your DataFrame\nRemoving duplicated values from DataFrames is a very commom operation in ETL pipelines. In pyspark you have two options to remove duplicated values, which are:\n\ndistinct() which removes all duplicated values considering the combination of all current columns in the DataFrame;\ndrop_duplicates() or dropDuplicates() which removes all duplicated values considering a specific combination (or set) of columns in the DataFrame;\n\nThese three methods above are all DataFrames methods. Furthermore, the methods drop_duplicates() and dropDuplicates() are equivalent. They both mean the same thing, and have the same arguments and perform the same operation.\nWhen you run drop_duplicates() or dropDuplicates() without any argument, they automatically use by default the combination of all columns available in the DataFrame to identify the duplicated values. As a consequence, over this specific situation, the methods drop_duplicates() or dropDuplicates() become equivalent to the distinct() method. Because they use the combination of all columns in the DataFrame.\nLets pick the supermarket_sales DataFrame exposed below as an example. You can see below, that this DataFrame contains some duplicated values, specifically on the transaction IDs “T001” e “T004”. We also have some “degree of duplication” on the transaction ID “T006”. But the two rows describing this ID “T006” are not precisely identical, since they have a small difference on the quantity column.\n\nfrom pyspark.sql.types import (\n    StructType, StructField,\n    StringType, IntegerType,\n    FloatType\n)\n\nschema = StructType([\n    StructField(\"transaction_id\", StringType(), True),\n    StructField(\"product_name\", StringType(), True),\n    StructField(\"quantity\", IntegerType(), True),\n    StructField(\"price\", FloatType(), True)\n])\n\ndata = [\n    (\"T001\", \"Apple\", 5, 1.2),\n    (\"T001\", \"Apple\", 5, 1.2),\n    (\"T002\", \"Banana\", 3, 0.8),\n    (\"T004\", \"Mango\", 2, 2.0),\n    (\"T004\", \"Mango\", 2, 2.0),\n    (\"T004\", \"Mango\", 2, 2.0),\n    (\"T005\", \"Grapes\", 1, 3.5),\n    (\"T006\", \"Apple\", 2, 1.2),\n    (\"T006\", \"Apple\", 1, 1.2),\n    (\"T007\", \"Banana\", 4, 0.8),\n    (\"T008\", \"Apple\", 3, 1.2)\n]\n\nsupermarket_sales = spark.createDataFrame(data, schema)\nsupermarket_sales.show(6)\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+--------------+------------+--------+-----+\n|transaction_id|product_name|quantity|price|\n+--------------+------------+--------+-----+\n|          T001|       Apple|       5|  1.2|\n|          T001|       Apple|       5|  1.2|\n|          T002|      Banana|       3|  0.8|\n|          T004|       Mango|       2|  2.0|\n|          T004|       Mango|       2|  2.0|\n|          T004|       Mango|       2|  2.0|\n+--------------+------------+--------+-----+\nonly showing top 6 rows\n\n\n\nWe can remove these duplicated values by using the distinct() method. In the example of transaction ID “T004”, all duplicated rows of this ID contains the same values (\"T004\", \"Mango\", 2, 2.0), precisely in this order. Because of that, the distinct() method is enough to remove all of these duplicated values from the table.\n\nsupermarket_sales\\\n    .distinct()\\\n    .show(6)\n\n+--------------+------------+--------+-----+\n|transaction_id|product_name|quantity|price|\n+--------------+------------+--------+-----+\n|          T001|       Apple|       5|  1.2|\n|          T002|      Banana|       3|  0.8|\n|          T004|       Mango|       2|  2.0|\n|          T005|      Grapes|       1|  3.5|\n|          T006|       Apple|       2|  1.2|\n|          T006|       Apple|       1|  1.2|\n+--------------+------------+--------+-----+\nonly showing top 6 rows\n\n\n\nHowever, the two rows describing the transaction ID “T006” have some difference on the quantity column, and as a result, the distinct() method does not identify these two rows as “duplicated values”, and they are not removed from the input DataFrame.\nNow, if we needed a DataFrame that contained one row for each transaction ID (that is, the values on transaction_id column must be unique), we could use the drop_duplicates() method with only the column transaction_id as the key to remove all duplicated values of this column. This way, we get a slightly different output as you can see below.\n\nsupermarket_sales\\\n    .drop_duplicates(['transaction_id'])\\\n    .show(8)\n\n+--------------+------------+--------+-----+\n|transaction_id|product_name|quantity|price|\n+--------------+------------+--------+-----+\n|          T001|       Apple|       5|  1.2|\n|          T002|      Banana|       3|  0.8|\n|          T004|       Mango|       2|  2.0|\n|          T005|      Grapes|       1|  3.5|\n|          T006|       Apple|       2|  1.2|\n|          T007|      Banana|       4|  0.8|\n|          T008|       Apple|       3|  1.2|\n+--------------+------------+--------+-----+\n\n\n\nIn the example above, the duplicated values of IDs “T001” and “T004” were removed as we expected. But we also removed the second value for ID “T006”. Because we did not listed the quantity column on drop_duplicates(), and, as a result, the drop_duplicates() method was not concerned with the differences on the quantity column. In other words, it used solely the transaction_id column to identify the duplicated values.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#other-techniques-for-dealing-with-null-values",
    "href": "Chapters/08-transforming2.html#other-techniques-for-dealing-with-null-values",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.2 Other techniques for dealing with null values",
    "text": "8.2 Other techniques for dealing with null values\nAt Section 5.5.5 I showed how you can use filter() or where() DataFrame methods to remove all rows that contained a null value on some column. There are two other DataFrames methods available in Spark that you might want use to deal with null values. In essence, you can either remove or replace these null values.\n\n8.2.1 Replacing null values\nInstead of removing the null values, and pretending that they never existed, maybe, you prefer to replace these null values by a more useful or representative value, such as 0, or an empty string (''), or a False value, etc. To do that in pyspark, we can use the na.fill() and fillna() DataFrame methods.\nBoth methods mean the same thing, and they work the exact same way. The most popular way of using this methods, is to provide a python dict as input. Inside this dict you have key-value pairs, where the key represents the column name, and the value represents the static value that will replace all null values that are found on the column specified by the key.\nIn the example below, I created a simple df DataFrame which contains some null values on the age column. By providing the dict {'age': 0} to fillna(), I am asking fillna() to replace all null values found on the age column by the value 0 (zero).\n\ndata = [\n    (1, \"John\", None, \"2023-04-05\"),\n    (2, \"Alice\", 25, \"2023-04-09\"),\n    (3, \"Bob\", None, \"2023-04-12\"),\n    (4, \"Jane\", 30, None),\n    (5, \"Mike\", 35, None)\n]\ncolumns = [\"id\", \"name\", \"age\", \"date\"]\ndf = spark.createDataFrame(data, columns)\n\n# Or `df.na.fill({'age': 0}).show()`\n# It is the same thing\ndf.fillna({'age': 0}).show()\n\n+---+-----+---+----------+\n| id| name|age|      date|\n+---+-----+---+----------+\n|  1| John|  0|2023-04-05|\n|  2|Alice| 25|2023-04-09|\n|  3|  Bob|  0|2023-04-12|\n|  4| Jane| 30|      NULL|\n|  5| Mike| 35|      NULL|\n+---+-----+---+----------+\n\n\n\nYou can see in the above example, that the null values present in the date column were maintained intact on the result. Because we did not asked to fillna() to replace the values of this column, by including it on the input dict that we provided.\nIf we do include this date column on the input dict, then, fillna() will take care of this column as well:\n\ndf.fillna({'age': 0, 'date': '2023-01-01'})\\\n    .show()\n\n+---+-----+---+----------+\n| id| name|age|      date|\n+---+-----+---+----------+\n|  1| John|  0|2023-04-05|\n|  2|Alice| 25|2023-04-09|\n|  3|  Bob|  0|2023-04-12|\n|  4| Jane| 30|2023-01-01|\n|  5| Mike| 35|2023-01-01|\n+---+-----+---+----------+\n\n\n\n\n\n8.2.2 Dropping all null values\nSpark also offers the na.drop() and dropna() DataFrames methods, which you can use to easily remove any row that contains a null value on any column of the DataFrame. This is different from filter() and where(), because on these two methods you have to build a logical expression that translate “not-null values”.\nIn contrast, on na.drop() and dropna() methods you do not have a logical expression. You just call these methods, and they do the heavy work for you. They search through the entire DataFrame. When it identify a null value on the DataFrame, it removes the entire row that contains such null value.\nFor example, if we apply these methods on the df DataFrame that we used on the previous section, this is the end result:\n\ndf.na.drop()\\\n    .show()\n\n+---+-----+---+----------+\n| id| name|age|      date|\n+---+-----+---+----------+\n|  2|Alice| 25|2023-04-09|\n+---+-----+---+----------+",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#union-operations",
    "href": "Chapters/08-transforming2.html#union-operations",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.3 Union operations",
    "text": "8.3 Union operations\nWhen you have many individual DataFrames that have the same columns, and you want to unify them into a single big DataFrame that have all the rows from these different DataFrames, you want to perform an UNION operation.\nAn UNION operation works on a pair of DataFrames. It returns the row-wise union of these two DataFrames. In pyspark, we perform UNION operations by using the union() DataFrame method. To use this method, you just provide the other DataFrame you want to make the union with. So the expression df1.union(df2) creates a new DataFrame which contains all the rows from both the df1 and df2 DataFrames.\nMoreover, in commom SQL engines there are usually two kinds of UNION operations, which are: union all and union distinct. When you use an union all operation, you are saying that you just want to unifiy the two DataFrames, no matter what data you find in each one of them. You do not care if duplicated values are generated in the process, because an observation “x” might be present both on df1 and df2.\nIn contrast, an union distinct operation is the exact opposite of that. It merges the rows from both DataFrames together, and then, it removes all duplicated values from the result. So you use an union distinct operation when you want a single DataFrame that contains all rows from both DataFrames df1 and df2, but, you do not want any duplicated rows into this single DataFrame.\nBy default, the union() method always perform an union all operation. However, to do an union distinct operation in pyspark, you actually have to use the union() method in conjunction with the distinct() or drop_duplicates() methods. In other words, there is not a direct method in pyspark that performs an union distinct operation on a single command.\nLook at the example below with df1 and df2 DataFrames.\n\ndf1 = [\n    (1, 'Anne', 'F'),\n    (5, 'Mike', 'M'),\n    (2, 'Francis', 'M'),\n]\n\ndf2 = [\n    (5, 'Mike', 'M'),\n    (7, 'Arthur', 'M'),\n    (1, 'Anne', 'F'),\n]\n\ndf1 = spark.createDataFrame(df1, ['ID', 'Name', 'Sex'])\ndf2 = spark.createDataFrame(df2, ['ID', 'Name', 'Sex'])\n\n# An example of UNION ALL operation:\ndf1.union(df2).show()\n\n+---+-------+---+\n| ID|   Name|Sex|\n+---+-------+---+\n|  1|   Anne|  F|\n|  5|   Mike|  M|\n|  2|Francis|  M|\n|  5|   Mike|  M|\n|  7| Arthur|  M|\n|  1|   Anne|  F|\n+---+-------+---+\n\n\n\n\n# An example of UNION DISTINCT operation\ndf1\\\n    .union(df2)\\\n    .distinct()\\\n    .show()\n\n+---+-------+---+\n| ID|   Name|Sex|\n+---+-------+---+\n|  1|   Anne|  F|\n|  5|   Mike|  M|\n|  2|Francis|  M|\n|  7| Arthur|  M|\n+---+-------+---+\n\n\n\nBecause an UNION operation merges the two DataFrames in a vertical way, the columns between the two DataFrames must match. If the columns between the two DataFrames are not in the same places, a mismatch happens during the operation, and Spark will do nothing to fix your mistake.\nMost programming languages would issue an error at this point, warning you about this conflict between the columns found on each DataFrame and their respective positions. However, in Spark, if the columns are out of order, Spark will continue with the UNION operation, as if nothing was wrong. Spark will not even raise a warning for you. Since this problem can easily pass unnotice, be aware of it.\nIn the example below, we have a third DataFrame called df3. Notice that the columns in df3 are the same of df1 and df2. However, the columns from df3 are in a different order than in df1 and df2.\n\ndata = [\n    ('Marla', 'F', 9),\n    ('Andrew', 'M', 15),\n    ('Peter', 'M', 12)\n]\ndf3 = spark.createDataFrame(data, ['Name', 'Sex', 'ID'])\ndf3.show()\n\n+------+---+---+\n|  Name|Sex| ID|\n+------+---+---+\n| Marla|  F|  9|\n|Andrew|  M| 15|\n| Peter|  M| 12|\n+------+---+---+\n\n\n\nIf we try to perform an UNION operation between, let’s say, df2 and df3, the operations just works. But, the end result of this operation is not correct, as you can see in the example below.\n\ndf2.union(df3).show()\n\n+------+------+---+\n|    ID|  Name|Sex|\n+------+------+---+\n|     5|  Mike|  M|\n|     7|Arthur|  M|\n|     1|  Anne|  F|\n| Marla|     F|  9|\n|Andrew|     M| 15|\n| Peter|     M| 12|\n+------+------+---+\n\n\n\nAlthough this might be problematic, Spark provides an easy-to-use solution when the columns are in different places between each DataFrame. This solution is the unionByName() method.\nThe difference between union() and unionByName() methods, is that the unionByName() method makes an matching by column name, before if performs the UNION. In other words, it compares the column names found on each DataFrame and it matches each column by its name. This way, the columns present on each DataFrame of the UNION must have the same name, but they do not need to be in the same positions on both DataFrames.\nIf we use this method on the same example as above, you can see below that we get a different result, and a correct one this time.\n\ndf2.unionByName(df3)\\\n    .show()\n\n+---+------+---+\n| ID|  Name|Sex|\n+---+------+---+\n|  5|  Mike|  M|\n|  7|Arthur|  M|\n|  1|  Anne|  F|\n|  9| Marla|  F|\n| 15|Andrew|  M|\n| 12| Peter|  M|\n+---+------+---+\n\n\n\nTherefore, if you want to make an UNION operation between two DataFrames, you can generally use the union() method. But if you suspect the columns from these DataFrames might be in different positions on each DataFrame, you can change to the unionByName() method.\nIn contrast, if the columns are different not only on position, but also, on column name, then, unionByName() will not work. The two DataFrames involved on an UNION operation must be very similar. If they are not similar, then, you will have a hard time trying to do the operation.\nAnother problem that you might face is if you try to unify two DataFrames that have different numbers of columns between them. In this situation, it means that the two DataFrames have “different widths”, and, as a result of that, an AnalysisException error will be raised by Spark if you try to unify them with an UNION operation, like in the example below:\n\nfrom pyspark.sql.types import (\n    StructField,\n    StructType,\n    LongType\n)\n\nschema = StructType([StructField('ID', LongType(), False)])\ndf4 = [\n    (19,), (17,), (16,)\n]\ndf4 = spark.createDataFrame(df4, schema)\ndf3.union(df4).show()\n\nAnalysisException: Union can only be performed on tables\nwith the same number of columns, but the first table has\n3 columns and the second table has 1 columns;\n'Union false, false\n:- LogicalRDD [Name#703, Sex#704, ID#705L], false\n+- LogicalRDD [ID#762L], false",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#join-operations",
    "href": "Chapters/08-transforming2.html#join-operations",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.4 Join operations",
    "text": "8.4 Join operations\nA JOIN operation is another very commom operation that is also used to bring data from scattered sources into a single unified DataFrame. In pyspark, we can build JOIN operations by using the join() DataFrame method. This method accepts three arguments, which are:\n\nother: the DataFrame you want to JOIN with (i.e. the DataFrame on the right side of the JOIN);\non: a column name, or a list of column names, that represents the key (or keys) of the JOIN;\nhow: the kind of JOIN you want to perform (inner, full, left, right);\n\nAs a first example, let’s use the info and band_instruments DataFrames. With the source code below, you can quickly re-create these two DataFrames in your session:\n\ninfo = [\n    ('Mick', 'Rolling Stones', '1943-07-26', True),\n    ('John', 'Beatles', '1940-09-10', True),\n    ('Paul', 'Beatles', '1942-06-18', True),\n    ('George', 'Beatles', '1943-02-25', True),\n    ('Ringo', 'Beatles', '1940-07-07', True)\n]\n\ninfo = spark.createDataFrame(\n    info,\n    ['name', 'band', 'born', 'children']\n)\n\nband_instruments = [\n    ('John', 'guitar'),\n    ('Paul', 'bass'),\n    ('Keith', 'guitar')\n]\n\nband_instruments = spark.createDataFrame(\n    band_instruments,\n    ['name', 'plays']\n)\n\nIf you look closely to these two DataFrames, you will probably notice that they both describe musicians from two famous rock bands from 60’s and 70’s. The info DataFrame have more personal or general informations about the musicians, while the band_instruments DataFrame have only data about the main musical instruments that they play.\n\ninfo.show()\n\n+------+--------------+----------+--------+\n|  name|          band|      born|children|\n+------+--------------+----------+--------+\n|  Mick|Rolling Stones|1943-07-26|    true|\n|  John|       Beatles|1940-09-10|    true|\n|  Paul|       Beatles|1942-06-18|    true|\n|George|       Beatles|1943-02-25|    true|\n| Ringo|       Beatles|1940-07-07|    true|\n+------+--------------+----------+--------+\n\n\n\n\nband_instruments.show()\n\n+-----+------+\n| name| plays|\n+-----+------+\n| John|guitar|\n| Paul|  bass|\n|Keith|guitar|\n+-----+------+\n\n\n\nIt might be of your interest, to have a single DataFrame that contains both the personal information and the musical instrument of each musician. In this case, you can build a JOIN operation between these DataFrames to get this result. An example of this JOIN in pyspark would be:\n\ninfo.join(band_instruments, on = 'name', how = 'left')\\\n    .show(5)\n\n+------+--------------+----------+--------+------+\n|  name|          band|      born|children| plays|\n+------+--------------+----------+--------+------+\n|  Mick|Rolling Stones|1943-07-26|    true|  NULL|\n|  John|       Beatles|1940-09-10|    true|guitar|\n|  Paul|       Beatles|1942-06-18|    true|  bass|\n|George|       Beatles|1943-02-25|    true|  NULL|\n| Ringo|       Beatles|1940-07-07|    true|  NULL|\n+------+--------------+----------+--------+------+\n\n\n\nIn the example above, we are performing a left join between the two DataFrames, using the name column as the JOIN key. Now, we have a single DataFrame with all 5 columns from both DataFrames (plays, children, name, band and born).\n\n8.4.1 What is a JOIN ?\nI imagine you are already familiar with JOIN operations. However, in order to build good and precise JOIN operations, is very important to know what a JOIN operation actually is. So let’s revisit it.\nA JOIN operation merges two different DataFrames together into a single unified DataFrame. It does this by using a column (or a set of columns) as keys to identify the observations of both DataFrames, and connects these observations together.\nA JOIN (like UNION) is also an operation that works on a pair of DataFrames. It is very commom to refer to this pair as “the sides of the JOIN”. That is, the DataFrame on the left side of the JOIN, and the DataFrame on the right side of the JOIN. Or also, the DataFrames “A” (left side) and “B” (right side).\nThe main idea (or objective) of the JOIN is to bring all data from the DataFrame on the right side, into the DataFrame on the left side. In other words, a JOIN between DataFrames A and B results into a DataFrame C which contains all columns and rows from both DataFrames A and B.\nIn an UNION operation, both DataFrames must have the same columns, because in an UNION operation you are concatenating both DataFrames together vertically, so the number of columns (or the “width” of the tables) need to match. However, in a JOIN operation, both DataFrames only need to have at least one column in commom. Apart from that, in a JOIN, both DataFrames can have very different structure and columns from each other.\nOne key characteristic of JOIN operations is it’s key matching mechanism. A JOIN uses the columns you provide to build a key. This key is used to identify rows (or “observations”) in both DataFrames. In other words, these keys identifies relationships between the two DataFrames. These relations are vital to the JOIN.\nIf we go back to info and band_instruments DataFrames, and analyse them for a bit more, we can see that they both have a name column which contains the name of the musician being described on the current row. This name column can be used as the key of the JOIN. Because this column is available on both DataFrames, and it can be used to identify a single observation (or a single musician) present in each DataFrame.\nSo the JOIN key is a column (or a combination of columns) that can identify what observations are (and are not) present on both DataFrames. At Figure 8.1, we can see the observations from info and band_instruments in a visual manner. You see in the figure that both Paul and John are described in both DataFrames. At the same time, Ringo, Mick and George are present only on info, while Keith is only at band_instruments.\n\n\n\n\n\n\nFigure 8.1: The relations between info and band_instruments DataFrames\n\n\n\nIn a certain way, you can see the JOIN key as a way to identify relationships between the two DataFrames. A JOIN operation use these relationships to merge your DataFrames in a precise way. A JOIN does not simply horizontally glue two DataFrames together. It uses the JOIN key to perform a matching process between the observations of the two DataFrames.\nThis matching process ensures that the data present DataFrame “B” is correctly transported to the DataFrame “A”. In other words, it ensures that the oranges are paired with oranges, apples with apples, bananas with bananas, you got it.\nJust to describe visually what this matching process is, we have the Figure 8.2 below. In this figure, we have two DataFrames on the left and center of the image, which represents the inputs of the JOIN. We also have a third DataFrame on the right side of the image, which is the output (or the result) of the JOIN.\nIn this specific example, the column that represents the JOIN key is the ID column. Not only this column is present on both DataFrames, but it also represents an unique identifier to each person described in both tables. And that is precisely the job of a JOIN key. It represents a way to identify observations (or “persons”, or “objects”, etc.) on both tables.\nYou can see at Figure 8.2, that when the ID 100 is found on the 1st row of the left DataFrame, the JOIN initiates a lookup/matching process on the center DataFrame, looking for a row in the DataFrame that matches this ID 100. When it finds this ID 100 (on the 4th row of the center DataFrame), it captures and connects these two rows on both DataFrames, because these rows describes the same person (or observation), and because of that, they should be connected. This same matching process happens for all remaining ID values.\n\n\n\n\n\n\nFigure 8.2: The matching process of a JOIN operation\n\n\n\n\n\n8.4.2 The different types of JOIN\nJOIN operations actually comes in different flavours (or types). The four main known types of JOINs are: full, left, right and inner. All of these different types of JOIN perform the same steps and matching processes that we described on the previous section. But they differ on the treatment they do to unmatched observations. In other words, these different types of JOINs differ on what they do in cases when an observation is not found on both DataFrames of the JOIN (e.g. when an observation is found only on table A).\nIn other words, all these four types will perform the same matching process between the two DataFrames, and will connect observations that are found in both DataFrames. However, which rows are included in the final output is what changes between each type (or “flavour”) of JOIN.\nIn this situation, the words “left” and “right” are identifiers to the DataFrames involved on the JOIN operation. That is, the word left refers to the DataFrame on the left side of the JOIN, while the word right refers to the DataFrame on the right side of the JOIN.\nA very useful way of understanding these different types of JOINs is to represent both DataFrames as numerical sets (as we learn in mathematics). The Figure 8.3 gives you a visual representation of each type of JOIN using this “set model” of representing JOINs. Remember, all of these different types of JOIN work the same way, they just do different actions when an observation is not found on both tables.\nThe most “complete” and “greedy” type of JOIN is the full join. Because this type returns all possible combinations of both DataFrames. In other words, this type of JOIN will result in a DataFrame that have all observations from both DataFrames. It does not matter if an observation is present only on table A, or only on table B, or maybe, on both tables. A full join will always try to connect as much observation as it can.\n\n\n\n\n\n\nFigure 8.3: A visual representation for types of JOIN using numerical sets\n\n\n\nThat is why the full join is represented on Figure 8.3 as the union between the two tables (or the two sets). In contrast, an inner join is the intersection of the two tables (or two sets). That is, an inner join will result in a new DataFrame which contains solely the observations that could be found on both tables. If a specific observation is found only on one table of the JOIN, this observation will be automatically removed from the result of the inner join.\nIf we go back to the info and band_instruments DataFrames, and use them as an example, you can see that only Paul and John are included on the result of an inner join. While in a full join, all musicians are included on the resulting DataFrame.\n\n# An inner join between `info` and `band_instruments`:\ninfo.join(band_instruments, on = 'name', how = 'inner')\\\n    .show()\n\n+----+-------+----------+--------+------+\n|name|   band|      born|children| plays|\n+----+-------+----------+--------+------+\n|John|Beatles|1940-09-10|    true|guitar|\n|Paul|Beatles|1942-06-18|    true|  bass|\n+----+-------+----------+--------+------+\n\n\n\n\n# A full join between `info` and `band_instruments`:\ninfo.join(band_instruments, on = 'name', how = 'full')\\\n    .show()\n\n+------+--------------+----------+--------+------+\n|  name|          band|      born|children| plays|\n+------+--------------+----------+--------+------+\n|George|       Beatles|1943-02-25|    true|  NULL|\n|  John|       Beatles|1940-09-10|    true|guitar|\n| Keith|          NULL|      NULL|    NULL|guitar|\n|  Mick|Rolling Stones|1943-07-26|    true|  NULL|\n|  Paul|       Beatles|1942-06-18|    true|  bass|\n| Ringo|       Beatles|1940-07-07|    true|  NULL|\n+------+--------------+----------+--------+------+\n\n\n\nOn the other hand, the left join and right join are kind of self-explanatory. On a left join, all the observations from the left DataFrame are kept intact on the resulting DataFrame of the JOIN, regardless of whether these observations were found or not on the right DataFrame. In contrast, an right join is the opposite of that. So, all observations from the right DataFrame are kept intact on the resulting DataFrame of the JOIN.\nIn pyspark, you can define the type of JOIN you want to use by setting the how argument at join() method. This argument accepts a string with the type of JOIN you want to use as input.\n\nhow = 'left': make a left join;\nhow = 'right': make a right join;\nhow = 'full': make a full join;\nhow = 'inner': make an inner join;\nhow = 'semi': make a semi join;\nhow = 'anti': make an anti join;\n\nYou can see on the list above, that pyspark do have two more types of JOINs, which are the semi join and anti join. These are “filtering types” of JOINs. Because they perform the matching process, and only filter the rows from table A (i.e. the DataFrame on the left side of the JOIN) based on the matches found on table B (i.e. the DataFrame on the right side of the JOIN).\nIn other words, these both types are used as a filter mechanism, and not as a merge mechanism. When you use these two types, instead of merging two DataFrames together, you are interested in filtering the rows of DataFrame A based on the existence of these rows in DataFrame B.\nThis is different from what we learned on left, right, full and inner types, because they do not only change which rows are included in the final result, but they also add the columns from table B into table A. Because of this behavior, these four main types are usually called as “additive types” of JOIN, since they are always adding data from table B into table A, i.e. they are merging the two tables together.\nIn more details, an anti join perform the exact opposite matching process of an inner join. This means that an anti join will always result in a new DataFrame that contains solely the observations that exists only on one DataFrame of the JOIN. In other words, the observations that are found on both tables are automatically removed from the resulting DataFrame of the JOIN. If we look at the example below, we can see that both John and Paul were removed from the resulting DataFrame of the anti join, because these two musicians are present on both DataFrames:\n\ninfo.join(band_instruments, on = 'name', how = 'anti')\\\n    .show()\n\n+------+--------------+----------+--------+\n|  name|          band|      born|children|\n+------+--------------+----------+--------+\n|  Mick|Rolling Stones|1943-07-26|    true|\n|George|       Beatles|1943-02-25|    true|\n| Ringo|       Beatles|1940-07-07|    true|\n+------+--------------+----------+--------+\n\n\n\nIn contrast, a semi join is equivalent to an inner join, with the difference that it does not adds the column from table B into table A. So this type of JOIN filter the rows from DataFrame A that also exists in DataFrame B. If an observation is found on both tables, this observation will appear on the resulting DataFrame.\n\ninfo.join(band_instruments, on = 'name', how = 'semi')\\\n    .show()\n\n+----+-------+----------+--------+\n|name|   band|      born|children|\n+----+-------+----------+--------+\n|John|Beatles|1940-09-10|    true|\n|Paul|Beatles|1942-06-18|    true|\n+----+-------+----------+--------+\n\n\n\nJust to keep using our visual model of sets, on Figure 8.4 you can see the semi and anti JOIN types represented as numerical sets.\n\n\n\n\n\n\nFigure 8.4: The two “filter types” of JOIN\n\n\n\n\n\n8.4.3 A cross JOIN as the seventh type\nWe described six different types of JOINs on the previous section. But Spark also offers a seventh type of JOIN called cross join. This is a special type of JOIN that you can use by calling the crossJoin() DataFrame method.\nIn essence, a cross join returns, as output, the cartesian product between two DataFrames. It is similar to R functions base::expand.grid() or dplyr::expand(), and also, the Python equivalent itertools.product().\nThis is a type of JOIN that you should avoid to use, specially if one (or both) of the DataFrames involved is a big DataFrame with thousands/millions of rows. Because a cross join will always produce a cartesian product between the two DataFrames involved. This means that, if DataFrame A contains \\(x\\) rows, and DataFrame B contains \\(y\\) rows, the end result of the cross join is a new DataFrame C that contains \\(x \\times y\\) rows.\nIn other words, the number of rows in the output of a cross join can grow exponentially. For example, a cross join between a DataFrame of 1 thousand rows, and another DataFrame of 10 thousand of rows (both are small DataFrames for the scale and sizes of a real-world big data environment), would produce a DataFrame with \\(10^3 \\times 10^4 = 10^7\\), that is, 10 milion of rows as output.\nIn a big data environment, dealing with something that grows exponentially… it is never a good idea. So try to avoid a cross join and use him solely on very small DataFrames.\nAs an example, to apply a cross join between info and band_instruments DataFrames we can use the crossJoin() method, like in the example below:\n\ninfo.crossJoin(band_instruments)\\\n    .show()\n\n[Stage 68:=================================&gt;                      (12 + 8) / 20]                                                                                [Stage 69:========&gt;                                             (16 + 12) / 100][Stage 69:============&gt;                                         (24 + 12) / 100][Stage 69:==================&gt;                                   (34 + 12) / 100][Stage 69:======================&gt;                               (42 + 12) / 100][Stage 69:===========================&gt;                          (51 + 12) / 100][Stage 69:================================&gt;                     (60 + 12) / 100][Stage 69:=====================================&gt;                (70 + 12) / 100][Stage 69:==========================================&gt;           (79 + 12) / 100][Stage 69:=============================================&gt;        (84 + 12) / 100]                                                                                \n\n\n+------+--------------+----------+--------+-----+------+\n|  name|          band|      born|children| name| plays|\n+------+--------------+----------+--------+-----+------+\n|  Mick|Rolling Stones|1943-07-26|    true| John|guitar|\n|  Mick|Rolling Stones|1943-07-26|    true| Paul|  bass|\n|  Mick|Rolling Stones|1943-07-26|    true|Keith|guitar|\n|  John|       Beatles|1940-09-10|    true| John|guitar|\n|  John|       Beatles|1940-09-10|    true| Paul|  bass|\n|  John|       Beatles|1940-09-10|    true|Keith|guitar|\n|  Paul|       Beatles|1942-06-18|    true| John|guitar|\n|  Paul|       Beatles|1942-06-18|    true| Paul|  bass|\n|  Paul|       Beatles|1942-06-18|    true|Keith|guitar|\n|George|       Beatles|1943-02-25|    true| John|guitar|\n|George|       Beatles|1943-02-25|    true| Paul|  bass|\n|George|       Beatles|1943-02-25|    true|Keith|guitar|\n| Ringo|       Beatles|1940-07-07|    true| John|guitar|\n| Ringo|       Beatles|1940-07-07|    true| Paul|  bass|\n| Ringo|       Beatles|1940-07-07|    true|Keith|guitar|\n+------+--------------+----------+--------+-----+------+\n\n\n\nA cross join is a special type of JOIN because it does not use “keys” and a matching process. It just computes every possible combination between the rows from both DataFrames. Because of the absence of these keys characteristics of a JOIN, many data analysts and engineers would not call a cross join as a type of JOIN (in other words, they would call it a type of something else). But regardless of our opinions, Spark decided to call this process as the cross join, so this is the way we are calling this process on this book.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#pivot-operations",
    "href": "Chapters/08-transforming2.html#pivot-operations",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.5 Pivot operations",
    "text": "8.5 Pivot operations\nPivot operations are extremely useful, and they are probably the main operation you can use to completely reformat your table. What these operations do is basically change the dimensions of your table. In other words, this kind of operation transform columns into rows, or vice versa.\nAs a comparison with other data frameworks, a pivot operation in Spark is the same operation performed by R functions tidyr::pivot_longer() and tidyr::pivot_wider() from the famous R framework tidyverse; or, the same as the pivot() and melt() methods from the Python framework pandas.\nIn Spark, pivot operations are performed by the pivot() DataFrame method, and by the stack() Spark SQL function. Pivot transformations are available in both directions. That is, you can transform either rows into columns (corresponds to pivot()), or, columns into rows (corresponds to stack()). Let’s begin with stack(), and after that, we explain the pivot() method.\n\n8.5.1 Transforming columns into rows\nThe stack() Spark SQL function allows you to transform columns into rows. In other words, you can make your DataFrame “longer” with this kind of operation, because you remove columns (“width”) from the table, and adds new rows (“heigth”). This gives an aspect of “longer” to your table, because after this operation, you table usually have more rows than columns.\nAs a first example, lets use the religion DataFrame, which you can re-create in your session with the source code below:\n\ndata = [\n    ('Agnostic', 27, 34, 60),\n    ('Atheist', 12, 27, 37),\n    ('Buddhist', 27, 21, 30)\n]\ncols = ['religion', '&lt;$10k', '$10k-$20k', '$20k-$30k']\nreligion = spark.createDataFrame(data, cols)\nreligion.show()\n\n+--------+-----+---------+---------+\n|religion|&lt;$10k|$10k-$20k|$20k-$30k|\n+--------+-----+---------+---------+\n|Agnostic|   27|       34|       60|\n| Atheist|   12|       27|       37|\n|Buddhist|   27|       21|       30|\n+--------+-----+---------+---------+\n\n\n\nThis DataFrame is showing us the average salary of people belonging to different religious groups. In each column of this DataFrame, you have data for a specific salary level (or range). This is a structure that can be easy and intuitive for some specific operations, but it also might impose some limitations, specially if you need to apply a vectorised operation over these salary ranges.\nThe basic unit of this DataFrame are the religious groups, and the salary ranges represents a characteristic of these groups. The different salary ranges are distributed across different columns. But what if we transformed these multiple columns into multiple rows? How can we accomplish that?\nWhat we need to do, is to concentrate the labels (or the column names) of salary ranges into a single column, and move the respective values of the salary levels into another column. In other words, we need to create a column that contains the labels, and another column that contains the values. Figure 8.5 have a visual representation of this process:\n\n\n\n\n\n\nFigure 8.5: A visual representation of a pivot operation\n\n\n\nLet’s build this transformation in pyspark. First, remember that stack() is not a DataFrame method. It is a Spark SQL function. However, it is not an exported Spark SQL function, which means that you cannot import this function from the pyspark.sql.function module. This means that stack() will never be directly available in your python session to use.\nSo how do you use it? The answer is: use it inside Spark SQL! The stack() function is not available directly in python, but it is always available in Spark SQL, so all you need to do, is to use stack() inside functions and methods such as expr() (that I introduced at Section 7.6.2), or sql() to access Spark SQL functionality.\nNow, the stack() function have two main arguments, which are the number of columns to transform into rows, and a sequence of key-value pairs that describes which columns will be transformed into rows, and the label values that corresponds to each column being transformed.\nAs a first example, the source code below replicates the transformation exposed at Figure 8.5:\n\nfrom pyspark.sql.functions import expr\nstack_expr = \"\"\"\nstack(3,\n    '&lt;$10k', `&lt;$10k`,\n    '$10k-$20k', `$10k-$20k`,\n    '$20k-$30k', `$20k-$30k`\n) AS (salary_range, avg_salary)\n\"\"\"\n\nlonger_religion = religion\\\n    .select('religion', expr(stack_expr))\n\nlonger_religion.show()\n\n+--------+------------+----------+\n|religion|salary_range|avg_salary|\n+--------+------------+----------+\n|Agnostic|       &lt;$10k|        27|\n|Agnostic|   $10k-$20k|        34|\n|Agnostic|   $20k-$30k|        60|\n| Atheist|       &lt;$10k|        12|\n| Atheist|   $10k-$20k|        27|\n| Atheist|   $20k-$30k|        37|\n|Buddhist|       &lt;$10k|        27|\n|Buddhist|   $10k-$20k|        21|\n|Buddhist|   $20k-$30k|        30|\n+--------+------------+----------+\n\n\n\nAn important aspect about the stack() function, is that it always outputs two new columns (one column for the labels - or the keys, and another for the values). In the example above, these new columns are salary_range and avg_salary.\nThe first column identifies from which column (before the stack() operation) the value present at avg_salary came from. This means that this first column produced by stack() works as a column of labels or identifiers. These labels identify from which of the three transformed columns (&lt;$10k, $10k-$20k and $20k-$30k) the row value came from. In the visual representation exposed at Figure 8.5, this “labels column” is the income column.\nOn the other hand, the second column in the output of stack() contains the actual values that were present on the columns that were transformed. This “values column” in the example above corresponds to the column avg_salary, while in the visual representation exposed at Figure 8.5, it is the values column.\nThe first argument in stack() is always the number of columns that will be transformed by the function into rows. In our example, we have three columns that we want to transform, which are &lt;$10k, $10k-$20k and $20k-$30k. That is why we have the number 3 as the first argument to stack().\nAfter that, we have a sequence of key-value pairs. In each pair, the value side (i.e. the right side) of the pair contains the name of the column that will be transformed, and the key side (i.e. the left side) of the pair contains the “label value”, or, in other words, which value represents, marks, label, or identifies the values that came from the column described in the right side of the pair.\nNormally, you set the label value to be equivalent to the column name. That is, both sides of each pair are usually pretty much the same. But you can change this behaviour if you want. In the example below, all values that came from the &lt;$10k are labeled as \"Below $10k\", while the values from the $10k-$20k column, are labeled in the output as \"Between $10k-$20k\", etc.\n\nstack_expr = \"\"\"\nstack(3,\n    'Below $10k', `&lt;$10k`,\n    'Between $10k-$20k', `$10k-$20k`,\n    'Between $20k-$30k', `$20k-$30k`\n) AS (salary_range, avg_salary)\n\"\"\"\n\nreligion\\\n    .select('religion', expr(stack_expr))\\\n    .show()\n\n+--------+-----------------+----------+\n|religion|     salary_range|avg_salary|\n+--------+-----------------+----------+\n|Agnostic|       Below $10k|        27|\n|Agnostic|Between $10k-$20k|        34|\n|Agnostic|Between $20k-$30k|        60|\n| Atheist|       Below $10k|        12|\n| Atheist|Between $10k-$20k|        27|\n| Atheist|Between $20k-$30k|        37|\n|Buddhist|       Below $10k|        27|\n|Buddhist|Between $10k-$20k|        21|\n|Buddhist|Between $20k-$30k|        30|\n+--------+-----------------+----------+\n\n\n\nFurthermore, because the stack() function always outputs two new columns, if you want to rename these two new columns being created, to give them more readable and meaningful names, you always need to provide two new column names at once, inside a tuple, to the AS keyword.\nIn the example above, this tuple is (salary_range, avg_salary). The first value in the tuple is the new name for the “labels column”, while the second value in the tuple, is the new name for the “values column”.\nNow, differently from other Spark SQL functions, the stack() function should not be used inside the withColumn() method, and the reason for this is very simple: stack() always returns two new columns as output, but the withColumn() method can only create one column at a time.\nThis is why you get an AnalysisException error when you try to use stack() inside withColumn(), like in the example below:\n\nreligion\\\n    .withColumn('salary_ranges', stack_expr)\\\n    .show()\n\nAnalysisException: The number of aliases supplied in the AS clause\ndoes not match the number of columns output by the UDTF expected\n2 aliases but got salary_ranges \n\n\n8.5.2 Transforming rows into columns\nOn the other side, if you want to transform rows into columns in your Spark DataFrame, you can use the pivot() method. One key aspect of the pivot() method, is that it must always be used in conjunction with the groupby() method that we introduced at Section 5.10.4. In other words, pivot() does not work without groupby().\nYou can think (or interpret) that the groupby() method does the job of defining (or identifying) which columns will be present in the output of pivot(). For example, if your DataFrame contains five columns, which are A, B, C, D and E; but you only listed columns A and C inside groupby(), this means that if you perform a pivot operation after that, the columns B, D and E will not be present in the output of pivot(). These three columns (B, D and E) will be automatically dropped during the pivot operation.\nIn contrast, the pivot() method does the job of identifying a single column containing the values that will be transformed into new columns. In other words, if you list the column car_brands inside pivot(), and, this column contains four unique values, for example, Audi, BMW, Jeep and Fiat, this means that, in the output of pivot(), four new columns will be created, named as Audi, BMW, Jeep and Fiat.\nTherefore, we use groupby() to define the columns that will be kept intact on the output of the pivot operation; we use pivot() to mark the column that contains the rows that we want to transform into new columns; at last, we must learn how to define which values will populate the new columns that will be created. Not only that, we also need to specify how these values will be calculated. And for that, we need to use an aggregating function.\nThis is really important, you can not do a pivot operation without aggregating the values that will compose (or populate) the new columns you are creating. Without it, Spark will not let you do the pivot operation using the pivot() method.\nAs a first example, let’s return to the religion DataFrame. More specifically, to the longer_religion DataFrame, which is the pivoted version of the religion DataFrame that we created on the previous section, using the stack() function.\n\nlonger_religion.show()\n\n+--------+------------+----------+\n|religion|salary_range|avg_salary|\n+--------+------------+----------+\n|Agnostic|       &lt;$10k|        27|\n|Agnostic|   $10k-$20k|        34|\n|Agnostic|   $20k-$30k|        60|\n| Atheist|       &lt;$10k|        12|\n| Atheist|   $10k-$20k|        27|\n| Atheist|   $20k-$30k|        37|\n|Buddhist|       &lt;$10k|        27|\n|Buddhist|   $10k-$20k|        21|\n|Buddhist|   $20k-$30k|        30|\n+--------+------------+----------+\n\n\n\nWe can use this longer_religion DataFrame and the pivot() method to perform the inverse operation we described at Figure 8.5. In other words, we can re-create the religion DataFrame through longer_religion using the pivot() method. The source code below demonstrates how we could do such thing:\n\nfrom pyspark.sql.functions import first\n\n# Equivalent to the `religion` DataFrame:\nlonger_religion\\\n    .groupby('religion')\\\n    .pivot('salary_range')\\\n    .agg(first('avg_salary'))\\\n    .show()\n\n+--------+---------+---------+-----+\n|religion|$10k-$20k|$20k-$30k|&lt;$10k|\n+--------+---------+---------+-----+\n|Agnostic|       34|       60|   27|\n|Buddhist|       21|       30|   27|\n| Atheist|       27|       37|   12|\n+--------+---------+---------+-----+\n\n\n\nIn the example above, you can see the three core parts that we described: 1) use groupby() to define which columns will be preserved from the input DataFrame; 2) use pivot() to define which column will be used to transform rows into new columns; 3) the aggregating functions describing which values will be used, and how they are going to be calculated - agg(first('avg_salary')). Figure 8.6 exposes these core parts in a visual manner.\n\n\n\n\n\n\nFigure 8.6: The three core parts that define the use of pivot()\n\n\n\nWhen you use pivot(), you have to apply an aggregating function over the column from which you want to extract the values that will populate the new columns created from the pivot operation. This is a prerequisite, because Spark needs to know what he must do in case he finds two (or more) values that are mapped to the same cell in the pivot operation.\nIn the above example, we used the first() function to aggregate the values from the avg_salary column. With this function, we are telling Spark, that if it finds two (or more) values that are mapped the same cell, then, Spark should pick the first value if finds in the input DataFrame, and simply ignore the remaining values.\nLet’s see an example. In the code chunk below, we are creating a new DataFrame called df:\n\ndata = [\n    ('2023-05-01', 'Anne', 1, 15),\n    ('2023-05-02', 'Mike', 2, 25),\n    ('2023-05-02', 'Mike', 2, 34),\n    ('2023-05-02', 'Mike', 2, 21),\n    ('2023-05-03', 'Dani', 3, 18)\n]\ncols = ['date', 'name', 'id', 'value']\ndf = spark.createDataFrame(data, cols)\ndf.show()\n\n+----------+----+---+-----+\n|      date|name| id|value|\n+----------+----+---+-----+\n|2023-05-01|Anne|  1|   15|\n|2023-05-02|Mike|  2|   25|\n|2023-05-02|Mike|  2|   34|\n|2023-05-02|Mike|  2|   21|\n|2023-05-03|Dani|  3|   18|\n+----------+----+---+-----+\n\n\n\nLet’s suppose you want to transform the rows in the name column into new columns, and, populate these new columns with the values from the value column. Following what we discussed until now, we could do this by using the following source code:\n\ndf\\\n    .groupby('date', 'id')\\\n    .pivot('name')\\\n    .agg(first('value'))\\\n    .show()\n\n+----------+---+----+----+----+\n|      date| id|Anne|Dani|Mike|\n+----------+---+----+----+----+\n|2023-05-02|  2|NULL|NULL|  25|\n|2023-05-03|  3|NULL|  18|NULL|\n|2023-05-01|  1|  15|NULL|NULL|\n+----------+---+----+----+----+\n\n\n\nHowever, there is a problem with this operation, because we lost some observations in the process. The specific combination ('2023-05-02', 'Mike') have three different values in the value column, which are 25, 34 and 21. But there is only a single cell in the new DataFrame (i.e. the output of the pivot operation) to hold these values. More specifically, the cell located at the first row in the fifth column.\nIn other words, Spark found three different values for a single cell (or single space), and this is always a problem. Spark cannot simply put three different values in a single cell. A Spark DataFrame just do not work that way. Every cell in a Spark DataFrame should always hold a single value, whatever that value is.\nThat is the exact problem that an aggregating function solves in a pivot operation. The aggregating function does the job of ensuring that a single value will be mapped to every new cell created from the pivot operation. Because an aggregating function is a function that aggregates (or that summarises) a set of values into a single value.\nIn the example above we used first() as our aggregating function. So when Spark encountered the three values from the combination ('2023-05-02', 'Mike'), it simply picked the first value from the three. That is why we find the value 25 at the first row in the fifth column.\nWe can change this behaviour by changing the aggregating function applied. In the example below, we are using sum(), and, as a result, we get now the value of 80 (which is the sum of values 25, 34 and 21) at the first row in the fifth column.\n\nfrom pyspark.sql.functions import sum\ndf\\\n    .groupby('date', 'id')\\\n    .pivot('name')\\\n    .agg(sum('value'))\\\n    .show()\n\n+----------+---+----+----+----+\n|      date| id|Anne|Dani|Mike|\n+----------+---+----+----+----+\n|2023-05-02|  2|NULL|NULL|  80|\n|2023-05-03|  3|NULL|  18|NULL|\n|2023-05-01|  1|  15|NULL|NULL|\n+----------+---+----+----+----+\n\n\n\nNow, depending on the data and the structure from your DataFrame, Spark might never encounter a situation where it finds two (or more) values mapped to the same cell in a pivot operation. On this specific case, the output from the pivot operation will likely be the same for many different aggregating functions. In other words, the choice of the aggregating function you want to use might be irrelevant over this specific situation.\nFor example, in the longer_religion example that we showed before, I could use the last() aggregating function (instead of first()), and get the exact same result as before. Because in this specific situation, Spark does not find any case of two (or more) values mapped to the same cell in the output DataFrame.\n\nfrom pyspark.sql.functions import last\nlonger_religion\\\n    .groupby('religion')\\\n    .pivot('salary_range')\\\n    .agg(last('avg_salary'))\\\n    .show()\n\n+--------+---------+---------+-----+\n|religion|$10k-$20k|$20k-$30k|&lt;$10k|\n+--------+---------+---------+-----+\n|Agnostic|       34|       60|   27|\n|Buddhist|       21|       30|   27|\n| Atheist|       27|       37|   12|\n+--------+---------+---------+-----+",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/08-transforming2.html#collecting-and-explode-operations",
    "href": "Chapters/08-transforming2.html#collecting-and-explode-operations",
    "title": "8  Transforming your Spark DataFrame - Part 2",
    "section": "8.6 Collecting and explode operations",
    "text": "8.6 Collecting and explode operations\nYou can retract or extend vertically your DataFrame, by nesting multiple rows into a single row, or, the inverse, which is unnesting (or exploding) one single row into multiple rows. The R tidyverse framework is probably the only data framework that have a defined name for this kind of operation, which are called “nesting” and “unnesting”.\nIn Spark, you perform this kind of operations by using the collect_list(), collect_set() and explode() functions, which all comes from the pyspark.sql.functions module. The collect_list() and collect_set() functions are used for retracting (or nesting) your DataFrame, while the explode() function is used for extending (or unnesting).\nAs a quick comparison, explode() is very similar to the R function tidyr::unnest_longer() from the tidyverse framework, and also, similar to the Python method explode() in the pandas framework. In the other hand, collect_list() and collect_set() functions does a similar job to the R function tidyr::nest().\n\n8.6.1 Expanding (or unnesting) with explode()\nAs an example, we have the employees DataFrame below. Each row in this DataFrame describes an employee. The knowledge column describes which programming language the employee have experience with, and, the employee_attrs column contains dictionaries with general attributes of each employee (such it’s department and it’s name).\n\ndata = [\n    (1, [\"R\", \"Python\"], {'dep': 'PR', 'name': 'Anne'}),\n    (2, [\"Scala\"], {'dep': 'PM', 'name': 'Mike'}),\n    (3, [\"Java\", \"Python\"], {'dep': 'HF', 'name': 'Sam'})\n]\ncolumns = [\"employee_id\", \"knowledge\", \"employee_attrs\"]\nemployees = spark.createDataFrame(data, columns)\nemployees.show()\n\n+-----------+--------------+--------------------+\n|employee_id|     knowledge|      employee_attrs|\n+-----------+--------------+--------------------+\n|          1|   [R, Python]|{name -&gt; Anne, de...|\n|          2|       [Scala]|{name -&gt; Mike, de...|\n|          3|[Java, Python]|{name -&gt; Sam, dep...|\n+-----------+--------------+--------------------+\n\n\n\nYou can use the printSchema() method that we introduced at Section 3.9 to see the schema of the DataFrame. You can see in the result below that knowledge is a column of arrays of strings (i.e. ArrayType), while employee_attrs is a column of maps (i.e. MapType).\n\nemployees.printSchema()\n\nroot\n |-- employee_id: long (nullable = true)\n |-- knowledge: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- employee_attrs: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n\n\nSuppose you wanted to calculate the number of employees that have experience in each programming language. To do that, it would be great to transform the knowledge column into a column of strings. Why? Because it would make the counting of the employees easier.\nIn other words, if an employee knows, for example, 3 different programming languages, it would be better to have 3 different rows that references this same employee, instead of having a single row that contains an array of three elements, like in the employees DataFrame. We can transform the current DataFrame into this new format by using the explode() function.\nWhen you apply the explode() function over a column of arrays, this function will create a new row for each element in each array it finds in the column. For example, the employee of ID 1 knows two programming languages (R and Python). As a result, when we apply explode() over the knowledge column, two rows are created for the employee of ID 1. As you can see in the example below:\n\nfrom pyspark.sql.functions import explode\nexplode_array = employees\\\n    .select(\n        'employee_id',\n        explode('knowledge')\n    )\n\nexplode_array.show()\n\n+-----------+------+\n|employee_id|   col|\n+-----------+------+\n|          1|     R|\n|          1|Python|\n|          2| Scala|\n|          3|  Java|\n|          3|Python|\n+-----------+------+\n\n\n\nOn the other hand, instead of arrays, the behaviour of explode() is slightly different when you apply it over a column of maps, such as employee_attrs. Because each element in a map have two components: a key and a value. As a consequence, when you apply explode() over a column of maps, each element in the map generates two different rows, which are stored in two separated columns, called key and value.\nTake the result below as an example. First, we have two new columns that were not present before (key and value). Each row in the key column represents the key for an element in the input map. While the value column represents the values of those elements.\n\nexplode_map = employees\\\n    .select(\n        'employee_id',\n        explode('employee_attrs')\n    )\n\nexplode_map.show()\n\n+-----------+----+-----+\n|employee_id| key|value|\n+-----------+----+-----+\n|          1|name| Anne|\n|          1| dep|   PR|\n|          2|name| Mike|\n|          2| dep|   PM|\n|          3|name|  Sam|\n|          3| dep|   HF|\n+-----------+----+-----+\n\n\n\nThis kind of output is powerful, specially with pivot operations, because you can easily organize all the data found in a column of maps into a series of new columns, like this:\n\nexplode_map\\\n    .groupby('employee_id')\\\n    .pivot('key')\\\n    .agg(first('value'))\\\n    .show()\n\n+-----------+---+----+\n|employee_id|dep|name|\n+-----------+---+----+\n|          1| PR|Anne|\n|          2| PM|Mike|\n|          3| HF| Sam|\n+-----------+---+----+\n\n\n\n\n\n8.6.2 The different versions of explode()\nThe explode() function have three brother functions, which are explode_outer(), posexplode() and posexplode_outer(). All of these functions have very small differences between them. However, these differences might be important/useful to you.\nFirst, the difference between explode() and explode_outer() is on the null values. If an array contains a null value, explode() will ignore this null value. In other words, explode() does not create a new row for any null value. In contrast, explode_outer() does create a new row even if the value is null.\nFor example, if explode() encounter an array with 5 elements where 2 of them are null values, then, explode() will output 3 new rows. The 2 null values in the input array are automatically ignored. However, if explode_outer() encountered the same array, then, it would output 5 new rows, no matter which values are inside this input array.\nSecond, the difference between explode() and posexplode(), is that posexplode() also returns the index that identifies the position in the input array where each value is. In other words, if we applied the posexplode() over the knowledge column of employee DataFrame, we would get a new column called pos. By looking at this column pos, we could see that the value \"Python\" for the employee of ID 1, was on the position of index 1 on the original array that the function encountered in the original knowledge column.\n\nfrom pyspark.sql.functions import posexplode\nemployees\\\n    .select(\n        'employee_id',\n        posexplode('knowledge')\n    )\\\n    .show()\n\n+-----------+---+------+\n|employee_id|pos|   col|\n+-----------+---+------+\n|          1|  0|     R|\n|          1|  1|Python|\n|          2|  0| Scala|\n|          3|  0|  Java|\n|          3|  1|Python|\n+-----------+---+------+\n\n\n\nThird, as you can probably imagine, the function posexplode_outer() incorporates the two visions from the previous brothers. So, not only posexplode_outer() creates a row for each element in the array, no matter if this element is a null value or not, but it also returns the index that identifies the position in the input array where each value is.\n\n\n8.6.3 Retracting (or nesting) with collect_list() and collect_set()\nIn resume, you use the collect_list() and collect_set() functions to retract your DataFrame. That is, to reduce the number of rows of the DataFrame, while keeping the same amount of information.\nTo do this you just aggregate your DataFrame, using the agg() method, and, apply the collect_list() or collect_set() function over the columns you want to retract (or nest). You likely want to use the groupby() method as well in this case, to perform an aggregation per group.\nBecause if you do not define any group in this situation, you will aggregate the entire DataFrame into a single value. This means, that you would get as output, a new DataFrame with only a single row, and, all rows from the input DataFrame would be condensed inside this single row in the output DataFrame.\nIn essence, what the collect_list() function do is collect a set of rows and store it in an array. The collect_set() function does the same thing, with the difference that it stores this set of rows in a set (which is an array of unique values). In other words, collect_set() collects a set of rows, then, it removes all duplicated rows in this set, then, it stores the remaining values in an array.\nTo demonstrate the collect_list() and collect_set() functions, we can use the supermarket_sales DataFrame that we introduced at Section 8.1 as an example:\n\nsupermarket_sales.show()\n\n+--------------+------------+--------+-----+\n|transaction_id|product_name|quantity|price|\n+--------------+------------+--------+-----+\n|          T001|       Apple|       5|  1.2|\n|          T001|       Apple|       5|  1.2|\n|          T002|      Banana|       3|  0.8|\n|          T004|       Mango|       2|  2.0|\n|          T004|       Mango|       2|  2.0|\n|          T004|       Mango|       2|  2.0|\n|          T005|      Grapes|       1|  3.5|\n|          T006|       Apple|       2|  1.2|\n|          T006|       Apple|       1|  1.2|\n|          T007|      Banana|       4|  0.8|\n|          T008|       Apple|       3|  1.2|\n+--------------+------------+--------+-----+\n\n\n\nSuppose you wanted a new DataFrame that had a single row for each transaction_id without losing any amount of information from the above DataFrame. We could do this by using the collect_list() function, like in the example below:\n\nfrom pyspark.sql.functions import collect_list\nsupermarket_sales\\\n    .groupby('transaction_id')\\\n    .agg(\n        collect_list('product_name').alias('product_name'),\n        collect_list('quantity').alias('quantity'),\n        collect_list('price').alias('price')\n    )\\\n    .show()\n\n+--------------+--------------------+---------+---------------+\n|transaction_id|        product_name| quantity|          price|\n+--------------+--------------------+---------+---------------+\n|          T001|      [Apple, Apple]|   [5, 5]|     [1.2, 1.2]|\n|          T002|            [Banana]|      [3]|          [0.8]|\n|          T004|[Mango, Mango, Ma...|[2, 2, 2]|[2.0, 2.0, 2.0]|\n|          T005|            [Grapes]|      [1]|          [3.5]|\n|          T006|      [Apple, Apple]|   [2, 1]|     [1.2, 1.2]|\n|          T007|            [Banana]|      [4]|          [0.8]|\n|          T008|             [Apple]|      [3]|          [1.2]|\n+--------------+--------------------+---------+---------------+\n\n\n\nThe expression groupby('transaction_id') ensures that we have (on the output DataFrame) a single row for each unique value in the transaction_id column. While the collect_list() function does the job of condensing all the information of the remaining columns into a single row for each transaction_id.\nNow, if we use collect_set() instead of collect_list(), we would get a slightly different result. Because, as we described before, the collect_set() function removes all duplicated values found in the set of rows it collects.\n\nfrom pyspark.sql.functions import collect_set\nsupermarket_sales\\\n    .groupby('transaction_id')\\\n    .agg(\n        collect_set('product_name').alias('product_name'),\n        collect_set('quantity').alias('quantity'),\n        collect_set('price').alias('price')\n    )\\\n    .show()\n\n+--------------+------------+--------+-----+\n|transaction_id|product_name|quantity|price|\n+--------------+------------+--------+-----+\n|          T001|     [Apple]|     [5]|[1.2]|\n|          T002|    [Banana]|     [3]|[0.8]|\n|          T004|     [Mango]|     [2]|[2.0]|\n|          T005|    [Grapes]|     [1]|[3.5]|\n|          T006|     [Apple]|  [1, 2]|[1.2]|\n|          T007|    [Banana]|     [4]|[0.8]|\n|          T008|     [Apple]|     [3]|[1.2]|\n+--------------+------------+--------+-----+",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transforming your Spark DataFrame - Part 2</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#the-write-object-as-the-main-entrypoint",
    "href": "Chapters/07-export.html#the-write-object-as-the-main-entrypoint",
    "title": "9  Exporting data out of Spark",
    "section": "9.1 The write object as the main entrypoint",
    "text": "9.1 The write object as the main entrypoint\nEvery Spark session you start has an built-in read object that you can use to read data and import it into Spark (this object was described at Section 6.1), and the same applies to writing data out of Spark. That is, Spark also offers a write object that you can use to write/output data out of Spark.\nBut in contrast to the read object, which is avaiable trough the SparkSession object (spark), this write object is available trough the write method of any DataFrame object. In other words, every DataFrame you create in Spark has a built-in write object that you can use to write/export the data present in this DataFrame out of Spark.\nAs an example, let’s use the transf DataFrame that I presented at Chapter 5. The write method of the transf DataFrame object is the main entrypoint to all the facilities that Spark offers to write/export transf’s data to somewhere else.\n\ntransf.write\n\n&lt;pyspark.sql.readwriter.DataFrameWriter at 0x7f66d544f7c0&gt;\n\n\nThis write object is very similar in structure to the read object. Essentially, this write object have a collection of write engines. Each write engine is speciallized in writing data into a specific file format. So you have an engine for CSV files, another engine for JSON files, another for Parquet files, etc.\nEvery write object have the following methods:\n\nmode(): set the mode of the write process. This affects how the data will be written to the files, and how the process will behaviour if exceptions (or erros) are raised during runtime.\noption(): set an option to be used in the write process. This option might be specific to the write engine used, or, might be an option that is global to the write process (i.e. an option that does not depend of the chosen engine).\ncsv(): the write engine to export data to CSV files.\njson(): the write engine to export data to JSON files.\nparquet(): the write engine to export data to Parquet files.\norc(): the write engine to export data to ORC files.\ntext(): the write engine to export data to text files.\njdbc(): saves the data of the current DataFrame into a database using the JDBC API.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#sec-write-example",
    "href": "Chapters/07-export.html#sec-write-example",
    "title": "9  Exporting data out of Spark",
    "section": "9.2 Exporting the transf DataFrame",
    "text": "9.2 Exporting the transf DataFrame\nAs a first example on how to export data out of Spark, I will export the data from the transf DataFrame. Over the next sections, I will cover individual aspects that influences this write/export process. You should know and consider each of these individual aspects when exporting your data.\n\n9.2.1 Quick export to a CSV file\nLets begin with a quick example of exporting the Spark data to a CSV file. For this job, we need to use the write engine for CSV files, which is the csv() method from the write object.\nThe first (and main) argument to all write engines available in Spark is a path to a folder where you want to store the exported files. This means that (whatever write engine you use) Spark will always write the files (with the exported data) inside a folder.\nSpark needs to use a folder to write the data. Because it generates some extra files during the process that serves as “placeholders” or as “statuses”. That is why Spark needs to create a folder, to store all of these different files together during the process.\nIn the example below, I decided to write this data into a folder called transf_export.\ntransf.write.csv(\"transf_export\")\nNow, after I executed the above command, if I take a look at my current working directory, I will see the transf_export folder that was created by Spark.\nfrom pathlib import Path\ncurrent_directory = Path(\".\")\nfolders_in_current_directory = [\n    str(item)\n    for item in current_directory.iterdir()\n    if item.is_dir()\n]\n\nprint(folders_in_current_directory)\n['metastore_db', 'transf_export']\nAnd if I look inside this transf_export folder I will see two files. One is the placeholder file (_SUCCESS), and the other, is a CSV file containing the exported data (part-*.csv).\nexport_folder = Path(\"transf_export\")\nfiles = [str(x.name) for x in export_folder.iterdir()]\nprint(files)\n['part-00000-a4ee2ff4-4b7f-499e-a904-cec8d524ac56-c000.csv', '_SUCCESS']\nWe can see this file structure by using the tree command line utility1 to build a diagram of this file structure:\nTerminal$ tree transf_export\ntransf_export\n├── part-00000-a4ee2ff4-4b7f-499e-a904-cec8d524ac56-c000.csv\n└── _SUCCESS\n\n\n9.2.2 Setting the write mode\nYou can set the mode of a write process by using the mode() method. This “mode of the write process” affects specially the behavior of the process when files for this particular DataFrame you trying to export already exists in your file system.\nThere are four write modes available in Spark:\n\nappend: will append the exported data to existing files of this specific DataFrame.\noverwrite: will overwrite the data inside existing files of this specific DataFrame with the data that is being currently exported.\nerror or errorifexists: will throw an exception in case already existing files for this specific DataFrame are found.\nignore: silently ignore/abort this write operation in case already existing files for this specific DataFrame are found.\n\nIf we set the write mode to overwrite, this means that every time we execute the command below, the files inside the folder transf_export are rewritten from scratch. Everytime we export the data, the files part-* inside the folder are rewritten to contain the most fresh data from transf DataFrame.\ntransf.write\\\n    .mode(\"overwrite\")\\\n    .csv(\"transf_export\")\nHowever, if we set the write mode to error, and run the command again, then an error will be raised to indicate that the folder (transf_export) where we are trying to write the files already exists.\ntransf.write\\\n    .mode(\"error\")\\\n    .csv(\"transf_export\")\nAnalysisException: [PATH_ALREADY_EXISTS]\nPath file:/home/pedro/Documentos/Projetos/Livros/Introd-pyspark/Chapters/transf_export\nalready exists. Set mode as \"overwrite\" to overwrite the existing path.\nIn contrast, if we set the write mode to append, then the current data of transf is appended (or “added”) to the folder transf_export.\ntransf.write\\\n    .mode(\"append\")\\\n    .csv(\"transf_export\")\nNow, if I take a look at the contents of the transf_export folder, I will see now two part-* files instead of just one. Both files have the same size (around 218 kb) because they both contain the same data, or the same lines from the transf DataFrame.\nTerminal$ tree transf_export\ntransf_export\n├── part-00000-a4ee2ff4-4b7f-499e-a904-cec8d524ac56-c000.csv\n├── part-00000-ffcc7487-fc60-403b-a815-a1dd56894062-c000.csv\n└── _SUCCESS\nThis means that the data is currently duplicated inside the transf_export folder. We can see this duplication by looking at the number of rows of the DataFrame contained inside transf_export. We can use spark.read.load() to quickly load the contents of the transf_export folder into a new DataFrame, and use count() method to see the number of rows.\ndf = spark.read.load(\n    \"transf_export\",\n    format = \"csv\",\n    header = False\n)\ndf.count()\n4842\nThe result above show us that the folder transf_export currently contains 4842 rows of data. This is the exact double of number of rows in the transf DataFrame, which have 2421 rows.\n\ntransf.count()\n\n2421\n\n\nSo, in resume, the difference between write mode overwrite and append, is that overwrite causes Spark to erase the contents of transf_export, before it starts to write the current data into the folder. This way, Spark exports the most recent version of the data stored inside the DataFrame. In contrast, append simply appends (or adds) new files to the folder transf_export with the most recent version of the data stored inside the DataFrame.\nAt Section 7.2.1.6 (or more specifically, at Figure 7.1) we presented this difference visually. So, in case you don’t understood fully the difference between these two write modes, you can comeback at Section 7.2.1.6 and check Figure 7.1 to see if it clears your understanding. OBS: save modes = write modes.\n\n\n9.2.3 Setting write options\nEach person might have different needs, and also, each file format (or each write engine) have its particularities or advantages that you may need to exploit. As a consequence, you might need to set some options to customize the writing process to fit into your needs.\nYou can set options for the write process using the option() method of the write object. This method works with key value pairs. Inside this method, you provide the a key that identifies the option you want to set, and the value you want to give to this option.\nFor CSV files, an option that is very popular is the sep option, that corresponds to the separator character of the CSV. This is a special character inside the CSV file that separates each column field.\nAs an example, if we wanted to build a CSV file which uses the semicolon (; - which is the european standard for CSV files) as the separator character, instead of the comma (, - which is the american standard for CSV files), we just need to set the sep option to ;, like this:\ntransf\\\n    .write\\\n    .mode(\"overwrite\")\\\n    .option(\"sep\", \";\")\\\n    .csv(\"transf_export\")\nEach file format (or each write engine) have different options that are specific (or characteristic) to the file format itself. For example, JSON and CSV files are text file formats, and because of that, one key aspect to them is the encoding of the text that is being stored inside these files. So both write engines for these file formats (csv() and json()) have an option called encoding that you can use to change the encoding being used to write the data into these files.\nIn the example below, we are asking Spark to write a CSV file using the Latin1 encoding (ISO-8859-1).\ntransf\\\n    .write\\\n    .mode(\"overwrite\")\\\n    .option(\"encoding\", \"ISO-8859-1\")\\\n    .csv(\"transf_export\")\nIs worth mentioning that the option() method sets one option at a time. So if you need to set various write options, you just stack option() calls on top of each other. In each call, you set a different option. Like in the example below where we are setting options sep, encoding and header:\ntransf\\\n    .write\\\n    .mode(\"overwrite\")\\\n    .option(\"sep\", \";\")\\\n    .option(\"encoding\", \"UTF-8\")\\\n    .option(\"header\", True)\\\n    .csv(\"transf_export\")\nIf you want to see the full list of options for each write engine, the documentation of Spark have a table with the complete list of options available at each write engine2.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#sec-export-partition-coalesce",
    "href": "Chapters/07-export.html#sec-export-partition-coalesce",
    "title": "9  Exporting data out of Spark",
    "section": "9.3 Number of partitions determines the number of files generated",
    "text": "9.3 Number of partitions determines the number of files generated\nAs I explained at Section 3.2, every DataFrame that exists in Spark is a distributed DataFrame, meaning that this DataFrame is divided into multiple pieces (that we call partitions), and these pieces are spread across the nodes in the Spark cluster.\nIn other words, each machine that is present in the Spark cluster, contains some partitions (or some pieces) of the total DataFrame. But why we are discussing partitions here? Is because the number of partitions of your DataFrame determines the number of files written by Spark when you export the data using the write method.\nOn the previous examples across Section 9.2, when we exported the transf DataFrame into CSV files, only one single CSV file was generated inside the transf_exported folder. That is because the transf DataFrame have only one single partition, as the code below demonstrates:\ntransf.rdd.getNumPartitions()\n1\nThat means that all the data from transf DataFrame is concentrated into a single partition. Having that in mind, we could say that Spark decided in this specific case to not actually distribute the data of transf. Because all of its data is concentrated into one single place.\nBut what would happen if the transf DataFrame was splitted across 5 different partitions? What would happen then? In that case, if the transf DataFrame had 5 different partitions, and I ran the command transf.write.csv(\"transf_export\") to export its data into CSV files, then, 5 different CSV files would be written by Spark inside the folder transf_export. One CSV file for each existing partition of the DataFrame.\nThe same goes for any other file format, or any write engine that you might use in Spark. Each file generated by the write process contains the data from a specific partition of the DataFrame.\n\n9.3.1 Avoid exporting too much data into a single file\nSpark will always try to organize your DataFrame into a partition distribution that yields the best performance in any data processing. Usually in production environments, we have huge amounts of data, and a single partition distribution is rarely the case that yields the best performance in these environments.\nThat is why most existing Spark DataFrames in production environments are splitted into multiple partitions across the Spark cluster. This means that Spark DataFrames that are by default concentrated into one single partition (like the transf DataFrame in the examples of this book) are very, very rare to find in the production environments.\nAs a consequence, if you really need to export your data into a single static file in a production environment, you will likely need to:\n\nrepartition your Spark DataFrame. That is, to reorganize the partitions of this DataFrame, so that all of its data get concentrated into a single partition.\nor you continue with the write process anyway, and then later, after the write process is finished, you merge all of the generated files together with some other tool, like pandas, or polars, or the tidyverse.\n\nThe option 2 above is a little out of the scope of this book, so I will not explain it further here. But if you really need to export all the data from your Spark DataFrame into a single static file (whatever is the file format you choose), and you choose to follow option 1, then, you need to perform a repartition operation to concentrate all data from your Spark DataFrame into a single partition.\nIs worth mentioning that I strongly advise against this option 1. Because option 1 may cause some serious bottlenecks in your data pipeline, depending specially on the size of the DataFrame you are trying to export.\nIn more details, when you do not perform any repartition operation, that is, when you just write your DataFrame as is, without touching in the existing partitions, then, the write process is a narrow transformation, as I explained at Section 5.3. Because each partition is exported into a single and separate file that is independent from the others.\nThis is really important, because narrow transformations are much more predictable and are more easily scaled than wide transformations. As a result, Spark tends to scale and perform better when dealing with narrow transformations.\nHowever, when you do perform a repartition operation to concentrate all the data into a single partition, then, three things happen:\n\nthe write process becomes a wide transformation, because all partitions needs to be merged together, and as a consequence, all nodes in the cluster needs to send their data to a single place (which is usually the driver node of the cluster).\na high amount of partition shuffles can happen inside the cluster, and if they do happen, then, depending on the amount of data that needs to be “shuffled” accross the cluster, this may cause some serious slowdown in the processing.\ndepending on the size of all partitions merged together, the risks for an “out of memory” error to be raised during the process scales rapidly.\n\nSo you should be aware of these risks above, and always try to avoid using the option 1. Actually, you should avoid as much as possible the need to write all the data into a single static file! Is best for you to just write the data using the default number of partitions that Spark choose for your DataFrame.\nBut anyway, if you really cannot avoid this need, and if you have, for example, a sales DataFrame you want to export, and this DataFrame contains 4 partitions:\nsales.rdd.getNumPartitions()\n4\nAnd you want to perform a repartition operation over this DataFrame to export its data into a single static file, you can do so by using the coalesce() DataFrame method. Just provide the number 1 to this method, and all of the partitions will be reorganized into a single partition:\nsales\\\n    .coalesce(1)\\\n    .rdd\\\n    .getNumPartitions()\n1\nHaving that in mind, the entire source code to export the DataFrame into a single static file would be something like this:\nsales\\\n    .coalesce(1)\\\n    .write\\\n    .mode(\"overwrite\")\\\n    .csv(\"sales_export\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#transforming-to-a-pandas-dataframe-as-a-way-to-export-data",
    "href": "Chapters/07-export.html#transforming-to-a-pandas-dataframe-as-a-way-to-export-data",
    "title": "9  Exporting data out of Spark",
    "section": "9.4 Transforming to a Pandas DataFrame as a way to export data",
    "text": "9.4 Transforming to a Pandas DataFrame as a way to export data\nIn case you don’t know about this, Spark offers an API that you can use to quickly convert your Spark DataFrames into a pandas DataFrame. This might be extremely useful for a number of reasons:\n\nyour colleague might be much more familiar with pandas, and work more productively with it than pyspark.\nyou might need to feed this data into an existing data pipeline that uses pandas extensively.\nwith pandas you can easily export this data into Excel files (.xlsx)3, which are not easily available in Spark.\n\nTo convert an existing Spark DataFrame into a pandas DataFrame, all you need to do is to call the toPandas() method of your Spark DataFrame, and you will get a pandas DataFrame as output, like in the example below:\n\nas_pandas_df = transf.toPandas()\ntype(as_pandas_df)\n\npandas.core.frame.DataFrame\n\n\nBut you should be careful with this method, because when you transform your Spark DataFrame into a pandas DataFrame you eliminate the distributed aspect of it. As a result, all the data from your DataFrame needs to be loaded into a single place (which is usually the driver’s memory).\nBecause of that, using this toPandas() method might cause very similar issues as the ones discussed at Section 9.3. In other words, you might face the same slowdowns caused by doing a repartition to concentrate all the data into a single partition.\nSo, as the Spark documentation itself suggests, you should use this toPandas() method only if you know that your DataFrame is small enough to fit into the driver’s memory.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#the-collect-method-as-a-way-to-export-data",
    "href": "Chapters/07-export.html#the-collect-method-as-a-way-to-export-data",
    "title": "9  Exporting data out of Spark",
    "section": "9.5 The collect() method as a way to export data",
    "text": "9.5 The collect() method as a way to export data\nThe collect() DataFrame method exports the DataFrame’s data from Spark into a Python native object, more specifically, into a normal Python list. To some extent, this is a viable way to export data from Spark.\nBecause by making this data from Spark available as a normal/standard Python object, many new possibilities become open for us. Such as:\n\nsending this data to another location via HTTP requests using the request Python package.\nsending this data by email using the email built-in Python package.\nsending this data by SFTP protocol with the paramiko Python package.\nsending this data to a cloud storage, such as Amazon S3 (using the boto3 Python package).\n\nBy having the DataFrame’s data easily available to Python as a Python list, we can do virtually anything with this data. We can use this data in basically anything that Python is capable of doing.\nJust as a simple example, if I needed to send the transf data to an fictitious endpoint using a POST HTTP request, the source code would probably be something similar to this:\nimport requests\n\ndataframe_rows = transf.collect()\n\nurl = 'https://example.com/api/v1/transf'\nfor row in dataframe_rows:\n    row_as_dict = row.asDict()\n    requests.post(url, data = row_as_dict)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/07-export.html#footnotes",
    "href": "Chapters/07-export.html#footnotes",
    "title": "9  Exporting data out of Spark",
    "section": "",
    "text": "https://www.geeksforgeeks.org/tree-command-unixlinux/↩︎\nhttps://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option.↩︎\nActually, there is a Spark plugin available that is capable of exporting data from Spark directly into Excel files. But you need to install this plugin separately, since it does not come with Spark from the factory: https://github.com/crealytics/spark-excel.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exporting data out of Spark</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#the-logs-dataframe",
    "href": "Chapters/09-strings.html#the-logs-dataframe",
    "title": "10  Tools for string manipulation",
    "section": "10.1 The logs DataFrame",
    "text": "10.1 The logs DataFrame\nOver the next examples in this chapter, we will use the logs DataFrame, which contains various log messages registered at a fictitious IP adress. The data that represents this DataFrame is freely available trough the logs.json file, which you can download from the official repository of this book1.\nEach line of this JSON file contains a message that was recorded by the logger of a fictitious system. Each log message have three main parts, which are: 1) the type of message (warning - WARN, information - INFO, error - ERROR); 2) timestamp of the event; 3) the content of the message. In the example below, we have an example of message:\n\n[INFO]: 2022-09-05 03:35:01.43 Looking for workers at South America region;\n\nTo import logs.json file into a Spark DataFrame, I can use the following code:\n\npath = './../Data/logs.json'\nlogs = spark.read.json(path)\nn_truncate = 50\nlogs.show(5, truncate = n_truncate)\n\n+--------------+--------------------------------------------------+\n|            ip|                                           message|\n+--------------+--------------------------------------------------+\n|  1.0.104.27  |[INFO]: 2022-09-05 03:35:01.43 Looking for work...|\n|  1.0.104.27  |[WARN]: 2022-09-05 03:35:58.007 Workers are una...|\n|  1.0.104.27  |[INFO]: 2022-09-05 03:40:59.054 Looking for wor...|\n|  1.0.104.27  |[INFO]: 2022-09-05 03:42:24 3 Workers were acqu...|\n|  1.0.104.27  |[INFO]: 2022-09-05 03:42:37 Initializing instan...|\n+--------------+--------------------------------------------------+\nonly showing top 5 rows\n\n\n\nBy default, when we use the show() action to see the contents of our Spark DataFrame, Spark will always truncate (or cut) any value in the DataFrame that is more than 20 characters long. Since the logs messages in the logs.json file are usually much longer than 20 characters, I am using the truncate argument of show() in the example above, to avoid this behaviour.\nBy setting this argument to 50, I am asking Spark to truncate (or cut) values at the 50th character (instead of the 20th). By doing this, you (reader) can actually see a much more significant part of the logs messages in the result above.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#changing-the-case-of-letters-in-a-string",
    "href": "Chapters/09-strings.html#changing-the-case-of-letters-in-a-string",
    "title": "10  Tools for string manipulation",
    "section": "10.2 Changing the case of letters in a string",
    "text": "10.2 Changing the case of letters in a string\nProbably the most basic string transformation that exists is to change the case of the letters (or characters) that compose the string. That is, to raise specific letters to upper-case, or reduce them to lower-case, and vice-versa.\nAs a first example, lets go back to the logs DataFrame, and try to change all messages in this DataFrame to lower case, upper case and title case, by using the lower(), upper(), and initcap() functions from the pyspark.sql.functions module.\n\nfrom pyspark.sql.functions import (\n    lower,\n    upper,\n    initcap\n)\n\nm = logs.select('message')\n# Change to lower case:\nm.withColumn('message', lower('message'))\\\n    .show(5, truncate = n_truncate)\n\n+--------------------------------------------------+\n|                                           message|\n+--------------------------------------------------+\n|[info]: 2022-09-05 03:35:01.43 looking for work...|\n|[warn]: 2022-09-05 03:35:58.007 workers are una...|\n|[info]: 2022-09-05 03:40:59.054 looking for wor...|\n|[info]: 2022-09-05 03:42:24 3 workers were acqu...|\n|[info]: 2022-09-05 03:42:37 initializing instan...|\n+--------------------------------------------------+\nonly showing top 5 rows\n\n\n\n\n# Change to upper case:\nm.withColumn('message', upper('message'))\\\n    .show(5, truncate = n_truncate)\n\n+--------------------------------------------------+\n|                                           message|\n+--------------------------------------------------+\n|[INFO]: 2022-09-05 03:35:01.43 LOOKING FOR WORK...|\n|[WARN]: 2022-09-05 03:35:58.007 WORKERS ARE UNA...|\n|[INFO]: 2022-09-05 03:40:59.054 LOOKING FOR WOR...|\n|[INFO]: 2022-09-05 03:42:24 3 WORKERS WERE ACQU...|\n|[INFO]: 2022-09-05 03:42:37 INITIALIZING INSTAN...|\n+--------------------------------------------------+\nonly showing top 5 rows\n\n\n\n\n# Change to title case\n# (first letter of each word is upper case):\nm.withColumn('message', initcap('message'))\\\n    .show(5, truncate = n_truncate)\n\n+--------------------------------------------------+\n|                                           message|\n+--------------------------------------------------+\n|[info]: 2022-09-05 03:35:01.43 Looking For Work...|\n|[warn]: 2022-09-05 03:35:58.007 Workers Are Una...|\n|[info]: 2022-09-05 03:40:59.054 Looking For Wor...|\n|[info]: 2022-09-05 03:42:24 3 Workers Were Acqu...|\n|[info]: 2022-09-05 03:42:37 Initializing Instan...|\n+--------------------------------------------------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#calculating-string-length",
    "href": "Chapters/09-strings.html#calculating-string-length",
    "title": "10  Tools for string manipulation",
    "section": "10.3 Calculating string length",
    "text": "10.3 Calculating string length\nIn Spark, you can use the length() function to get the length (i.e. the number of characters) of a string. In the example below, we can see that the first log message is 74 characters long, while the second log message have 112 characters.\n\nfrom pyspark.sql.functions import length\nlogs\\\n    .withColumn('length', length('message'))\\\n    .show(5)\n\n+--------------+--------------------+------+\n|            ip|             message|length|\n+--------------+--------------------+------+\n|  1.0.104.27  |[INFO]: 2022-09-0...|    74|\n|  1.0.104.27  |[WARN]: 2022-09-0...|   112|\n|  1.0.104.27  |[INFO]: 2022-09-0...|    75|\n|  1.0.104.27  |[INFO]: 2022-09-0...|    94|\n|  1.0.104.27  |[INFO]: 2022-09-0...|    65|\n+--------------+--------------------+------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#trimming-or-removing-spaces-from-strings",
    "href": "Chapters/09-strings.html#trimming-or-removing-spaces-from-strings",
    "title": "10  Tools for string manipulation",
    "section": "10.4 Trimming or removing spaces from strings",
    "text": "10.4 Trimming or removing spaces from strings\nThe process of removing unnecessary spaces from strings is usually called “trimming”. In Spark, we have three functions that do this process, which are:\n\ntrim(): removes spaces from both sides of the string;\nltrim(): removes spaces from the left side of the string;\nrtrim(): removes spaces from the right side of the string;\n\n\nfrom pyspark.sql.functions import (\n    trim, rtrim, ltrim\n)\n\nlogs\\\n    .select('ip')\\\n    .withColumn('ip_trim', trim('ip'))\\\n    .withColumn('ip_ltrim', ltrim('ip'))\\\n    .withColumn('ip_rtrim', rtrim('ip'))\\\n    .show(5)\n\n+--------------+----------+------------+------------+\n|            ip|   ip_trim|    ip_ltrim|    ip_rtrim|\n+--------------+----------+------------+------------+\n|  1.0.104.27  |1.0.104.27|1.0.104.27  |  1.0.104.27|\n|  1.0.104.27  |1.0.104.27|1.0.104.27  |  1.0.104.27|\n|  1.0.104.27  |1.0.104.27|1.0.104.27  |  1.0.104.27|\n|  1.0.104.27  |1.0.104.27|1.0.104.27  |  1.0.104.27|\n|  1.0.104.27  |1.0.104.27|1.0.104.27  |  1.0.104.27|\n+--------------+----------+------------+------------+\nonly showing top 5 rows\n\n\n\nFor the most part, I tend to remove these unnecessary strings when I want to: 1) tidy the values; 2) avoid weird and confusing mistakes in filters on my DataFrame. The second case is worth describing in more details.\nLet’s suppose you wanted to filter all rows from the logs DataFrame where ip is equal to the 1.0.104.27 IP adress. However, you can see in the result above, that I get nothing. Not a single row of result.\n\nfrom pyspark.sql.functions import col\nlogs.filter(col('ip') == \"1.0.104.27\")\\\n    .show(5)\n\n+---+-------+\n| ip|message|\n+---+-------+\n+---+-------+\n\n\n\nBut if you see the result of the previous example (where we appliead the three versions of “trim functions”), you know that this IP adress 1.0.104.27 exists in the DataFrame. You know that the filter above should find values for this IP adress. So why it did not find any rows?\nThe answer is these annoying (and hidden) spaces on both sides of the values from the ip column. If we remove these unnecessary spaces from the values of the ip column, we suddenly find the rows that we were looking for.\n\nlogs.filter(trim(col('ip')) == \"1.0.104.27\")\\\n    .show(5)\n\n+--------------+--------------------+\n|            ip|             message|\n+--------------+--------------------+\n|  1.0.104.27  |[INFO]: 2022-09-0...|\n|  1.0.104.27  |[WARN]: 2022-09-0...|\n|  1.0.104.27  |[INFO]: 2022-09-0...|\n|  1.0.104.27  |[INFO]: 2022-09-0...|\n|  1.0.104.27  |[INFO]: 2022-09-0...|\n+--------------+--------------------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#extracting-substrings",
    "href": "Chapters/09-strings.html#extracting-substrings",
    "title": "10  Tools for string manipulation",
    "section": "10.5 Extracting substrings",
    "text": "10.5 Extracting substrings\nThere are five main functions that we can use in order to extract substrings of a string, which are:\n\nsubstring() and substr(): extract a single substring based on a start position and the length (number of characters) of the collected substring2;\nsubstring_index(): extract a single substring based on a delimiter character3;\nsplit(): extract one or multiple substrings based on a delimiter character;\nregexp_extract(): extracts substrings from a given string that match a specified regular expression pattern;\n\nYou can obviously extract a substring that matches a particular regex (regular expression) as well, by using the regexp_extract() function. However, I will describe this function, and the regex functionality available in pyspark at Section 10.7, or, more specifically, at Section 10.7.5. For now, just understand that you can also use regex to extract substrings from your text data.\n\n10.5.1 A substring based on a start position and length\nThe substring() and substr() functions they both work the same way. However, they come from different places. The substring() function comes from the spark.sql.functions module, while the substr() function is actually a method from the Column class.\nOne interesting aspect of these functions, is that they both use a one-based index, instead of a zero-based index. This means that the first character in the full string is identified by the index 1, instead of the index 0.\nThe first argument in both function is the index that identifies the start position of the substring. If you set this argument to, let’s say, 4, it means that the substring you want to extract starts at the 4th character in the input string.\nThe second argument is the amount of characters in the substring, or, in other words, it’s length. For example, if you set this argument to 10, it means that the function will extract the substring that is formed by walking \\(10 - 1 = 9\\) characters ahead from the start position you specified at the first argument. We can also interpret this as: the function will walk ahead on the string, from the start position, until it gets a substring that is 10 characters long.\nIn the example below, we are extracting the substring that starts at the second character (index 2) and ends at the sixth character (index 6) in the string.\n\nfrom pyspark.sql.functions import col, substring\n# `df1` and `df2` are equal, because\n# they both mean the same thing\ndf1 = (logs\n    .withColumn('sub', col('message').substr(2, 5))\n)\n\ndf2 = (logs\n    .withColumn('sub', substring('message', 2, 5))\n)\n\ndf2.show(5)\n\n+--------------+--------------------+-----+\n|            ip|             message|  sub|\n+--------------+--------------------+-----+\n|  1.0.104.27  |[INFO]: 2022-09-0...|INFO]|\n|  1.0.104.27  |[WARN]: 2022-09-0...|WARN]|\n|  1.0.104.27  |[INFO]: 2022-09-0...|INFO]|\n|  1.0.104.27  |[INFO]: 2022-09-0...|INFO]|\n|  1.0.104.27  |[INFO]: 2022-09-0...|INFO]|\n+--------------+--------------------+-----+\nonly showing top 5 rows\n\n\n\nJust to be very clear on how substring() and substr() both works. The Figure 10.1 illustrates the result of the above code.\n\n\n\n\n\n\nFigure 10.1: How substring() and substr() works\n\n\n\n\n\n10.5.2 A substring based on a delimiter\nThe substring_index() function works very differently. It collects the substring formed between the start of the string, and the nth occurrence of a particular character.\nFor example, if you ask substring_index() to search for the 3rd occurrence of the character $ in your string, the function will return to you the substring formed by all characters that are between the start of the string until the 3rd occurrence of this character $.\nYou can also ask substring_index() to read backwards. That is, to start the search on the end of the string, and move backwards in the string until it gets to the 3rd occurrence of this character $.\nAs an example, let’s look at the 10th log message present in the logs DataFrame. I used the collect() DataFrame method to collect this message into a raw python string, so we can easily see the full content of the message.\n\nfrom pyspark.sql.functions import monotonically_increasing_id\n\nmes_10th = (\n    logs\n    .withColumn(\n        'row_id',\n        monotonically_increasing_id()\n    )\n    .where(col('row_id') == 9)\n)\n\nmessage = mes_10th.collect()[0]['message']\nprint(message)\n\n[INFO]: 2022-09-05 04:02:09.05 Libraries installed: pandas, flask, numpy, spark_map, pyspark\n\n\nWe can see that this log message is listing a set of libraries that were installed somewhere. Suppose you want to collect the first and the last libraries in this list. How would you do it?\nA good start is to isolate the list of libraries from the rest of the message. In other words, there is a bunch of characters in the start of the log message, that we do not care about. So let’s get rid of them.\nIf you look closely to the message, you can see that the character : appears twice whithin the message. One close to the start of the string, and another time right before the start of the list of the libraries. We can use this character as our first delimiter, to collect the third substring that it creates within the total string, which is the substring that contains the list of libraries.\nThis first stage is presented visually at Figure 10.2.\n\n\n\n\n\n\nFigure 10.2: Substrings produced by the : delimiter character\n\n\n\nNow that we identified the substrings produced by the “delimiter character”, we just need to understand better which index we need to use in substring_index() to get this third substring that we want. The Figure 10.3 presents in a visual manner how the count system of substring_index() works.\nWhen you use a positive index, substring_index() will count the occurrences of the delimiter character from left to right. But, when you use a negative index, the opposite happens. That is, substring_index() counts the occurrences of the delimiter character from right to left.\n\n\n\n\n\n\nFigure 10.3: The count system of substring_index()\n\n\n\nThe index 1 represents the first substring that is before the the 1st occurence of the delimiter ([INFO]). The index 2 represents everything that is before the 2nd occurence of the delimiter ([INFO]: 2022-09-05 04:02.09.05 Libraries installed). etc.\nIn contrast, the index -1 represents everything that is after the 1st occurence of the delimiter, couting from right to left (pandas, flask, numpy, spark_map, pyspark). The index -2 represents everything that is after the 2nd occurence of the delimiter (2022-09-05 04:02.09.05 Libraries installed: pandas, flask, numpy, spark_map, pyspark). Again, couting from right to left.\nHaving all these informations in mind, we can conclude that the following code fit our first objective. Note that I applied the trim() function over the result of substring_index(), to ensure that the result substring does not contain any unnecessary spaces at both ends.\n\nfrom pyspark.sql.functions import substring_index\nmes_10th = mes_10th\\\n    .withColumn(\n        'list_of_libraries',\n        trim(substring_index('message', ':', -1))\n    )\n\nmes_10th.select('list_of_libraries')\\\n    .show(truncate = n_truncate)\n\n+----------------------------------------+\n|                       list_of_libraries|\n+----------------------------------------+\n|pandas, flask, numpy, spark_map, pyspark|\n+----------------------------------------+\n\n\n\n\n\n10.5.3 Forming an array of substrings\nNow is a good time to introduce the split() function, because we can use it to extract the first and the last library from the list libraries of stored at the mes_10th DataFrame.\nBasically, this function also uses a delimiter character to cut the total string into multiple pieces. However, this function stores these multiple pieces (or multiple substrings) into an array of substrings. With this strategy, we can now access each substring (or each piece of the total string) individually.\nIf we look again at the string that we stored at the list_of_libraries column, we have a list of libraries, separated by a comma.\n\nmes_10th\\\n    .select('list_of_libraries')\\\n    .show(truncate = n_truncate)\n\n+----------------------------------------+\n|                       list_of_libraries|\n+----------------------------------------+\n|pandas, flask, numpy, spark_map, pyspark|\n+----------------------------------------+\n\n\n\nThe comma character (,) plays an important role in this string, by separating each value in the list. And we can use this comma character as the delimiter inside split(), to get an array of substrings. Each element of this array is one of the many libraries in the list. The Figure 10.4 presents this process visually.\n\n\n\n\n\n\nFigure 10.4: Building an array of substrings with split()\n\n\n\nThe code to make this process is very straightforward. In the example below, the column array_of_libraries becomes a column of data type ArrayType(StringType), that is, an array of string values.\n\nfrom pyspark.sql.functions import split\nmes_10th = mes_10th\\\n    .withColumn(\n        'array_of_libraries',\n        split('list_of_libraries', ', ')\n    )\n\nmes_10th\\\n    .select('array_of_libraries')\\\n    .show(truncate = n_truncate)\n\n+------------------------------------------+\n|                        array_of_libraries|\n+------------------------------------------+\n|[pandas, flask, numpy, spark_map, pyspark]|\n+------------------------------------------+\n\n\n\nBy having this array of substring, we can very easily select a specific element in this array, by using the getItem() column method, or, by using the open brackets as you would normally use to select an element in a python list.\nYou just need to give the index of the element you want to select, like in the example below that we select the first and the fifth libraries in the array.\n\nmes_10th\\\n    .withColumn('lib_1', col('array_of_libraries')[0])\\\n    .withColumn('lib_5', col('array_of_libraries').getItem(4))\\\n    .select('lib_1', 'lib_5')\\\n    .show(truncate = n_truncate)\n\n+------+-------+\n| lib_1|  lib_5|\n+------+-------+\n|pandas|pyspark|\n+------+-------+",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#concatenating-multiple-strings-together",
    "href": "Chapters/09-strings.html#concatenating-multiple-strings-together",
    "title": "10  Tools for string manipulation",
    "section": "10.6 Concatenating multiple strings together",
    "text": "10.6 Concatenating multiple strings together\nSometimes, we need to concatenate multiple strings together, to form a single and longer string. To do this process, Spark offers two main functions, which are: concat() and concat_ws(). Both of these functions receives a list of columns as input, and will perform the same task, which is to concatenate the values of each column in the list, sequentially.\nHowever, the concat_ws() function have an extra argument called sep, where you can define a string to be used as the separator (or the “delimiter”) between the values of each column in the list. In some way, this sep argument and the concat_ws() function works very similarly to the join() string method of python4.\nLet’s comeback to the penguins DataFrame to demonstrate the use of these functions:\n\npath = \"../Data/penguins.csv\"\npenguins = spark.read\\\n    .csv(path, header = True)\n\npenguins.select('species', 'island', 'sex')\\\n    .show(5)\n\n+-------+---------+------+\n|species|   island|   sex|\n+-------+---------+------+\n| Adelie|Torgersen|  male|\n| Adelie|Torgersen|female|\n| Adelie|Torgersen|female|\n| Adelie|Torgersen|  NULL|\n| Adelie|Torgersen|female|\n+-------+---------+------+\nonly showing top 5 rows\n\n\n\nSuppose you wanted to concatenate the values of the columns species, island and sex together, and, store these new values on a separate column. All you need to do is to list these columns inside the concat() or concat_ws() function.\nIf you look at the example below, you can see that I also used the lit() function to add a underline character (_) between the values of each column. This is more verbose, because if you needed to concatenate 10 columns together, and still add a “delimiter character” (like the underline) between the values of each column, you would have to write lit('_') for 9 times on the list.\nIn contrast, the concat_ws() offers a much more succinct way of expressing this same operation. Because the first argument of concat_ws() is the character to be used as the delimiter between each column, and, after that, we have the list of columns to be concatenated.\n\nfrom pyspark.sql.functions import (\n    concat,\n    concat_ws,\n    lit\n)\n\npenguins\\\n    .withColumn(\n        'using_concat',\n        concat(\n            'species', lit('_'), 'island',\n            lit('_'), 'sex')\n    )\\\n    .withColumn(\n        'using_concat_ws',\n        concat_ws(\n            '_', # The delimiter character\n            'species', 'island', 'sex' # The list of columns\n        )\n    )\\\n    .select('using_concat', 'using_concat_ws')\\\n    .show(5, truncate = n_truncate)\n\n+-----------------------+-----------------------+\n|           using_concat|        using_concat_ws|\n+-----------------------+-----------------------+\n|  Adelie_Torgersen_male|  Adelie_Torgersen_male|\n|Adelie_Torgersen_female|Adelie_Torgersen_female|\n|Adelie_Torgersen_female|Adelie_Torgersen_female|\n|                   NULL|       Adelie_Torgersen|\n|Adelie_Torgersen_female|Adelie_Torgersen_female|\n+-----------------------+-----------------------+\nonly showing top 5 rows\n\n\n\nIf you look closely to the result above, you can also see, that concat() and concat_ws() functions deal with null values in different ways. If concat() finds a null value for a particular row, in any of the listed columns to be concatenated, the end result of the process is a null value for that particular row.\nOn the other hand, concat_ws() will try to concatenate as many values as he can. If he does find a null value, he just ignores this null value and go on to the next column, until it hits the last column in the list.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#sec-regex",
    "href": "Chapters/09-strings.html#sec-regex",
    "title": "10  Tools for string manipulation",
    "section": "10.7 Introducing regular expressions",
    "text": "10.7 Introducing regular expressions\nSpark also provides some basic regex (regular expressions) functionality. Most of this functionality is available trough two functions that comes from the pyspark.sql.functions module, which are:\n\nregexp_replace(): replaces all occurrences of a specified regular expression pattern in a given string with a replacement string.;\nregexp_extract(): extracts substrings from a given string that match a specified regular expression pattern;\n\nThere is also a column method that provides an useful way of testing if the values of a column matchs a regular expression or not, which is the rlike() column method. You can use the rlike() method in conjunction with the filter() or where() DataFrame methods, to find all values that fit (or match) a particular regular expression, like we demonstrated at Section 5.5.7.2.\n\n10.7.1 The Java regular expression standard\nAt this point, is worth remembering a basic fact about Apache Spark that we introduced at Chapter 2. Apache Spark is written in Scala, which is a modern programming language deeply connected with the Java programming language. One of the many consequences from this fact, is that all regular expression functionality available in Apache Spark is based on the Java java.util.regex package.\nThis means that you should always write regular expressions on your pyspark code that follows the Java regular expression syntax, and not the Python regular expression syntax, which is based on the python module re.\nAlthough this detail is important, these two flavors of regular expressions (Python syntax versus Java syntax) are very, very similar. So, for the most part, you should not see any difference between these two syntaxes.\nIf for some reason, you need to consult the full list of all metacharacters available in the Java regular expression standard, you can always check the Java documentation for the java.util.regex package. More specifically, the documentation for the java.util.regex.Pattern class5.\nThe following list gives you a quick description of a small fraction of the available metacharacters in the Java syntax, and, as a result, metacharacters that you can use in pyspark:\n\n. : Matches any single character;\n* : Matches zero or more occurrences of the preceding character or pattern;\n+ : Matches one or more occurrences of the preceding character or pattern;\n? : Matches zero or one occurrence of the preceding character or pattern;\n| : Matches either the expression before or after the |;\n[] : Matches any single character within the brackets;\n\\d : Matches any digit character;\n\\b : Matches a word boundary character;\n\\w : Matches any word character. Equivalent to the \"\\b([a-zA-Z_0-9]+)\\b\" regular expression;\n\\s : Matches any whitespace character;\n() : Groups a series of pattern elements to a single element;\n\n\n\n10.7.2 Using an invalid regular expression\nWhen you write an invalid regular expression in your code, Spark usually complains with a java.util.regex.PatternSyntaxException runtime error. The code presented below is an example of code that produces such error.\nIn this example, the regular expression \\b([a-z] is invalid because it is missing a closing parenthesis. If you try to execute this code, Spark will raise a with the message “Unclosed group near index 7”. This error message indicates that there is a syntax error in the regular expression, due to an unclosed group (i.e., a missing closing parenthesis).\nfrom pyspark.sql.functions import col\nweird_regex = '\\b([a-z]'\nlogs\\\n    .filter(col('message').rlike(weird_regex))\\\n    .show(5)\nPy4JJavaError: An error occurred while calling o261.showString.\n:   java.util.regex.PatternSyntaxException: Unclosed group near index 7\n([a-z]\nTo avoid these runtime errors, due to invalid regular expressions, is always a good idea to test your regular expressions, before you use them in your pyspark code. You can easily test your regular expressions by using online tools, such as the Regex101 website6.\n\n\n10.7.3 Replacing occurrences of a particular regular expression with regexp_replace()\nOne of the most essential actions with regular expression is to find text that fits into a particular regular expression, and, rewriting this text into a different format, or, even removing it completely from the string.\nThe regexp_replace() function (from the pyspark.sql.functions module) is the function that allows you to perform this kind of operation on string values of a column in a Spark DataFrame.\nThis function replaces all occurrences of a specified regular expression pattern in a given string with a replacement string, and it takes three different arguments:\n\nThe input column name or expression that contains the string values to be modified;\nThe regular expression pattern to search for within the input string values;\nThe replacement string that will replace all occurrences of the matched pattern in the input string values;\n\nAs an example, lets suppose we want to remove completely the type of the message in all log messages present in the logs DataFrame. To that, we first need to get a regular expression capable of identifying all possibilities for these types.\nA potential candidate would be the regular expression '\\\\[(INFO|ERROR|WARN)\\\\]: ', so lets give it a shot. Since we are trying to remove this particular part from all log messages, we should replace this part of the string by an empty string (''), like in the example below:\n\nfrom pyspark.sql.functions import regexp_replace\n\ntype_regex = '\\\\[(INFO|ERROR|WARN)\\\\]: '\n\nlogs\\\n    .withColumn(\n        'without_type',\n        regexp_replace('message', type_regex, '')\n    )\\\n    .select('message', 'without_type')\\\n    .show(truncate = 30)\n\n+------------------------------+------------------------------+\n|                       message|                  without_type|\n+------------------------------+------------------------------+\n|[INFO]: 2022-09-05 03:35:01...|2022-09-05 03:35:01.43 Look...|\n|[WARN]: 2022-09-05 03:35:58...|2022-09-05 03:35:58.007 Wor...|\n|[INFO]: 2022-09-05 03:40:59...|2022-09-05 03:40:59.054 Loo...|\n|[INFO]: 2022-09-05 03:42:24...|2022-09-05 03:42:24 3 Worke...|\n|[INFO]: 2022-09-05 03:42:37...|2022-09-05 03:42:37 Initial...|\n|[WARN]: 2022-09-05 03:52:02...|2022-09-05 03:52:02.98 Libr...|\n|[INFO]: 2022-09-05 04:00:33...|2022-09-05 04:00:33.210 Lib...|\n|[INFO]: 2022-09-05 04:01:15...|2022-09-05 04:01:15 All clu...|\n|[INFO]: 2022-09-05 04:01:35...|2022-09-05 04:01:35.022 Mak...|\n|[INFO]: 2022-09-05 04:02:09...|2022-09-05 04:02:09.05 Libr...|\n|[INFO]: 2022-09-05 04:02:09...|2022-09-05 04:02:09.05 The ...|\n|[INFO]: 2022-09-05 04:02:09...|2022-09-05 04:02:09.05 An e...|\n|[ERROR]: 2022-09-05 04:02:1...|2022-09-05 04:02:12 A task ...|\n|[ERROR]: 2022-09-05 04:02:3...|2022-09-05 04:02:34.111 Err...|\n|[ERROR]: 2022-09-05 04:02:3...|2022-09-05 04:02:34.678 Tra...|\n|[ERROR]: 2022-09-05 04:02:3...|2022-09-05 04:02:35.14 Quit...|\n+------------------------------+------------------------------+\n\n\n\nIs useful to remind that this regexp_replace() function searches for all occurrences of the regular expression on the input string values, and replaces all of these occurrences by the input replacement string that you gave. However, if the function does not find any matchs for your regular expression inside a particular value in the column, then, the function simply returns this value intact.\n\n\n10.7.4 Introducing capturing groups on pyspark\nOne of the many awesome functionalities of regular expressions, is the capability of enclosing parts of a regular expression inside groups, and actually store (or cache) the substring matched by this group. This process of grouping parts of a regular expression inside a group, and capturing substrings with them, is usually called of “grouping and capturing”.\nIs worth pointing out that this capturing groups functionality is available both in regexp_replace() and regexp_extract().\n\n10.7.4.1 What is a capturing group ?\nOk, but, what is this group thing? You create a group inside a regular expression by enclosing a particular section of your regular expression inside a pair of parentheses. The regular expression that is written inside this pair of of parentheses represents a capturing group.\nA capturing group inside a regular expression is used to capture a specific part of the matched string. This means that the actual part of the input string that is matched by the regular expression that is inside this pair of parentheses, is captured (or cached, or saved) by the group, and, can be reused later.\n\nBesides grouping part of a regular expression together, parentheses also create a numbered capturing group. It stores the part of the string matched by the part of the regular expression inside the parentheses. …. The regex “Set(Value)?” matches “Set” or “SetValue”. In the first case, the first (and only) capturing group remains empty. In the second case, the first capturing group matches “Value”. (Goyvaerts 2023).\n\nSo, remember, to use capturing groups in a regular expression, you must enclose the part of the pattern that you want to capture in parentheses (). Each set of parentheses creates a new capturing group. This means that you can create multiple groups inside a single regular expression, and, then, reuse latter the substrings captured by all of these multiple groups. Awesome, right?\nEach new group (that is, each pair of parentheses) that you create in your regular expression have a different index. That means that the first group is identified by the index 1, the second group, by the index 2, the third group, by the index 3, etc.\n\nJust to quickly demonstrate these capturing groups, here is a quick example, in pure Python:\n\nimport re\n\n# A regular expression that contains\n# three different capturing groups\nregex = r\"(\\d{3})-(\\d{2})-(\\d{4})\"\n\n# Match the regular expression against a string\ntext = \"My social security number is 123-45-6789.\"\nmatch = re.search(regex, text)\n\n# Access the captured groups\ngroup1 = match.group(1)  # \"123\"\ngroup2 = match.group(2)  # \"45\"\ngroup3 = match.group(3)  # \"6789\"\n\nIn the above example, the regular expression r\"(\\d{3})-(\\d{2})-(\\d{4})\" contains three capturing groups, each enclosed in parentheses. When the regular expression is matched against the string \"My social security number is 123-45-6789.\", the first capturing group matches the substring \"123\", the second capturing group matches \"45\", and the third capturing group matches \"6789\".\n\n\n\n\n\n\nFigure 10.5: Example of capturing groups\n\n\n\nIn Python, we can access the captured groups using the group() method of the Match object returned by re.search(). In this example, match.group(1) returns the captured substring of the first capturing group (which is \"123\"), match.group(2) returns second \"45\", and match.group(3) returns \"6789\".\n\n\n10.7.4.2 How can we use capturing groups in pyspark ?\nOk, now that we understood what capturing groups is, how can we use them in pypspark? First, remember, capturing groups will be available to you, only if you enclose a part of your regular expression in a pair of parentheses. So the first part is to make sure that the capturing groups are present in your regular expressions.\nAfter that, you can access the substring matched by the capturing group, by using the reference index that identifies this capturing group you want to use. In pure Python, we used the group() method with the group index (like 1, 2, etc.) to access these values.\nBut in pyspark, we access these groups by using a special pattern formed by the group index preceded by a dollar sign ($). That is, the text $1 references the first capturing group, $2 references the second capturing group, etc.\nAs a first example, lets go back to the regular expression we used at Section 10.7.3: \\\\[(INFO|ERROR|WARN)\\\\]:. This regular expression contains one capturing group, which captures the type label of the log message: (INFO|ERROR|WARN).\nIf we use the special pattern $1 to reference this capturing group inside of regexp_replace(), what is going to happen is: regexp_replace() will replace all occurrences of the input regular expression found on the input string, by the substring matched by the first capturing group. See in the example below:\n\nlogs\\\n    .withColumn(\n        'using_groups',\n        regexp_replace('message', type_regex, 'Type Label -&gt; $1 | ')\n    )\\\n    .select('message', 'using_groups')\\\n    .show(truncate = 30)\n\n+------------------------------+------------------------------+\n|                       message|                  using_groups|\n+------------------------------+------------------------------+\n|[INFO]: 2022-09-05 03:35:01...|Type Label -&gt; INFO | 2022-0...|\n|[WARN]: 2022-09-05 03:35:58...|Type Label -&gt; WARN | 2022-0...|\n|[INFO]: 2022-09-05 03:40:59...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 03:42:24...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 03:42:37...|Type Label -&gt; INFO | 2022-0...|\n|[WARN]: 2022-09-05 03:52:02...|Type Label -&gt; WARN | 2022-0...|\n|[INFO]: 2022-09-05 04:00:33...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 04:01:15...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 04:01:35...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 04:02:09...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 04:02:09...|Type Label -&gt; INFO | 2022-0...|\n|[INFO]: 2022-09-05 04:02:09...|Type Label -&gt; INFO | 2022-0...|\n|[ERROR]: 2022-09-05 04:02:1...|Type Label -&gt; ERROR | 2022-...|\n|[ERROR]: 2022-09-05 04:02:3...|Type Label -&gt; ERROR | 2022-...|\n|[ERROR]: 2022-09-05 04:02:3...|Type Label -&gt; ERROR | 2022-...|\n|[ERROR]: 2022-09-05 04:02:3...|Type Label -&gt; ERROR | 2022-...|\n+------------------------------+------------------------------+\n\n\n\nIn essence, you can reuse the substrings matched by the capturing groups, by using the special patterns $1, $2, $3, etc. This means that you can reuse the substrings captured by multiple groups at the same time inside regexp_replace() and regexp_extract(). For example, if we use the replacement string \"$1, $2, $3\" inside regexp_replace(), we would get the substrings matched by the first, second and third capturing groups, separated by commas.\nHowever, is also good to emphasize a small limitation that this system has. When you need to reuse the substrings captured by multiple groups together, is important that you make sure to add some amount of space (or some delimiter character) between each group reference, like \"$1 $2 $3\".\nBecause if you write these group references one close to each other (like in \"$1$2$3\"), it is not going to work. In other words, Spark will not understand that you are trying to access a capturing group. It will interpret the text \"$1$2$3\" as the literal value \"$1$2$3\", and not as a special pattern that references multiple capturing groups in the regular expression.\n\n\n\n10.7.5 Extracting substrings with regexp_extract()\nAnother very useful regular expression activity is to extract a substring from a given string that match a specified regular expression pattern. The regexp_extract() function is the main method used to do this process.\nThis function takes three arguments, which are:\n\nThe input column name or expression that contains the string to be searched;\nThe regular expression pattern to search for within the input string;\nThe index of the capturing group within the regular expression pattern that corresponds to the substring to extract;\n\nYou may (or may not) use capturing groups inside of regexp_replace(). However, on the other hand, the regexp_extract() function is based on the capturing groups functionality. As a consequence, when you use regexp_extract(), you must give a regular expression that contains some capturing group. Because otherwise, the regexp_extract() function becomes useless.\nIn other words, the regexp_extract() function extracts substrings that are matched by the capturing groups present in your input regular expression. If you want, for example, to use regexp_extract() to extract the substring matched by a entire regular expression, then, you just need to surround this entire regular expression by a pair of parentheses. This way you transform this entire regular expression in a capturing group, and, therefore, you can extract the substring matched by this group.\nAs an example, lets go back again to the regular expression we used in the logs DataFrame: \\\\[(INFO|ERROR|WARN)\\\\]:. We can extract the type of log message label, by using the index 1 to reference the first (and only) capturing group in this regular expression.\n\nfrom pyspark.sql.functions import regexp_extract\n\nlogs\\\n    .withColumn(\n        'message_type',\n        regexp_extract('message', type_regex, 1)\n    )\\\n    .select('message', 'message_type')\\\n    .show(truncate = 30)\n\n+------------------------------+------------+\n|                       message|message_type|\n+------------------------------+------------+\n|[INFO]: 2022-09-05 03:35:01...|        INFO|\n|[WARN]: 2022-09-05 03:35:58...|        WARN|\n|[INFO]: 2022-09-05 03:40:59...|        INFO|\n|[INFO]: 2022-09-05 03:42:24...|        INFO|\n|[INFO]: 2022-09-05 03:42:37...|        INFO|\n|[WARN]: 2022-09-05 03:52:02...|        WARN|\n|[INFO]: 2022-09-05 04:00:33...|        INFO|\n|[INFO]: 2022-09-05 04:01:15...|        INFO|\n|[INFO]: 2022-09-05 04:01:35...|        INFO|\n|[INFO]: 2022-09-05 04:02:09...|        INFO|\n|[INFO]: 2022-09-05 04:02:09...|        INFO|\n|[INFO]: 2022-09-05 04:02:09...|        INFO|\n|[ERROR]: 2022-09-05 04:02:1...|       ERROR|\n|[ERROR]: 2022-09-05 04:02:3...|       ERROR|\n|[ERROR]: 2022-09-05 04:02:3...|       ERROR|\n|[ERROR]: 2022-09-05 04:02:3...|       ERROR|\n+------------------------------+------------+\n\n\n\nAs another example, lets suppose we wanted to extract not only the type of the log message, but also, the timestamp and the content of the message, and store these different elements in separate columns.\nTo do that, we could build a more complete regular expression. An expression capable of matching the entire log message, and, at the same time, capture each of these different elements inside a different capturing group. The code below is an example that produces such regular expression, and applies it over the logs DataFrame.\n\ntype_regex = r'\\[(INFO|ERROR|WARN)\\]: '\n\ndate_regex = r'\\d{4}-\\d{2}-\\d{2}'\ntime_regex = r' \\d{2}:\\d{2}:\\d{2}([.]\\d+)?'\ntimestamp_regex = date_regex + time_regex\ntimestamp_regex = r'(' + timestamp_regex + r')'\n\nregex = type_regex + timestamp_regex + r'(.+)$'\n\nlogs\\\n    .withColumn(\n        'message_type',\n        regexp_extract('message', regex, 1)\n    )\\\n    .withColumn(\n        'timestamp',\n        regexp_extract('message', regex, 2)\n    )\\\n    .withColumn(\n        'message_content',\n        regexp_extract('message', regex, 4)\n    )\\\n    .select('message_type', 'timestamp', 'message_content')\\\n    .show(5, truncate = 30)\n\n+------------+-----------------------+------------------------------+\n|message_type|              timestamp|               message_content|\n+------------+-----------------------+------------------------------+\n|        INFO| 2022-09-05 03:35:01.43| Looking for workers at Sou...|\n|        WARN|2022-09-05 03:35:58.007| Workers are unavailable at...|\n|        INFO|2022-09-05 03:40:59.054| Looking for workers at Sou...|\n|        INFO|    2022-09-05 03:42:24| 3 Workers were acquired at...|\n|        INFO|    2022-09-05 03:42:37| Initializing instances in ...|\n+------------+-----------------------+------------------------------+\nonly showing top 5 rows\n\n\n\n\n\n10.7.6 Identifying values that match a particular regular expression with rlike()\nThe rlike() column method is useful for checking if a string value in a column matches a specific regular expression. We briefly introduced this method at Section 5.5.7.2. This method has only one input, which is the regular expression you want to apply over the column values.\nAs an example, lets suppose you wanted to identify timestamp values inside your strings. You could use a regular expression pattern to find which text values had these kinds of values inside them.\nA possible regular expression candidate would be \"[0-9]{2}:[0-9]{2}:[0-9]{2}([.][0-9]+)?\". This regex matches timestamp values in the format “hh:mm:ss.sss”. This pattern consists of the following building blocks, or, elements:\n\n[0-9]{2}: Matches any two digits from 0 to 9.\n:: Matches a colon character.\n([.][0-9]+)?: Matches an optional decimal point followed by one or more digits.\n\nIf we apply this pattern over all log messages stored in the logs DataFrame, we would find that all logs messages matches this particular regular expression. Because all log messages contains a timestamp value at the start of the message:\n\nfrom pyspark.sql.functions import col\n\npattern = \"[0-9]{2}:[0-9]{2}:[0-9]{2}([.][0-9]+)?\"\nlogs\\\n    .withColumn(\n        'does_it_match?',\n        col(\"message\").rlike(pattern)\n    )\\\n    .select('message', 'does_it_match?')\\\n    .show(5, truncate = n_truncate)\n\n+--------------------------------------------------+--------------+\n|                                           message|does_it_match?|\n+--------------------------------------------------+--------------+\n|[INFO]: 2022-09-05 03:35:01.43 Looking for work...|          true|\n|[WARN]: 2022-09-05 03:35:58.007 Workers are una...|          true|\n|[INFO]: 2022-09-05 03:40:59.054 Looking for wor...|          true|\n|[INFO]: 2022-09-05 03:42:24 3 Workers were acqu...|          true|\n|[INFO]: 2022-09-05 03:42:37 Initializing instan...|          true|\n+--------------------------------------------------+--------------+\nonly showing top 5 rows\n\n\n\n\n\n\n\nGoyvaerts, Jan. 2023. “Regular-Expressions.info.” https://www.regular-expressions.info/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/09-strings.html#footnotes",
    "href": "Chapters/09-strings.html#footnotes",
    "title": "10  Tools for string manipulation",
    "section": "",
    "text": "https://github.com/pedropark99/Introd-pyspark/tree/main/Data↩︎\nInstead of using a zero based index (which is the default for Python), these functions use a one based index.↩︎\nInstead of using a zero based index (which is the default for Python), these functions use a one based index.↩︎\nhttps://docs.python.org/3/library/stdtypes.html#str.join↩︎\nhttps://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html↩︎\nhttps://regex101.com/↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tools for string manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#sec-create-dates",
    "href": "Chapters/10-datetime.html#sec-create-dates",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.1 Creating date values",
    "text": "11.1 Creating date values\nDates are normally interpreted in pyspark using the DateType data type. There are three commom ways to create date objects, which are:\n\nfrom strings (like \"3 of June of 2023\", or maybe, \"2023-02-05\").\nby extracting the date component from datetime values (i.e. values of type TimestampType).\nby combining day, month and year components to build a date object.\n\n\n11.1.1 From strings\nWhen you have a StringType column in your DataFrame that contains dates that are currently being stored inside strings, and you want to convert this column into a DateType column, you basically have two choices: 1) use the automatic column conversion with cast() or astype(); 2) use the to_date() Spark SQL function to convert the strings using a specific date format.\nWhen you use the cast() (or astype()) column method that we introduced at Section 5.7.3, Spark will perform a quick and automatic conversion to DateType by casting the strings you have into the DateType. But when you use this method, Spark will always assume that the dates you have are in the ISO-8601 format, which is the international standard for dates. This format is presented at Figure 11.1:\n\n\n\n\n\n\nFigure 11.1: The ISO-8601 format for dates\n\n\n\nBasically, the ISO-8601 standard specifies that dates are represented in the format “YYYY-MM-DD” (or “Year-Month-Date”), like 2023-09-19, 1926-05-21, or 2005-11-01. This is also the format that dates are usually formatted in United States of America. So, if the dates you have (which are currently stored inside strings) are formatted like the ISO-8601 standard, then, you can safely and easily convert them into the DateType by using the cast() or astype() column methods.\nHowever, if these dates are formatted in a different way, then, the cast() method will very likely produce null values as a result, because it cannot parse dates that are outside the ISO-8601 format. If that is your case, then you should use the to_date() function, which allows you to specify the exact format of your dates.\nThere are many examples of date formats which are outside of the ISO-8601 format. Like:\n\nIn Brazil and Spain, dates are formatted as “Day/Month/Year”. Example: “23/04/2022” for April 23, 2022.\nIn Japan, dates are formatted as “year month day (weekday)”, with the Japanese characters meaning “year”, “month” and “day” inserted after the numerals. Example: 2008年12月31日 (水) for “Wednesday 31 December 2008”.\nMany websites display dates using the full name of the month, like “November 18, 2023”. This is an important fact considering that web-scrapping is a real and important area of data analysis these days.\n\nI will describe at Section 11.3 how you can use the to_date() function to convert dates that are outside of the ISO format to DateType values. But for now, for simplicity sake, I will consider only strings that contains date in the ISO format.\nAs a first example, lets consider the DataFrame df below:\n\nfrom pyspark.sql import Row\n\ndata = [\n    Row(date_registered = \"2021-01-01\"),\n    Row(date_registered = \"2021-01-01\"),\n    Row(date_registered = \"2021-01-02\"),\n    Row(date_registered = \"2021-01-03\")\n]\n\ndf = spark.createDataFrame(data)\n\nIf we look at the DataFrame schema of df, we can see that the date_registered column is currently being interpreted as a column of type StringType:\n\ndf.printSchema()\n\nroot\n |-- date_registered: string (nullable = true)\n\n\n\nSince the dates from the date_registered column are formatted like the ISO-8601 standard, we can safely use cast() to get a column of type DateType. And if we look again at the DataFrame schema after the transformation, we can certify that the date_registered column is in fact now, a column of type DateType.\n\nfrom pyspark.sql.functions import col\n\ndf = df.withColumn(\n    'date_registered',\n    col('date_registered').cast('date')\n)\n\ndf.show()\n\n[Stage 0:===========================================================(1 + 0) / 1]                                                                                \n\n\n+---------------+\n|date_registered|\n+---------------+\n|     2021-01-01|\n|     2021-01-01|\n|     2021-01-02|\n|     2021-01-03|\n+---------------+\n\n\n\n\ndf.printSchema()\n\nroot\n |-- date_registered: date (nullable = true)\n\n\n\n\n\n11.1.2 From datetime values\nA datetime value is a value that contains both a date component and a time component. But you can obviously extract just the date component from a datetime value. Let’s use the following DataFrame as example:\n\nfrom pyspark.sql import Row\nfrom datetime import datetime\n\ndata = [\n    {'as_datetime': datetime(2021, 6, 12, 10, 0, 0)},\n    {'as_datetime': datetime(2021, 6, 12, 18, 0, 0)},\n    {'as_datetime': datetime(2021, 6, 13, 7, 0, 0)},\n    {'as_datetime': datetime(2021, 6, 14, 19, 30, 0)}\n]\n\ndf = spark.createDataFrame(data)\ndf.printSchema()\n\nroot\n |-- as_datetime: timestamp (nullable = true)\n\n\n\nYou can extract the date component from the as_datetime column by directly casting the column into the DateType type. Like you would normally do with a string column.\n\ndf.withColumn('date_component', col('as_datetime').cast('date'))\\\n    .show()\n\n+-------------------+--------------+\n|        as_datetime|date_component|\n+-------------------+--------------+\n|2021-06-12 10:00:00|    2021-06-12|\n|2021-06-12 18:00:00|    2021-06-12|\n|2021-06-13 07:00:00|    2021-06-13|\n|2021-06-14 19:30:00|    2021-06-14|\n+-------------------+--------------+\n\n\n\n\n\n11.1.3 From individual components\nIf you have 3 columns in your DataFrame, one for each component of a date value (day, month, year), you can group these components together to form date values. In pyspark you do this by using the make_date() function.\nTo use this function, you just list the columns of each component in the following order: year, month and day. Like in this example:\n\nfrom pyspark.sql.functions import make_date\ndf3 = spark.createDataFrame([\n    Row(day=14,month=2,year=2021),\n    Row(day=30,month=4,year=2021),\n    Row(day=2,month=5,year=2021),\n    Row(day=6,month=5,year=2021)\n])\n\ndf3\\\n    .withColumn(\n        'as_date',\n        make_date(\n            col('year'),\n            col('month'),\n            col('day')\n        )\n    )\\\n    .show()\n\n+---+-----+----+----------+\n|day|month|year|   as_date|\n+---+-----+----+----------+\n| 14|    2|2021|2021-02-14|\n| 30|    4|2021|2021-04-30|\n|  2|    5|2021|2021-05-02|\n|  6|    5|2021|2021-05-06|\n+---+-----+----+----------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#creating-datetime-values",
    "href": "Chapters/10-datetime.html#creating-datetime-values",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.2 Creating datetime values",
    "text": "11.2 Creating datetime values\nDatetime values are values that contains both a date and a time components. These datetime values might also come with a time zone component, although this component is not mandatory.\nDifferent programming languages and frameworks might have different names for these kind of values. Because of that, you might know datetime values by a different name. For example, “timestamps” is also a very popular name for this kind of values. Anyway, in pyspark, datetime (or timestamp) values are interpreted by the TimestampType data type.\nThere are three commom ways to create datetime objects, which are:\n\nfrom strings (like \"2023-02-05 12:30:00\").\nfrom integer values (i.e. IntegerType and LongType).\nby combining the individual components of a datetime value (day, month, year, hour, minute, second, etc.) to form a complete datetime value.\n\n\n11.2.1 From strings\nAgain, if your datetime values are being currently stored inside strings, and, you want to convert them to datetime values, you can use the automatic conversion of Spark with the cast() column method. However, as we stated at Section 11.1, when you use this path, Spark will always assume that your datetime values follow the ISO-8601 format.\nThe ISO-8601 standard states that datetime values should always follow the format “YYYY-MM-DD HH:MM:SS Z” (or “Year-Month-Day Hour:Minute:Second TimeZone”). You can see at Figure 11.2, that are components that are mandatory (meaning that they always appear in a datetime value), and components that are optional (meaning that they might appear or might not in a datetime value).\nThe time zone component is always optional, and because of that, they usually are not present in the datetime values you have. There are some variations and extra characters that might appear at some points, for example, the character ‘T’ might appear to clearly separate the date from the time component, and the character ‘Z’ to separate the time zone from the time component. Also, the time component might have a microseconds section to identify a more precise point in time.\nBut despite all these variations, datetime values that are ISO-8601 compatible are basically always following this same pattern (or this same order of components) of “Year-Month-Day Hour:Minute:Second TimeZone”. Examples of values that follows this standard are “2014-10-18T16:30:00Z”, “2019-05-09 08:20:05.324” and “2020-01-01 12:30:00 -03”.\n\n\n\n\n\n\nFigure 11.2: The ISO-8601 format for datetime/timestamp values\n\n\n\nThis means that if your datetime values are not in this format, if they do not follow the ISO-8601 standard, then, you should not use the cast() method. If that is your case, you should use datetime patterns with the to_timestamp() function. We describe these assets in more depth at Section 11.3.\nFor now, let’s focus on examples that use the ISO-8601 format. As an example, lets use the df2 DataFrame below:\n\ndata = [\n    Row(datetime_as_string = \"2021-02-23T04:41:57Z\"),\n    Row(datetime_as_string = \"2021-05-18T12:30:05Z\"),\n    Row(datetime_as_string = \"2021-11-13T16:30:00Z\"),\n    Row(datetime_as_string = \"2021-08-09T00:30:16Z\")\n]\n\ndf2 = spark.createDataFrame(data)\ndf2.printSchema()\n\nroot\n |-- datetime_as_string: string (nullable = true)\n\n\n\nYou can see above, that the datetime_as_string column is currently being interpreted as a column of strings. But since the values are in ISO-8601 format, I can use the cast method to directly convert them into timestamp values.\n\ndf2 = df2\\\n    .withColumn(\n        'datetime_values',\n        col('datetime_as_string').cast('timestamp')\n    )\n\ndf2.printSchema()\n\nroot\n |-- datetime_as_string: string (nullable = true)\n |-- datetime_values: timestamp (nullable = true)\n\n\n\n\ndf2.show()\n\n+--------------------+-------------------+\n|  datetime_as_string|    datetime_values|\n+--------------------+-------------------+\n|2021-02-23T04:41:57Z|2021-02-23 01:41:57|\n|2021-05-18T12:30:05Z|2021-05-18 09:30:05|\n|2021-11-13T16:30:00Z|2021-11-13 13:30:00|\n|2021-08-09T00:30:16Z|2021-08-08 21:30:16|\n+--------------------+-------------------+\n\n\n\n\n\n11.2.2 From integers\nYou can also convert integers directly to datetime values by using the cast() method. In this situation, the integers are interpreted as being the number of seconds since the UNIX time epoch, which is mid-night of 1 January of 1970 (\"1970-01-01 00:00:00\"). In other words, the integer 60 will be converted the point in time that is 60 seconds after \"1970-01-01 00:00:00\", which would be \"1970-01-01 00:01:00\".\nIn the example below, the number 1,000,421,325 is converted into 19:48:45 of 13 September of 2001. Because this exact point in time is 1.000421 billion of seconds ahead of the UNIX epoch.\n\ndf3 = spark.createDataFrame([\n    Row(datetime_as_integer = 1000421325),\n    Row(datetime_as_integer = 1000423628),\n    Row(datetime_as_integer = 500),\n    Row(datetime_as_integer = 1000493412)\n])\n\ndf3 = df3\\\n    .withColumn(\n        'datetime_values',\n        col('datetime_as_integer').cast('timestamp')\n    )\n\ndf3.show()\n\n+-------------------+-------------------+\n|datetime_as_integer|    datetime_values|\n+-------------------+-------------------+\n|         1000421325|2001-09-13 19:48:45|\n|         1000423628|2001-09-13 20:27:08|\n|                500|1969-12-31 21:08:20|\n|         1000493412|2001-09-14 15:50:12|\n+-------------------+-------------------+\n\n\n\nHowever, you probably notice in the example above, that something is odd. Because the number 500 was converted into \"1969-12-31 21:08:20\", which is in theory, behind the UNIX epoch, which is 1 January of 1970. Why did that happen? The answer is that your time zone is always taken into account during a conversion from integers to datetime values!\nIn the example above, Spark is running on an operating system that is using the America/Sao_Paulo time zone (which is 3 hours late from international time zone - UTC-3) as the “default time zone” of the system. As a result, integers will be interpreted as being the number of seconds since the UNIX time epoch minus 3 hours, which is \"1969-12-31 21:00:00\". So, in this context, the integer 60 would be converted into \"1969-12-31 21:01:00\" (instead of the usual \"1970-01-01 00:01:00\" that you would expect).\nThat is why the number 500 was converted into \"1969-12-31 21:08:20\". Because it is 500 seconds ahead of \"1969-12-31 21:00:00\", which is 3 hours behind the UNIX time epoch.\nBut what if you wanted to convert your integers into a UTC-0 time zone? Well, you could just set the time zone of your Spark Session to the international time zone before you do the conversion, with the command below:\nspark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\nBut you could also do the conversion anyway and adjust the values later. That is, you just perform the conversion with the cast() method, even if the result would be in your current time zone. After that, you add the amount of time necessary to transpose your datetime values to the international time zone (UTC-0).\nSo in the above example, since I was using the Brasília time zone (UTC-3) during the above conversion, I just need to add 3 hours to all datetime values to get their equivalents values in international time zone. You can do that by using interval expressions, which will be discussed in more depth at Section 11.5.\n\nfrom pyspark.sql.functions import expr\ndf3 = df3\\\n    .withColumn(\n        'datetime_values_utc0',\n        expr(\"datetime_values + INTERVAL 3 HOURS\")\n    )\n\ndf3.show()\n\n+-------------------+-------------------+--------------------+\n|datetime_as_integer|    datetime_values|datetime_values_utc0|\n+-------------------+-------------------+--------------------+\n|         1000421325|2001-09-13 19:48:45| 2001-09-13 22:48:45|\n|         1000423628|2001-09-13 20:27:08| 2001-09-13 23:27:08|\n|                500|1969-12-31 21:08:20| 1970-01-01 00:08:20|\n|         1000493412|2001-09-14 15:50:12| 2001-09-14 18:50:12|\n+-------------------+-------------------+--------------------+\n\n\n\n\n\n11.2.3 From individual components\nIf you have 6 columns in your DataFrame, one for each component of a datetime value (day, month, year, hour, minute, second), you can group these components together to compose datetime values. We do this in pyspark by using the make_timestamp() function.\nTo use this function, you just list the columns of each component in the following order: year, month, day, hours, minutes, seconds. Like in this example:\n\nfrom pyspark.sql.functions import make_timestamp\ndf3 = spark.createDataFrame([\n    Row(day=14,month=2,year=2021,hour=12,mins=45,secs=0),\n    Row(day=30,month=4,year=2021,hour=8,mins=10,secs=0),\n    Row(day=2,month=5,year=2021,hour=5,mins=9,secs=12),\n    Row(day=6,month=5,year=2021,hour=0,mins=34,secs=4)\n])\n\ndf3\\\n    .withColumn(\n        'as_datetime',\n        make_timestamp(\n            col('year'),\n            col('month'),\n            col('day'),\n            col('hour'),\n            col('mins'),\n            col('secs')\n        )\n    )\\\n    .show()\n\n+---+-----+----+----+----+----+-------------------+\n|day|month|year|hour|mins|secs|        as_datetime|\n+---+-----+----+----+----+----+-------------------+\n| 14|    2|2021|  12|  45|   0|2021-02-14 12:45:00|\n| 30|    4|2021|   8|  10|   0|2021-04-30 08:10:00|\n|  2|    5|2021|   5|   9|  12|2021-05-02 05:09:12|\n|  6|    5|2021|   0|  34|   4|2021-05-06 00:34:04|\n+---+-----+----+----+----+----+-------------------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#sec-datetime-patterns",
    "href": "Chapters/10-datetime.html#sec-datetime-patterns",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.3 Introducing datetime patterns",
    "text": "11.3 Introducing datetime patterns\nEvery time you have strings that contains dates and datetime values that are outside of the ISO-8601 format, you usually have to use datetime patterns to convert these strings to date or datetime values. In other words, in this section I will describe how you can use datetime patterns to convert string values (that are outside of the ISO-8601 format) into dates or datetimes values.\nDespite the existence of an international standard (like ISO-8601), the used date and datetime formats varies accross different countries around the world. There are tons of examples of these different formats. For example, in Brazil, dates are usually formatted like “Day/Month/Year”. Not only the order of the date components (day, month and year) are different, but also, the separator character (“/” instead of “-”).\nIn essence, a datetime pattern is a string pattern that describes a specific date or datetime format. This means that we can use a datetime pattern to describe any date or datetime format. A datetime pattern is a string value that is constructed by grouping letters together. Each individual letter represents an individual component of a date or a datetime value.\nYou can see at Table 11.1 a list of the most commom used letters in datetime patterns. If you want, you can also see the full list of possible letters in datetime patterns by visiting the Spark Datetime Patterns page1.\n\n\n\nTable 11.1: List of letters to represent date and datetime components\n\n\n\n\n\n\n\n\n\n\nLetter\nMeaning\nExample of a valid value\n\n\n\n\nG\nera\nAD; Anno Domini\n\n\ny\nyear\n2020; 20\n\n\nD\nday-of-year\n189\n\n\nM/L\nmonth-of-year\n7; 07; Jul; July\n\n\nd\nday-of-month\n28\n\n\nQ/q\nquarter-of-year\n3; 03; Q3; 3rd quarter\n\n\nE\nday-of-week\nTue; Tuesday\n\n\nF\naligned day of week\n3\n\n\na\nam-pm-of-day\nPM\n\n\nh\nclock-hour-of-am-pm\n12\n\n\nK\nhour-of-am-pm\n0\n\n\nk\nclock-hour-of-day\n0\n\n\nH\nhour-of-day\n0\n\n\nm\nminute-of-hour\n30\n\n\ns\nsecond-of-minute\n55\n\n\nS\nfraction-of-second\n978\n\n\nV\ntime-zone ID\nAmerica/Los_Angeles; Z; -08:30\n\n\nz\ntime-zone name\nPacific Standard Time; PST\n\n\nO\nlocalized zone-offset\nGMT+8; GMT+08:00; UTC-08:00;\n\n\nX\nzone-offset ‘Z’ for zero\nZ; -08; -0830; -08:30; -083015; -08:30:15;\n\n\nx\nzone-offset\n+0000; -08; -0830; -08:30; -083015; -08:30:15;\n\n\nZ\nzone-offset\n+0000; -0800; -08:00;\n\n\n\n\n\n\n\n11.3.1 Using datetime patterns to get date values\nFollowing Table 11.1, if we had a date in the format “Day, Month of Year”, like “12, November of 1997”, we would use the letters d, M and y for each of the three components that are present in this format. In fact, let’s create a DataFrame with this exact value, and let’s demonstrate how could you convert it to a DateType value.\n\ndata = [ {\"date\": \"12, November of 1997\"} ]\nweird_date = spark.createDataFrame(data)\nweird_date.show()\n\n+--------------------+\n|                date|\n+--------------------+\n|12, November of 1997|\n+--------------------+\n\n\n\nTo convert it from the StringType to DateType we have to use the to_date() Spark SQL function. Because with this function, we can provide the datetime pattern that describes the exact format that these dates use. But before we use to_date(), we need to build a datetime pattern to use.\nThe date example above (“12, November of 1997”) starts with a two-digit day number. That is why we begin our datetime pattern with two d’s, to represent this section of the date. After that we have a comma and a space followed by the month name. However, both month name and the year number at the end of the date are in their full formats, instead of their abbreviated formats.\nBecause of that, we need to tell Spark to use the full format instead of the abbreviated format on both of these two components. To do that, we use four M’s and four y’s, instead of just two. At last, we have a literal “of” between the month name and the year name, and to describe this specific section of the date, we insert 'of' between the M’s and y’s.\nIn essence, we have the datetime pattern \"dd, MMMM 'of' yyyy\". Now, we can just use to_date() with this datetime pattern to convert the string value to a date value.\n\nfrom pyspark.sql.functions import to_date\npattern = \"dd, MMMM 'of' yyyy\"\nweird_date = weird_date\\\n    .withColumn(\"date\", to_date(col(\"date\"), pattern))\n\nweird_date.show()\n\n+----------+\n|      date|\n+----------+\n|1997-11-12|\n+----------+\n\n\n\n\nweird_date.printSchema()\n\nroot\n |-- date: date (nullable = true)\n\n\n\nSo, if you have a constant (or fixed) text that is always present in all of your date values, like the “of” in the above example, you must encapsulate this text between quotation marks in your datetime patterns. Also, depending if your components are in a abbreviated format (like “02” for year 2002, or “Jan” for the January month), or in a full format (like the year “1997” or the month “October”), you might want to repeat the letters one or two times (to use abbreviated formats), or you might want to repeat it four times (to use the full formats). In other words, if you use less than 4 pattern letters, then, Spark will use the short text form, typically an abbreviation form of the component you are reffering to Apache Spark Official Documentation (2022).\nAs another example of unusual date formats, lets use the user_events DataFrame. You can easily get the data of this DataFrame by dowloading the JSON file from the official repository of this book2. After you downloaded the JSON file that contains the data, you can import it to your Spark session with the commands below:\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql.types import LongType, TimestampType, DateType\npath = \"../Data/user-events.json\"\nschema = StructType([\n    StructField('dateOfEvent', StringType(), False),\n    StructField('timeOfEvent', StringType(), False),\n    StructField('userId', StringType(), False),\n    StructField('nameOfEvent', StringType(), False)\n])\n\nuser_events = spark.read\\\n    .json(path, schema = schema)\n\nuser_events.show()\n\n+-----------+-------------------+--------------------+--------------------+\n|dateOfEvent|        timeOfEvent|              userId|         nameOfEvent|\n+-----------+-------------------+--------------------+--------------------+\n| 15/06/2022|15/06/2022 14:33:10|b902e51e-d043-4a6...|               entry|\n| 15/06/2022|15/06/2022 14:40:08|b902e51e-d043-4a6...|         click: shop|\n| 15/06/2022|15/06/2022 15:48:41|b902e51e-d043-4a6...|select: payment-m...|\n+-----------+-------------------+--------------------+--------------------+\n\n\n\nThe dateOfEvent column of this DataFrame contains date values in the Brazilian format “Day/Month/Year”. To describe this date format, we can use the datetime pattern \"dd/MM/yyyy\".\n\ndate_pattern = \"dd/MM/yyyy\"\nuser_events = user_events\\\n    .withColumn('dateOfEvent', to_date(col('dateOfEvent'), date_pattern))\n\nuser_events.show()\n\n+-----------+-------------------+--------------------+--------------------+\n|dateOfEvent|        timeOfEvent|              userId|         nameOfEvent|\n+-----------+-------------------+--------------------+--------------------+\n| 2022-06-15|15/06/2022 14:33:10|b902e51e-d043-4a6...|               entry|\n| 2022-06-15|15/06/2022 14:40:08|b902e51e-d043-4a6...|         click: shop|\n| 2022-06-15|15/06/2022 15:48:41|b902e51e-d043-4a6...|select: payment-m...|\n+-----------+-------------------+--------------------+--------------------+\n\n\n\n\n\n11.3.2 Using datetime patterns to get datetime/timestamp values\nFurthermore, the user_events table also contains the timeOfEvent column, which is a column of datetime (or timestamp) values, also in the usual Brazilian format “Day/Month/Year Hour:Minutes:Seconds”. Following Table 11.1, we can describe this datetime format with the pattern \"dd/MM/yyyy HH:mm:ss\".\nWhen you have a column of string values that you want to convert into the TimestampType type, but your values are not in ISO-8601 format, you should use the to_timestamp() function (which also comes from the pyspark.sql.functions module) together with a datetime pattern to describe the actual format of your values.\nYou use the to_timestamp() function in the same way you would use the to_date() function, you reference the name of the column you want to convert into a column of timestamp values, and you also provide a datetime pattern to be used in the conversion. The difference between the two functions is solely on the type of columns they produce. The to_timestamp() function always produce a column of type TimestampType as a result, while the to_date() function produces a column of type DateType.\n\nfrom pyspark.sql.functions import to_timestamp\ndatetime_pattern = \"dd/MM/yyyy HH:mm:ss\"\nuser_events = user_events\\\n    .withColumn(\n        'timeOfEvent',\n        to_timestamp(col('timeOfEvent'), datetime_pattern)\n    )\n\nuser_events.show()\n\n+-----------+-------------------+--------------------+--------------------+\n|dateOfEvent|        timeOfEvent|              userId|         nameOfEvent|\n+-----------+-------------------+--------------------+--------------------+\n| 2022-06-15|2022-06-15 14:33:10|b902e51e-d043-4a6...|               entry|\n| 2022-06-15|2022-06-15 14:40:08|b902e51e-d043-4a6...|         click: shop|\n| 2022-06-15|2022-06-15 15:48:41|b902e51e-d043-4a6...|select: payment-m...|\n+-----------+-------------------+--------------------+--------------------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#extracting-date-or-datetime-components",
    "href": "Chapters/10-datetime.html#extracting-date-or-datetime-components",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.4 Extracting date or datetime components",
    "text": "11.4 Extracting date or datetime components\nOne very commom operation when dealing with dates and datetime values is extracting components from these values. For example, you might want to create a new column that contains the day component from all of your dates. Or maybe, the month… anyway.\nThakfully, pyspark made it pretty easy to extract these kinds of components. You just use the corresponding function to each component! All of these functions come from the pyspark.sql.functions module. As a quick list, the functions below are used to extract components from both dates and datetime values:\n\nyear(): extract the year of a given date or datetime as integer.\nmonth(): extract the month of a given date or datetime as integer.\ndayofmonth(): extract the day of the month of a given date or datetime as integer.\ndayofweek(): extract the day of the week of a given date or datetime as integer. The integer returned will be inside the range 1 (for a Sunday) through to 7 (for a Saturday).\ndayofyear(): extract the day of the year of a given date or datetime as integer.\nquarter(): extract the quarter of a given date or datetime as integer.\n\nOn the other hand, the list of functions below only applies to datetime values, because they extract time components (which are not present on date values). These functions also comes to the pyspark.sql.functions module:\n\nhour(): extract the hours of a given datetime as integer.\nminute(): extract the minutes of a given datetime as integer.\nsecond(): extract the seconds of a given datetime as integer.\n\nIn essence, you just apply these functions at the column that contains your date and datetime values, and you get as output the component you want. As an example, lets go back to the user_events DataFrame and extract the day, month and year components of each date, into separate columns:\n\nfrom pyspark.sql.functions import (\n    dayofmonth,\n    month,\n    year\n)\n\nuser_events\\\n    .withColumn('dayOfEvent', dayofmonth(col('dateOfEvent')))\\\n    .withColumn('monthOfEvent', month(col('dateOfEvent')))\\\n    .withColumn('yearOfEvent', year(col('dateOfEvent')))\\\n    .select(\n        'dateOfEvent', 'dayOfEvent',\n        'monthOfEvent', 'yearOfEvent'\n    )\\\n    .show()\n\n+-----------+----------+------------+-----------+\n|dateOfEvent|dayOfEvent|monthOfEvent|yearOfEvent|\n+-----------+----------+------------+-----------+\n| 2022-06-15|        15|           6|       2022|\n| 2022-06-15|        15|           6|       2022|\n| 2022-06-15|        15|           6|       2022|\n+-----------+----------+------------+-----------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#sec-interval-express",
    "href": "Chapters/10-datetime.html#sec-interval-express",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.5 Adding time to date and datetime values with interval expressions",
    "text": "11.5 Adding time to date and datetime values with interval expressions\nAnother very commom operation is to add some amount of time to a date or datetime values. For example, you might want to advance your dates by 3 days. Or, delay your datetime values by 3 hours. In pyspark, you can perform this kind by either using functions or interval expressions.\nWhen we talk about functions available through the pyspark.sql.functions module, we have date_add() and date_sub(), which you can use to add or subtract days to a date, and add_months(), which you can use to add months to a date. Yes, the naming pattern of these functions is a little bit weird, but let’s ignore this fact.\n\nfrom pyspark.sql.functions import (\n    date_add,\n    date_sub,\n    add_months\n)\n\nuser_events\\\n    .withColumn('timePlus10Days', date_add(col('timeOfEvent'), 10))\\\n    .withColumn('timeMinus5Days', date_sub(col('timeOfEvent'), 5))\\\n    .withColumn('timePlus8Months', add_months(col('timeOfEvent'), 8))\\\n    .select(\n        'timeOfEvent', 'timePlus10Days',\n        'timeMinus5Days', 'timePlus8Months'\n    )\\\n    .show()\n\n+-------------------+--------------+--------------+---------------+\n|        timeOfEvent|timePlus10Days|timeMinus5Days|timePlus8Months|\n+-------------------+--------------+--------------+---------------+\n|2022-06-15 14:33:10|    2022-06-25|    2022-06-10|     2023-02-15|\n|2022-06-15 14:40:08|    2022-06-25|    2022-06-10|     2023-02-15|\n|2022-06-15 15:48:41|    2022-06-25|    2022-06-10|     2023-02-15|\n+-------------------+--------------+--------------+---------------+\n\n\n\nNow, you can use interval expressions to add whatever time unit you want (years, months, days, hours, minutes or seconds) to either a date or datetime value. However, interval expressions are only available at the Spark SQL level. As a consequence, to use an interval expression, you must use expr() or spark.sql() (that we introduced at Chapter 7) to get access to Spark SQL. An interval expression follows this pattern:\nINTERVAL 3 HOURS\nAn interval expression is an expression that begins with the INTERVAL keyword, and then, it is followed by a number that specifies an amount. You define how much exactly this amout represents by using another keyword that specifies the unit of time to be used. So the keyword HOUR specifies that you want to use the “hour” unit of time.\nHaving this in mind, the expression INTERVAL 3 HOURS defines a 3 hours interval expression. In contrast, the expression INTERVAL 3 SECONDS represents 3 seconds, and the expression INTERVAL 90 MINUTES represents 90 minutes, and so on.\nBy having an interval expression, you can just add or subtract this interval to your column to change the timestamp you have.\n\nfrom pyspark.sql.functions import expr\nuser_events\\\n    .withColumn(\n        'timePlus3Hours',\n        expr('timeOfEvent + INTERVAL 3 HOURS')\n    )\\\n    .withColumn(\n        'timePlus2Years',\n        expr('timeOfEvent + INTERVAL 2 YEARS')\n    )\\\n    .select('timeOfEvent', 'timePlus3Hours', 'timePlus2Years')\\\n    .show()\n\n+-------------------+-------------------+-------------------+\n|        timeOfEvent|     timePlus3Hours|     timePlus2Years|\n+-------------------+-------------------+-------------------+\n|2022-06-15 14:33:10|2022-06-15 17:33:10|2024-06-15 14:33:10|\n|2022-06-15 14:40:08|2022-06-15 17:40:08|2024-06-15 14:40:08|\n|2022-06-15 15:48:41|2022-06-15 18:48:41|2024-06-15 15:48:41|\n+-------------------+-------------------+-------------------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#calculating-differences-between-dates-and-datetime-values",
    "href": "Chapters/10-datetime.html#calculating-differences-between-dates-and-datetime-values",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.6 Calculating differences between dates and datetime values",
    "text": "11.6 Calculating differences between dates and datetime values\nAnother very commom operation is to calculate how much time is between two dates or two datetimes. In other words, you might want to calculate how many days the date \"2023-05-12\" is far ahead of \"2023-05-05\", or how many hours the timestamp \"2023-07-10 13:13:00\" is ahead of \"2023-07-10 05:18:00\".\nWhen you have two columns of type DateType (or TimestampType), you subtract one by the other directly to get the difference between the values of these columns. As an example, lets use the df4 below:\n\nfrom random import randint, seed\nfrom datetime import (\n    date,\n    datetime,\n    timedelta\n)\n\nseed(10)\ndates = [\n    date(2023,4,1) + timedelta(days = randint(1,39))\n    for i in range(4)\n]\n\ndatetimes = [\n    datetime(2023,4,1,8,14,54) + timedelta(hours = randint(1,39))\n    for i in range(4)\n]\n\n\ndf4 = spark.createDataFrame([\n    Row(\n        date1 = date(2023,4,1),\n        date2 = dates[i],\n        datetime1 = datetime(2023,4,1,8,14,54),\n        datetime2 = datetimes[i]\n    )\n    for i in range(4)\n])\n\ndf4.show()\n\n+----------+----------+-------------------+-------------------+\n|     date1|     date2|          datetime1|          datetime2|\n+----------+----------+-------------------+-------------------+\n|2023-04-01|2023-05-08|2023-04-01 08:14:54|2023-04-02 21:14:54|\n|2023-04-01|2023-04-04|2023-04-01 08:14:54|2023-04-01 09:14:54|\n|2023-04-01|2023-04-29|2023-04-01 08:14:54|2023-04-01 22:14:54|\n|2023-04-01|2023-05-02|2023-04-01 08:14:54|2023-04-02 14:14:54|\n+----------+----------+-------------------+-------------------+\n\n\n\nIf we subtract columns date2 by date1 and datetime2 by datetime1, we get two new columns of type DayTimeIntervalType. These new columns represent the interval of time between the values of these columns.\n\ndf4\\\n    .select('date1', 'date2')\\\n    .withColumn('datediff', col('date2') - col('date1'))\\\n    .show()\n\n+----------+----------+-----------------+\n|     date1|     date2|         datediff|\n+----------+----------+-----------------+\n|2023-04-01|2023-05-08|INTERVAL '37' DAY|\n|2023-04-01|2023-04-04| INTERVAL '3' DAY|\n|2023-04-01|2023-04-29|INTERVAL '28' DAY|\n|2023-04-01|2023-05-02|INTERVAL '31' DAY|\n+----------+----------+-----------------+\n\n\n\n\ndf4\\\n    .select('datetime1', 'datetime2')\\\n    .withColumn('datetimediff', col('datetime2') - col('datetime1'))\\\n    .show()\n\n+-------------------+-------------------+--------------------+\n|          datetime1|          datetime2|        datetimediff|\n+-------------------+-------------------+--------------------+\n|2023-04-01 08:14:54|2023-04-02 21:14:54|INTERVAL '1 13:00...|\n|2023-04-01 08:14:54|2023-04-01 09:14:54|INTERVAL '0 01:00...|\n|2023-04-01 08:14:54|2023-04-01 22:14:54|INTERVAL '0 14:00...|\n|2023-04-01 08:14:54|2023-04-02 14:14:54|INTERVAL '1 06:00...|\n+-------------------+-------------------+--------------------+\n\n\n\nBut another very commom way to calculate this difference is to convert the datetime values to seconds. Then, you subtract the calculated seconds, so you get as output, the difference in seconds between the two datetime values.\nTo use this method, you first use the unix_timestamp() function to convert the timestamps into seconds as integer values, then, you subtract these new integer values that were created.\n\nfrom pyspark.sql.functions import unix_timestamp\ndf4 = df4\\\n    .select('datetime1', 'datetime2')\\\n    .withColumn('datetime1', unix_timestamp(col('datetime1')))\\\n    .withColumn('datetime2', unix_timestamp(col('datetime2')))\\\n    .withColumn('diffinseconds', col('datetime2') - col('datetime1'))\n\ndf4.show()\n\n+----------+----------+-------------+\n| datetime1| datetime2|diffinseconds|\n+----------+----------+-------------+\n|1680347694|1680480894|       133200|\n|1680347694|1680351294|         3600|\n|1680347694|1680398094|        50400|\n|1680347694|1680455694|       108000|\n+----------+----------+-------------+\n\n\n\nNow that we have the diffinseconds column, which represents the difference in seconds between datetime1 and datetime2 columns, we can divide this diffinseconds column by 60 to get the difference in minutes, or divide by 3600 to get the difference in hours, etc.\n\ndf4\\\n    .withColumn('diffinminutes', col('diffinseconds') / 60)\\\n    .withColumn('diffinhours', col('diffinseconds') / 3600)\\\n    .show()\n\n+----------+----------+-------------+-------------+-----------+\n| datetime1| datetime2|diffinseconds|diffinminutes|diffinhours|\n+----------+----------+-------------+-------------+-----------+\n|1680347694|1680480894|       133200|       2220.0|       37.0|\n|1680347694|1680351294|         3600|         60.0|        1.0|\n|1680347694|1680398094|        50400|        840.0|       14.0|\n|1680347694|1680455694|       108000|       1800.0|       30.0|\n+----------+----------+-------------+-------------+-----------+",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#getting-the-now-and-today-values",
    "href": "Chapters/10-datetime.html#getting-the-now-and-today-values",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "11.7 Getting the now and today values",
    "text": "11.7 Getting the now and today values\nIf for some reason, you need to know which timestamp is now, or which date is today, you can use the current_timestamp() and current_date() functions from pyspark.sql.functions module. When you execute these functions, they output the current timestamp and date in your system.\n\nfrom pyspark.sql.types import StructType,StructField, StringType\nfrom pyspark.sql.functions import (\n    current_date,\n    current_timestamp,\n    lit\n)\n\ndata = [Row(today=\"\", now=\"\")]\nschema = StructType([\n    StructField('today', StringType(), True),\n    StructField('now', StringType(), True)\n])\n\nspark.createDataFrame(data, schema = schema)\\\n    .withColumn('today', lit(current_date()))\\\n    .withColumn('now', lit(current_timestamp()))\\\n    .show()\n\n+----------+--------------------+\n|     today|                 now|\n+----------+--------------------+\n|2023-12-20|2023-12-20 17:07:...|\n+----------+--------------------+\n\n\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1. https://spark.apache.org/docs/latest/.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/10-datetime.html#footnotes",
    "href": "Chapters/10-datetime.html#footnotes",
    "title": "11  Tools for dates and datetimes manipulation",
    "section": "",
    "text": "https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html↩︎\nhttps://github.com/pedropark99/Introd-pyspark/tree/main/Data↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tools for dates and datetimes manipulation</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#sec-window-def",
    "href": "Chapters/11-window.html#sec-window-def",
    "title": "12  Introducing window functions",
    "section": "12.1 How to define windows",
    "text": "12.1 How to define windows\nIn order to use a window function you need to define the windows of your DataFrame first. You do this by creating a Window object in your session.\nEvery window object have two components, which are partitioning and ordering, and you specify each of these components by using the partitionBy() and orderBy() methods from the Window class. In order to create a Window object, you need to import the Window class from the pyspark.sql.window module:\n\nfrom pyspark.sql.window import Window\n\nOver the next examples, I will be using the transf DataFrame that we presented at Chapter 5. If you don’t remember how to import/get this DataFrame into your session, come back to Section 5.4.\n\ntransf.show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       NULL|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       NULL|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       NULL|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       NULL|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       NULL|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nNow, lets create a window object using the transf DataFrame as our target. This DataFrame describes a set of transfers made in a fictitious bank. So a reasonable way of splitting this DataFrame is by day. That means that we can split this DataFrame into groups (or ranges) of rows by using the dateTransfer column. As a result, each partition in the dateTransfer column will create/identify a different window in this DataFrame.\n\nwindow_spec = Window.partitionBy('dateTransfer')\n\nThe above window object specifies that each unique value present in the dateTransfer column identifies a different window frame in the transf DataFrame. Figure 12.1 presents this idea visually. So each partition in the dateTransfer column creates a different window frame. And each window frame will become an input to a window function (when we use one).\n\n\n\n\n\n\nFigure 12.1: Visualizing the window frames - Part 1\n\n\n\nUntil this point, defining windows are very much like defining groups in your DataFrame with group by functions (i.e. windows are very similar to groups). But in the above example, we specified only the partition component of the windows. The partitioning component of the window object specifies which partitions of the DataFrame are translated into windows. In the other hand, the ordering component of the window object specifies how the rows within the window are ordered.\nDefining the ordering component becomes very important when we are working with window functions that outputs (or that uses) indexes. As an example, you might want to use in your calculations the first (or the nth) row in each window. In a situation like this, the order in which these rows are founded inside the window affects directly the output of your window function. That is why the ordering component matters.\nFor example, we can say that the rows within each window should be in descending order according to the datetimeTransfer column:\n\nfrom pyspark.sql.functions import col\nwindow_spec = Window\\\n    .partitionBy('dateTransfer')\\\n    .orderBy(col('datetimeTransfer').desc())\n\nWith the above snippet, we are not only specifying how the window frames in the DataFrame are created (with the partitionBy()), but we are also specifying how the rows within the window are sorted (with the orderBy()). If we update our representation with the above window specification, we get something similar to Figure 12.2:\n\n\n\n\n\n\nFigure 12.2: Visualizing the window frames - Part 2\n\n\n\nIs worth mentioning that, both partitionBy() and orderBy() methods accepts multiple columns as input. In other words, you can use a combination of columns both to define how the windows in your DataFrame will be created, and how the rows within these windows will be sorted.\nAs an example, the window specification below is saying: 1) that a window frame is created for each unique combination of dateTransfer and clientNumber; 2) that the rows within each window are ordered accordingly to transferCurrency (ascending order) and datetimeTransfer (descending order).\n\nwindow_spec = Window\\\n    .partitionBy('dateTransfer', 'clientNumber')\\\n    .orderBy(\n        col('transferCurrency').asc(),\n        col('datetimeTransfer').desc()\n    )\n\n\n12.1.1 Partitioning or ordering or none\nIs worth mentioning that both partioning and ordering components of the window specification are optional. You can create a window object that contains only a partioning component defined, or, only a ordering component, or, in fact, a window object that basically have neither of them defined.\nAs an example, all three objects below (w1, w2 and w3) are valid window objects. w1 have only the partition component defined, while w2 have only the ordering component defined. However, w3 have basically none of them defined, because w3 is partitioned by nothing. In a situation like this, a single window is created, and this window covers the entire DataFrame. It covers all the rows at once. Is like you were not using any window at all.\n\nw1 = Window.partitionBy('x')\nw2 = Window.orderBy('x')\nw3 = Window.partitionBy()\n\nSo just be aware of this. Be aware that you can cover the entire DataFrame into a single window. Be aware that if you use a window object with neither components defined (Window.partitionBy()) your window function basically works with the entire DataFrame at once. In essence, this window function becomes similar to a normal aggregating function.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#introducing-the-over-clause",
    "href": "Chapters/11-window.html#introducing-the-over-clause",
    "title": "12  Introducing window functions",
    "section": "12.2 Introducing the over() clause",
    "text": "12.2 Introducing the over() clause\nIn order to use a window function you need to combine an over clause with a window object. If you pair these two components together, then, the function you are using becomes a window function.\nSince we know now how to define window objects for our DataFrame, we can actually create and use this object to access window functionality, by pairing this window object with an over() clause.\nIn pyspark this over() clause is actually a method from the Column class. Since all aggregating functions available from the pyspark.sql.functions module produces a new Column object as output, we tend to use the over() method right after the function call.\nFor example, if we wanted to calculate the mean of x with the mean() function, and we had a window object called window_spec, we could use the mean() as a window function by writing mean(col('x')).over(window_spec).\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import mean, col\nwindow_spec = Window\\\n    .partitionBy('y', 'z')\\\n    .orderBy('t')\n\nmean(col('x')).over(window_spec)\nIf you see this over() method after a call of an aggregating function (such as sum(), mean(), etc.), then, you know that this aggregating function is being called as a window function.\nThe over() clause is also available in Spark SQL as the SQL keyword OVER. This means that you can use window functions in Spark SQL as well. But in Spark SQL, you write the window specification inside parentheses after the OVER keyword, and you specify each component with PARTITION BY AND ORDER BY keywords. We could replicate the above example in Spark SQL like this:\nSELECT mean(x) OVER (PARTITION BY y, z ORDER BY t ASC)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#window-functions-vs-group-by-functions",
    "href": "Chapters/11-window.html#window-functions-vs-group-by-functions",
    "title": "12  Introducing window functions",
    "section": "12.3 Window functions vs group by functions",
    "text": "12.3 Window functions vs group by functions\nDespite their similarities, window functions and group by functions are used for different purposes. One big difference between them, is that when you use groupby() + agg() you get one output row per each input group of rows, but in contrast, a window function outputs one row per input row. In other words, for a window of \\(n\\) input rows a window function outputs \\(n\\) rows that contains the same result (or the same aggregate result).\nFor example, lets suppose you want to calculate the total value transfered within each day. If you use a groupby() + agg() strategy, you get as result a new DataFrame containing one row for each unique date present in the dateTransfer column:\n\nfrom pyspark.sql.functions import sum\ntransf\\\n    .orderBy('dateTransfer')\\\n    .groupBy('dateTransfer')\\\n    .agg(sum(col('transferValue')).alias('dayTotalTransferValue'))\\\n    .show(5)\n\n+------------+---------------------+\n|dateTransfer|dayTotalTransferValue|\n+------------+---------------------+\n|  2022-01-01|              39630.7|\n|  2022-01-02|             70031.46|\n|  2022-01-03|   50957.869999999995|\n|  2022-01-04|             56068.34|\n|  2022-01-05|             47082.04|\n+------------+---------------------+\nonly showing top 5 rows\n\n\n\nOn the other site, if you use sum() as a window function instead, you get as result one row for each transfer. That is, you get one row of output for each input row in the transf DataFrame. The value that is present in the new column created (dayTotalTransferValue) is the total value transfered for the window (or the range of rows) that corresponds to the date in the dateTransfer column.\nIn other words, the value 39630.7 below corresponds to the sum of the transferValue column when dateTransfer == \"2022-01-01\":\n\nwindow_spec = Window.partitionBy('dateTransfer')\ntransf\\\n    .select('dateTransfer', 'transferID', 'transferValue')\\\n    .withColumn(\n        'dayTotalTransferValue',\n        sum(col('transferValue')).over(window_spec)\n    )\\\n    .show(5)\n\n+------------+----------+-------------+---------------------+\n|dateTransfer|transferID|transferValue|dayTotalTransferValue|\n+------------+----------+-------------+---------------------+\n|  2022-01-01|  20221148|      5547.13|              39630.7|\n|  2022-01-01|  20221147|       9941.0|              39630.7|\n|  2022-01-01|  20221146|       5419.9|              39630.7|\n|  2022-01-01|  20221145|       5006.0|              39630.7|\n|  2022-01-01|  20221144|      8640.06|              39630.7|\n+------------+----------+-------------+---------------------+\nonly showing top 5 rows\n\n\n\nYou probably already seen this pattern in other data frameworks. As a quick comparison, if you were using the tidyverse framework, you could calculate the exact same result above with the following snippet of R code:\ntransf |&gt;\n    group_by(dateTransfer) |&gt;\n    mutate(\n        dayTotalTransferValue = sum(transferValue)\n    )\nIn contrast, you would need the following snippet of Python code to get the same result in the pandas framework:\ntransf['dayTotalTransferValue'] = transf['transferValue']\\\n    .groupby(transf['dateTransfer'])\\\n    .transform('sum')",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#ranking-window-functions",
    "href": "Chapters/11-window.html#ranking-window-functions",
    "title": "12  Introducing window functions",
    "section": "12.4 Ranking window functions",
    "text": "12.4 Ranking window functions\nThe functions row_number(), rank() and dense_rank() from the pyspark.sql.functions module are ranking functions, in the sense that they seek to rank each row in the input window according to a ranking system. These functions are identical to their peers in MySQL4 ROW_NUMBER(), RANK() and DENSE_RANK().\nThe function row_number() simply returns a unique and sequential number to each row in a window, starting from 1. It is a quick way of marking each row with an unique and sequential number.\n\nfrom pyspark.sql.functions import row_number\nwindow_spec = Window\\\n    .partitionBy('dateTransfer')\\\n    .orderBy('datetimeTransfer')\n\ntransf\\\n    .select(\n        'dateTransfer',\n        'datetimeTransfer',\n        'transferID'\n    )\\\n    .withColumn('rowID', row_number().over(window_spec))\\\n    .show(5)\n\n+------------+-------------------+----------+-----+\n|dateTransfer|   datetimeTransfer|transferID|rowID|\n+------------+-------------------+----------+-----+\n|  2022-01-01|2022-01-01 03:56:58|  20221143|    1|\n|  2022-01-01|2022-01-01 04:07:44|  20221144|    2|\n|  2022-01-01|2022-01-01 09:00:18|  20221145|    3|\n|  2022-01-01|2022-01-01 10:17:04|  20221146|    4|\n|  2022-01-01|2022-01-01 16:14:30|  20221147|    5|\n+------------+-------------------+----------+-----+\nonly showing top 5 rows\n\n\n\nThe row_number() function is also very useful when you are trying to collect the rows in each window that contains the smallest or biggest value in the window. If the ordering of your window specification is in ascending order, then, the first row in the window will contain the smallest value in the current window. In contrast, if the ordering is in descending order, then, the first row in the window will contain the biggest value in the current window.\nThis is interesting, because lets suppose you wanted to find the rows that contained the maximum transfer values in each day. A groupby() + agg() strategy would tell you which are the maximum transfer values in each day. But it would not tell you where are the rows in the DataFrame that contains these maximum values. A Window object + row_number() + filter() can help you to get this answer.\n\nwindow_spec = Window\\\n    .partitionBy('dateTransfer')\\\n    .orderBy(col('transferValue').desc())\n\n# The row with rowID == 1 is the first row in each window\ntransf\\\n    .withColumn('rowID', row_number().over(window_spec))\\\n    .filter(col('rowID') == 1)\\\n    .select('dateTransfer', 'rowID', 'transferID', 'transferValue')\\\n    .show(5)\n\n+------------+-----+----------+-------------+\n|dateTransfer|rowID|transferID|transferValue|\n+------------+-----+----------+-------------+\n|  2022-01-01|    1|  20221147|       9941.0|\n|  2022-01-02|    1|  20221157|     10855.01|\n|  2022-01-03|    1|  20221165|      8705.65|\n|  2022-01-04|    1|  20221172|       9051.0|\n|  2022-01-05|    1|  20221179|       9606.0|\n+------------+-----+----------+-------------+\nonly showing top 5 rows\n\n\n\nThe rank() and dense_rank() functions are similar to each other. They both rank the rows with integers, just like row_number(). But if there is a tie between two rows (that means that both rows have the same value in the ordering column, so it becomes a tie, we do not know which one of these rows should come first), then, these functions will repeat the same number/index for these rows in tie. Lets use the df below as a quick example:\n\ndata = [\n    (1, 3000), (1, 2400),\n    (1, 4200), (1, 4200),\n    (2, 1500), (2, 2000),\n    (2, 3000), (2, 3000),\n    (2, 4500), (2, 4600)\n]\ndf = spark.createDataFrame(data, ['id', 'value'])\n\nIf we apply both rank() and dense_rank() over this DataFrame with the same window specification, we can see the difference between these functions. In essence, rank() leave gaps in the indexes that come right after any tied rows, while dense_rank() does not.\n\nfrom pyspark.sql.functions import rank, dense_rank\nwindow_spec = Window\\\n    .partitionBy('id')\\\n    .orderBy('value')\n\n# With rank() there are gaps in the indexes\ndf.withColumn('with_rank', rank().over(window_spec))\\\n    .show()\n\n+---+-----+---------+\n| id|value|with_rank|\n+---+-----+---------+\n|  1| 2400|        1|\n|  1| 3000|        2|\n|  1| 4200|        3|\n|  1| 4200|        3|\n|  2| 1500|        1|\n|  2| 2000|        2|\n|  2| 3000|        3|\n|  2| 3000|        3|\n|  2| 4500|        5|\n|  2| 4600|        6|\n+---+-----+---------+\n\n\n\n[Stage 14:&gt;                                                       (0 + 12) / 12]                                                                                \n\n\n\n# With dense_rank() there are no gaps in the indexes\ndf.withColumn('with_dense_rank', dense_rank().over(window_spec))\\\n    .show()\n\n+---+-----+---------------+\n| id|value|with_dense_rank|\n+---+-----+---------------+\n|  1| 2400|              1|\n|  1| 3000|              2|\n|  1| 4200|              3|\n|  1| 4200|              3|\n|  2| 1500|              1|\n|  2| 2000|              2|\n|  2| 3000|              3|\n|  2| 3000|              3|\n|  2| 4500|              4|\n|  2| 4600|              5|\n+---+-----+---------------+",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#agreggating-window-functions",
    "href": "Chapters/11-window.html#agreggating-window-functions",
    "title": "12  Introducing window functions",
    "section": "12.5 Agreggating window functions",
    "text": "12.5 Agreggating window functions\nIn essence, all agreggating functions from the pyspark.sql.functions module (like sum(), mean(), count(), max() and min()) can be used as a window function. So you can apply any agreggating function as a window function. You just need to use the over() clause with a Window object.\nWe could for example see how much each transferValue deviates from the daily mean of transfered value. This might be a valuable information in case you are planning to do some statistical inference over this data. Here is an example of what this would looks like in pyspark:\n\nfrom pyspark.sql.functions import mean\nwindow_spec = Window.partitionBy('dateTransfer')\n\nmean_deviation_expr = (\n    col('transferValue')\n    - mean(col('transferValue')).over(window_spec)\n)\n\ntransf\\\n    .select('dateTransfer', 'transferValue')\\\n    .withColumn('meanDeviation', mean_deviation_expr)\\\n    .show(5)\n\n+------------+-------------+-------------------+\n|dateTransfer|transferValue|      meanDeviation|\n+------------+-------------+-------------------+\n|  2022-01-01|      5547.13|-1057.9866666666658|\n|  2022-01-01|       9941.0|  3335.883333333334|\n|  2022-01-01|       5419.9|-1185.2166666666662|\n|  2022-01-01|       5006.0|-1599.1166666666659|\n|  2022-01-01|      8640.06| 2034.9433333333336|\n+------------+-------------+-------------------+\nonly showing top 5 rows\n\n\n\nAs another example, you might want to calculate how much a specific transfer value represents of represents of the total amount transferred daily. You could just get the total amount transferred daily by applying the sum() function over windows partitioned by dateTransfer. Then, you just need to divide the current transferValue by the result of this sum() function, and you get the proportion you are looking for.\n\nfrom pyspark.sql.functions import sum\nproportion_expr = (\n    col('transferValue')\n    / sum(col('transferValue')).over(window_spec)\n)\n\ntransf\\\n    .select('dateTransfer', 'transferValue')\\\n    .withColumn('proportionDailyTotal', proportion_expr)\\\n    .show(5)\n\n+------------+-------------+--------------------+\n|dateTransfer|transferValue|proportionDailyTotal|\n+------------+-------------+--------------------+\n|  2022-01-01|      5547.13|  0.1399705278988259|\n|  2022-01-01|       9941.0| 0.25084088850310493|\n|  2022-01-01|       5419.9|  0.1367601379738435|\n|  2022-01-01|       5006.0|  0.1263162144499088|\n|  2022-01-01|      8640.06|  0.2180143171833957|\n+------------+-------------+--------------------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#getting-the-next-and-previous-row-with-lead-and-lag",
    "href": "Chapters/11-window.html#getting-the-next-and-previous-row-with-lead-and-lag",
    "title": "12  Introducing window functions",
    "section": "12.6 Getting the next and previous row with lead() and lag()",
    "text": "12.6 Getting the next and previous row with lead() and lag()\nThere is one pair functions that is worth talking about in this chapter, which are lead() and lag(). These functions are very useful in the context of windows, because they return the value in the next and previous rows considering your current position in your DataFrame.\nThese functions basically performs the same operation as their peers dplyr::lead() and dplyr::lag()5 from the tidyverse framework. In essence, lead() will return the value of the next row, while lag() will return the value of the previous row.\n\nfrom pyspark.sql.functions import lag, lead\nwindow_spec = Window\\\n    .partitionBy('dateTransfer')\\\n    .orderBy('datetimeTransfer')\n\nlead_expr = lead('transferValue').over(window_spec)\nlag_expr = lag('transferValue').over(window_spec)\n\ntransf\\\n    .withColumn('nextValue', lead_expr)\\\n    .withColumn('previousValue', lag_expr)\\\n    .select(\n        'datetimeTransfer',\n        'transferValue',\n        'nextValue',\n        'previousValue'\n    )\\\n    .show(5)\n\n+-------------------+-------------+---------+-------------+\n|   datetimeTransfer|transferValue|nextValue|previousValue|\n+-------------------+-------------+---------+-------------+\n|2022-01-01 03:56:58|      5076.61|  8640.06|         NULL|\n|2022-01-01 04:07:44|      8640.06|   5006.0|      5076.61|\n|2022-01-01 09:00:18|       5006.0|   5419.9|      8640.06|\n|2022-01-01 10:17:04|       5419.9|   9941.0|       5006.0|\n|2022-01-01 16:14:30|       9941.0|  5547.13|       5419.9|\n+-------------------+-------------+---------+-------------+\nonly showing top 5 rows",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/11-window.html#footnotes",
    "href": "Chapters/11-window.html#footnotes",
    "title": "12  Introducing window functions",
    "section": "",
    "text": "https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html↩︎\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html↩︎\nhttps://dplyr.tidyverse.org/reference/group_by.html↩︎\nhttps://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number↩︎\nhttps://dplyr.tidyverse.org/reference/lead-lag.html↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing window functions</span>"
    ]
  },
  {
    "objectID": "Chapters/references.html",
    "href": "Chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Apache Spark Official Documentation. 2022. Documentation for\nApache Spark 3.2.1. https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive\nGuide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly\nMedia.\n\n\nDamji, Jules, Brooke Wenig, Tathagata Das, and Denny Lee. 2020.\nLearning Spark: Lightning-Fast Data Analytics. Sebastopol, CA:\nO’Reilly Media.\n\n\nGoyvaerts, Jan. 2023. “Regular-Expressions.info.” https://www.regular-expressions.info/.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.\nLearning Spark: Lightning-Fast Data Analytics. Sebastopol, CA:\nO’Reilly Media.",
    "crumbs": [
      "References"
    ]
  }
]